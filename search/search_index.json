{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Hayhooks","text":"<p>Hayhooks makes it easy to deploy and serve Haystack Pipelines and Agents.</p> <p>With Hayhooks, you can:</p> <ul> <li>\ud83d\udce6 Deploy your Haystack pipelines and agents as REST APIs with maximum flexibility and minimal boilerplate code.</li> <li>\ud83d\udee0\ufe0f Expose your Haystack pipelines and agents over the MCP protocol, making them available as tools in AI dev environments like Cursor or Claude Desktop. Under the hood, Hayhooks runs as an MCP Server, exposing each pipeline and agent as an MCP Tool.</li> <li>\ud83d\udcac Integrate your Haystack pipelines and agents with Open WebUI as OpenAI-compatible chat completion backends with streaming support.</li> <li>\ud83d\udda5\ufe0f Embed a Chainlit chat UI directly in Hayhooks with <code>pip install \"hayhooks[chainlit]\"</code> and <code>hayhooks run --with-chainlit</code> -- zero-configuration frontend with streaming, pipeline selection, and custom UI widgets.</li> <li>\ud83d\udd79\ufe0f Control Hayhooks core API endpoints through chat - deploy, undeploy, list, or run Haystack pipelines and agents by chatting with Claude Desktop, Cursor, or any other MCP client.</li> </ul> <p> </p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#1-install-hayhooks","title":"1. Install Hayhooks","text":"<pre><code># Install Hayhooks\npip install hayhooks\n</code></pre>"},{"location":"#2-start-hayhooks","title":"2. Start Hayhooks","text":"<pre><code>hayhooks run\n</code></pre>"},{"location":"#3-create-a-simple-agent","title":"3. Create a simple agent","text":"<p>Create a minimal agent wrapper with streaming chat support and a simple HTTP POST API:</p> <pre><code>from typing import AsyncGenerator\nfrom haystack.components.agents import Agent\nfrom haystack.dataclasses import ChatMessage\nfrom haystack.tools import Tool\nfrom haystack.components.generators.chat import OpenAIChatGenerator\nfrom hayhooks import BasePipelineWrapper, async_streaming_generator\n\n\n# Define a Haystack Tool that provides weather information for a given location.\ndef weather_function(location):\n    return f\"The weather in {location} is sunny.\"\n\nweather_tool = Tool(\n    name=\"weather_tool\",\n    description=\"Provides weather information for a given location.\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\"location\": {\"type\": \"string\"}},\n        \"required\": [\"location\"],\n    },\n    function=weather_function,\n)\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def setup(self) -&gt; None:\n        self.agent = Agent(\n            chat_generator=OpenAIChatGenerator(model=\"gpt-4o-mini\"),\n            system_prompt=\"You're a helpful agent\",\n            tools=[weather_tool],\n        )\n\n    # This will create a POST /my_agent/run endpoint\n    #\u00a0`question` will be the input argument and will be auto-validated by a Pydantic model\n    async def run_api_async(self, question: str) -&gt; str:\n        result = await self.agent.run_async(messages=[ChatMessage.from_user(question)])\n        return result[\"replies\"][0].text\n\n    # This will create an OpenAI-compatible /chat/completions endpoint\n    async def run_chat_completion_async(\n        self, model: str, messages: list[dict], body: dict\n    ) -&gt; AsyncGenerator[str, None]:\n        chat_messages = [\n            ChatMessage.from_openai_dict_format(message) for message in messages\n        ]\n\n        return async_streaming_generator(\n            pipeline=self.agent,\n            pipeline_run_args={\n                \"messages\": chat_messages,\n            },\n        )\n</code></pre> <p>Save as <code>my_agent_dir/pipeline_wrapper.py</code>.</p>"},{"location":"#4-deploy-it","title":"4. Deploy it","text":"<pre><code>hayhooks pipeline deploy-files -n my_agent ./my_agent_dir\n</code></pre>"},{"location":"#5-run-it","title":"5. Run it","text":"<p>Call the HTTP POST API (<code>/my_agent/run</code>):</p> <pre><code>curl -X POST http://localhost:1416/my_agent/run \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"question\": \"What can you do?\"}'\n</code></pre> <p>Call the OpenAI-compatible chat completion API (streaming enabled):</p> <pre><code>curl -X POST http://localhost:1416/chat/completions \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"model\": \"my_agent\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"What can you do?\"}]\n  }'\n</code></pre> <p>Or chat with it in the embedded Chainlit UI (<code>hayhooks run --with-chainlit</code>) or integrate it with Open WebUI!</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#easy-deployment","title":"\ud83d\ude80 Easy Deployment","text":"<ul> <li>Deploy Haystack pipelines and agents as REST APIs with minimal setup</li> <li>Support for both YAML-based and wrapper-based pipeline deployment</li> <li>Automatic OpenAI-compatible endpoint generation</li> </ul>"},{"location":"#multiple-integration-options","title":"\ud83c\udf10 Multiple Integration Options","text":"<ul> <li>MCP Protocol: Expose pipelines as MCP tools for use in AI development environments</li> <li>Chainlit UI: Embedded chat frontend with streaming, pipeline selection, and custom UI widgets</li> <li>Open WebUI Integration: Use Hayhooks as a backend for Open WebUI with streaming support</li> <li>OpenAI Compatibility: Seamless integration with OpenAI-compatible tools and frameworks</li> </ul>"},{"location":"#developer-friendly","title":"\ud83d\udd27 Developer Friendly","text":"<ul> <li>CLI for easy pipeline management</li> <li>Flexible configuration options</li> <li>Comprehensive logging and debugging support</li> <li>Custom route and middleware support</li> </ul>"},{"location":"#file-upload-support","title":"\ud83d\udcc1 File Upload Support","text":"<ul> <li>Built-in support for handling file uploads in pipelines</li> <li>Perfect for RAG systems and document processing</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Get started with Hayhooks</li> <li>Installation - Install Hayhooks and dependencies</li> <li>Configuration - Configure Hayhooks for your needs</li> <li>Examples - Explore example implementations</li> </ul>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub: deepset-ai/hayhooks</li> <li>Issues: GitHub Issues</li> <li>Documentation: Full Documentation</li> </ul> <p>Hayhooks is actively maintained by the deepset team.</p>"},{"location":"about/license/","title":"License","text":"<p>Hayhooks is open-source software released under the Apache License 2.0.</p>"},{"location":"about/license/#trademarks","title":"Trademarks","text":"<p>Haystack and the Haystack logo are trademarks of deepset GmbH. All other trademarks are the property of their respective owners.</p>"},{"location":"about/license/#contact","title":"Contact","text":"<p>For licensing questions or inquiries:</p> <ul> <li>Email: info@deepset.ai</li> <li>Website: https://deepset.ai</li> <li>GitHub: https://github.com/deepset-ai/hayhooks</li> <li>Contact form: https://www.deepset.ai/contact-us</li> <li>Phone number: +49 30 726210544</li> </ul>"},{"location":"about/license/#editorial-responsibility","title":"Editorial responsibility","text":"<p>deepset GmbH\\ Zinnowitzerstr. 1\\ 10115 Berlin, Germany\\ Court: Amtgericht Berlin-Charlottenburg\\ Handelsregister: HRB 197429 B\\ Managing Directors: Milos Rusic; Malte Pietsch\\ Umsatzsteuer-ID: DE319210575</p> <p>Authorized Information according to \u00a7 5 TMG (Telekommunikationsgesetz)\\ Malte Pietsch\\ deepset GmbH\\ Zinnowitzerstr. 1\\ 10115 Berlin\\ Germany</p> <p>Responsible for the Content According to \u00a7 55 RStV (Rundfunkstaatsvertrag)\\ Malte Pietsch\\ Milos Rusic</p>"},{"location":"about/license/#data-protection","title":"Data Protection","text":"<p>Data protection officer contact email:\\ dsb at secjur.com</p>"},{"location":"about/license/#disclaimer","title":"Disclaimer","text":"<p>Accountability for content The contents of our pages have been created with the utmost care. However, we cannot guarantee the contents' accuracy, completeness or topicality. According to statutory provisions, we are furthermore responsible for our own content on these web pages. In this context, please note that we are accordingly not obliged to monitor merely the transmitted or saved information of third parties, or investigate circumstances pointing to illegal activity. Our obligations to remove or block the use of information under generally applicable laws remain unaffected by this as per \u00a7\u00a7 8 to 10 of the Telemedia Act (TMG).</p> <p>Accountability for links Responsibility for the content of external links (to web pages of third parties) lies solely with the operators of the linked pages. No violations were evident to us at the time of linking. Should any legal infringement become known to us, we will remove the respective link immediately.</p>"},{"location":"about/license/#additional-resources","title":"Additional Resources","text":"<ul> <li>Apache License 2.0 FAQ</li> <li>Open Source Initiative</li> <li>Software Freedom Law Center</li> </ul>"},{"location":"advanced/advanced-configuration/","title":"Advanced Configuration","text":"<p>This guide covers programmatic customization, custom routes, and middleware for advanced Hayhooks usage.</p> <p>For basic configuration, see Configuration. For deployment and performance tuning, see Deployment Guidelines.</p>"},{"location":"advanced/advanced-configuration/#custom-routes-and-middleware","title":"Custom Routes and Middleware","text":""},{"location":"advanced/advanced-configuration/#when-to-add-custom-routes","title":"When to add custom routes","text":"<ul> <li>Add specialized endpoints for application-specific logic</li> <li>Provide admin/operations endpoints (restart, status, maintenance tasks)</li> <li>Expose health checks, metrics, and webhook handlers for integrations</li> <li>Implement authentication/authorization flows</li> <li>Offer file management or other utility endpoints</li> </ul>"},{"location":"advanced/advanced-configuration/#when-to-add-middleware","title":"When to add middleware","text":"<ul> <li>Apply cross-cutting concerns (logging/tracing, correlation IDs)</li> <li>Enforce security controls (authn/z, rate limiting, quotas)</li> <li>Control headers, CORS, compression, and caching</li> <li>Normalize inputs/outputs and error handling consistently</li> </ul>"},{"location":"advanced/advanced-configuration/#programmatic-customization","title":"Programmatic Customization","text":"<p>You can create a custom Hayhooks app instance to add routes or middleware:</p> <pre><code>import uvicorn\nfrom hayhooks.settings import settings\nfrom fastapi import Request\nfrom hayhooks import create_app\n\n# Create the Hayhooks app\nhayhooks = create_app()\n\n# Add a custom route\n@hayhooks.get(\"/custom\")\nasync def custom_route():\n    return {\"message\": \"Custom route\"}\n\n# Add custom middleware\n@hayhooks.middleware(\"http\")\nasync def custom_middleware(request: Request, call_next):\n    response = await call_next(request)\n    response.headers[\"X-Custom-Header\"] = \"value\"\n    return response\n\nif __name__ == \"__main__\":\n    uvicorn.run(\"app:hayhooks\", host=settings.host, port=settings.port)\n</code></pre> <p>This allows you to build custom applications with Hayhooks as the core engine while adding your own business logic and integrations.</p>"},{"location":"advanced/advanced-configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Deployment Guidelines - Performance tuning, workers, scaling, and deployment strategies</li> <li>Code Sharing - Reusable components across pipelines</li> </ul>"},{"location":"advanced/code-sharing/","title":"Code Sharing Between Wrappers","text":"<p>Hayhooks provides two ways to organize and share code in pipeline wrappers:</p> <ol> <li>Relative Imports - Import from sibling modules within the same pipeline folder</li> <li>Shared Python Path - Share code across multiple pipeline wrappers</li> </ol>"},{"location":"advanced/code-sharing/#relative-imports-recommended","title":"Relative Imports (Recommended)","text":"<p>Pipeline wrappers are loaded as Python packages, enabling you to use relative imports to organize your code into multiple files within the same pipeline folder.</p>"},{"location":"advanced/code-sharing/#structure","title":"Structure","text":"<pre><code>my_pipeline/\n\u251c\u2500\u2500 pipeline_wrapper.py    # Main wrapper\n\u251c\u2500\u2500 utils.py               # Helper functions\n\u251c\u2500\u2500 prompts.py             # Prompt templates\n\u2514\u2500\u2500 config.py              # Configuration\n</code></pre>"},{"location":"advanced/code-sharing/#usage","title":"Usage","text":"<p>Use Python's relative import syntax (<code>from .module import ...</code>):</p> <pre><code># pipeline_wrapper.py\nfrom haystack import Pipeline\nfrom hayhooks import BasePipelineWrapper\n\n# Relative imports from sibling modules\nfrom .utils import process_text, format_response\nfrom .prompts import SYSTEM_PROMPT\nfrom .config import DEFAULT_MODEL\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def setup(self) -&gt; None:\n        self.pipeline = Pipeline()\n        # ... setup using imported utilities\n\n    def run_api(self, query: str) -&gt; str:\n        processed = process_text(query)\n        result = self.pipeline.run({\"prompt\": {\"query\": processed}})\n        return format_response(result)\n</code></pre>"},{"location":"advanced/code-sharing/#benefits","title":"Benefits","text":"<ul> <li>No configuration needed - Works out of the box</li> <li>Clean organization - Split large wrappers into logical modules</li> <li>IDE support - Full autocomplete and type checking</li> <li>Tracing compatibility - Works with Phoenix/OpenInference and other tracing libraries</li> </ul> <p>Ruff Linting</p> <p>If your project uses ruff with the <code>flake8-tidy-imports</code> plugin, you may need to disable the <code>TID252</code> rule which bans relative imports. Add this comment at the top of your <code>pipeline_wrapper.py</code>:</p> <pre><code># ruff: noqa: TID252\n</code></pre> <p>Or configure it in your <code>pyproject.toml</code>:</p> <pre><code>[tool.ruff.lint.flake8-tidy-imports]\nban-relative-imports = \"parents\"  # Allow sibling relative imports\n</code></pre>"},{"location":"advanced/code-sharing/#example","title":"Example","text":"<p>See examples/pipeline_wrappers/relative_imports for a complete working example.</p>"},{"location":"advanced/code-sharing/#shared-python-path","title":"Shared Python Path","text":"<p>For sharing code across multiple pipeline wrappers, add a common folder to the Hayhooks Python path.</p>"},{"location":"advanced/code-sharing/#configuration","title":"Configuration","text":"<p>Set <code>HAYHOOKS_ADDITIONAL_PYTHON_PATH</code> to point to your shared code directory:</p> Environment Variable.env FileCLI Flag <pre><code>export HAYHOOKS_ADDITIONAL_PYTHON_PATH='./common'\nhayhooks run\n</code></pre> <pre><code># .env\nHAYHOOKS_ADDITIONAL_PYTHON_PATH=./common\n</code></pre> <pre><code>hayhooks run --additional-python-path ./common\n</code></pre>"},{"location":"advanced/code-sharing/#usage_1","title":"Usage","text":"<p>Once configured, import shared code in your wrappers:</p> <pre><code># In your pipeline_wrapper.py\nfrom my_custom_lib import sum_two_numbers\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def run_api(self, a: int, b: int) -&gt; int:\n        return sum_two_numbers(a, b)\n</code></pre>"},{"location":"advanced/code-sharing/#example_1","title":"Example","text":"<p>See examples/shared_code_between_wrappers for a complete working example.</p>"},{"location":"advanced/code-sharing/#choosing-the-right-approach","title":"Choosing the Right Approach","text":"Use Case Recommended Approach Splitting a large wrapper into modules Relative Imports Helpers specific to one pipeline Relative Imports Sharing utilities across many pipelines Shared Python Path Company-wide libraries Shared Python Path"},{"location":"advanced/running-pipelines/","title":"Running Pipelines","text":"<p>Execute deployed pipelines via CLI, HTTP API, or programmatically.</p>"},{"location":"advanced/running-pipelines/#quick-reference","title":"Quick Reference","text":"HayhooksCLIHayhooks HTTP APIPythonAsync PythonOpenAI Chat Completion (<code>curl</code>)Open WebUI <pre><code>hayhooks pipeline run my_pipeline --param 'query=\"What is Haystack?\"'\n</code></pre> <pre><code>curl -X POST http://localhost:1416/my_pipeline/run \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"query\":\"What is Haystack?\"}'\n</code></pre> <pre><code>import requests\n\nresp = requests.post(\n    \"http://localhost:1416/my_pipeline/run\",\n    json={\"query\": \"What is Haystack?\"}\n)\nprint(resp.json())\n</code></pre> <pre><code>import httpx\nimport asyncio\n\nasync def main():\n    async with httpx.AsyncClient() as client:\n        r = await client.post(\n            \"http://localhost:1416/my_pipeline/run\",\n            json={\"query\": \"What is Haystack?\"}\n        )\n        print(r.json())\n\nasyncio.run(main())\n</code></pre> <pre><code>curl -X POST http://localhost:1416/v1/chat/completions \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"model\": \"my_pipeline\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"What is Haystack?\"}]\n  }'\n</code></pre> <p>See Open WebUI Integration for setup details.</p> <p>For more CLI examples, see CLI Commands.</p>"},{"location":"advanced/running-pipelines/#file-uploads","title":"File Uploads","text":""},{"location":"advanced/running-pipelines/#cli","title":"CLI","text":"<pre><code># Single file\nhayhooks pipeline run my_pipeline --file document.pdf --param 'query=\"Summarize\"'\n\n# Multiple files\nhayhooks pipeline run my_pipeline --file doc1.pdf --file doc2.txt\n\n# Directory\nhayhooks pipeline run my_pipeline --dir ./documents\n</code></pre>"},{"location":"advanced/running-pipelines/#http","title":"HTTP","text":"<pre><code>curl -X POST http://localhost:1416/my_pipeline/run \\\n  -F 'files=@document.pdf' \\\n  -F 'query=\"Summarize this document\"'\n</code></pre> <p>See File Upload Support for implementation details.</p>"},{"location":"advanced/running-pipelines/#python-integration","title":"Python Integration","text":""},{"location":"advanced/running-pipelines/#requests","title":"Requests","text":"<pre><code>import requests\n\nresp = requests.post(\n    \"http://localhost:1416/my_pipeline/run\",\n    json={\"query\": \"What is Haystack?\"}\n)\nprint(resp.json())\n</code></pre>"},{"location":"advanced/running-pipelines/#async-httpx","title":"Async (httpx)","text":"<pre><code>import httpx\nimport asyncio\n\nasync def main():\n    async with httpx.AsyncClient() as client:\n        r = await client.post(\n            \"http://localhost:1416/my_pipeline/run\",\n            json={\"query\": \"What is Haystack?\"}\n        )\n        print(r.json())\n\nasyncio.run(main())\n</code></pre>"},{"location":"advanced/running-pipelines/#streaming","title":"Streaming","text":"<p>Implement <code>run_chat_completion</code> or <code>run_chat_completion_async</code> in your wrapper. See OpenAI Compatibility for details.</p>"},{"location":"advanced/running-pipelines/#error-handling-retry-logic","title":"Error Handling &amp; Retry Logic","text":"<pre><code>import requests\nfrom requests.exceptions import RequestException\nimport time\n\ndef run_with_retry(pipeline_name, params, max_retries=3):\n    url = f\"http://localhost:1416/{pipeline_name}/run\"\n\n    for attempt in range(max_retries):\n        try:\n            response = requests.post(url, json=params)\n            response.raise_for_status()\n            return response.json()\n        except RequestException as e:\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(2 ** attempt)  # Exponential backoff\n</code></pre>"},{"location":"advanced/running-pipelines/#logging","title":"Logging","text":"<p>Add logging to your pipeline wrappers:</p> <pre><code>from hayhooks import log\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def run_api(self, query: str) -&gt; str:\n        log.info(\"Processing query: {}\", query)\n        result = self.pipeline.run({\"prompt\": {\"query\": query}})\n        log.info(\"Pipeline completed\")\n        return result[\"llm\"][\"replies\"][0]\n</code></pre> <p>See Logging Reference for details.</p>"},{"location":"advanced/running-pipelines/#next-steps","title":"Next Steps","text":"<ul> <li>CLI Commands - Complete CLI reference</li> <li>Examples - Working examples</li> </ul>"},{"location":"concepts/agent-deployment/","title":"Agent Deployment","text":"<p>This page summarizes how to deploy Haystack Agents with Hayhooks and points you to the canonical examples.</p>"},{"location":"concepts/agent-deployment/#overview","title":"Overview","text":"<p>Agents are deployed using the same <code>PipelineWrapper</code> mechanism as pipelines. Implement <code>run_chat_completion</code> or <code>run_chat_completion_async</code> to expose OpenAI-compatible chat endpoints (with streaming support).</p>"},{"location":"concepts/agent-deployment/#quick-start","title":"Quick Start","text":"<p>Deploy agents using the same <code>PipelineWrapper</code> mechanism as pipelines. The key is implementing <code>run_chat_completion</code> or <code>run_chat_completion_async</code> for OpenAI-compatible chat endpoints with streaming support.</p> <p>See the example below for a complete agent setup with tools, streaming, and Open WebUI events.</p>"},{"location":"concepts/agent-deployment/#example","title":"Example","text":"<p>An agent deployment with tools, streaming, and Open WebUI events:</p>"},{"location":"concepts/agent-deployment/#agent-with-tool-call-interception-and-open-webui-events","title":"Agent with tool call interception and Open WebUI events","text":"<p>This example demonstrates:</p> <ul> <li>Agent setup with tools</li> <li>Async streaming chat completion</li> <li>Tool call lifecycle hooks (<code>on_tool_call_start</code>, <code>on_tool_call_end</code>)</li> <li>Open WebUI status events and notifications</li> </ul> <p>See the full file: open_webui_agent_on_tool_calls/pipeline_wrapper.py</p> <pre><code>import time\nfrom collections.abc import AsyncGenerator\n\nfrom haystack.components.agents import Agent\nfrom haystack.components.generators.chat import OpenAIChatGenerator\nfrom haystack.dataclasses import ChatMessage\nfrom haystack.tools import Tool\n\nfrom hayhooks import BasePipelineWrapper, async_streaming_generator\nfrom hayhooks.open_webui import (\n    OpenWebUIEvent,\n    create_details_tag,\n    create_notification_event,\n    create_status_event,\n)\n\n\ndef weather_function(location):\n    \"\"\"Mock weather API with a small delay\"\"\"\n    weather_info = {\n        \"Berlin\": {\"weather\": \"mostly sunny\", \"temperature\": 7, \"unit\": \"celsius\"},\n        \"Paris\": {\"weather\": \"mostly cloudy\", \"temperature\": 8, \"unit\": \"celsius\"},\n        \"Rome\": {\"weather\": \"sunny\", \"temperature\": 14, \"unit\": \"celsius\"},\n    }\n    time.sleep(3)\n    return weather_info.get(location, {\"weather\": \"unknown\", \"temperature\": 0, \"unit\": \"celsius\"})\n\n\nweather_tool = Tool(\n    name=\"weather_tool\",\n    description=\"Provides weather information for a given location.\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\"location\": {\"type\": \"string\"}},\n        \"required\": [\"location\"],\n    },\n    function=weather_function,\n)\n\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def setup(self) -&gt; None:\n        self.agent = Agent(\n            chat_generator=OpenAIChatGenerator(model=\"gpt-4o-mini\"),\n            system_prompt=\"You're a helpful agent\",\n            tools=[weather_tool],\n        )\n\n    def on_tool_call_start(\n        self,\n        tool_name: str,\n        arguments: dict,  # noqa: ARG002\n        id: str,  # noqa: ARG002, A002\n    ) -&gt; list[OpenWebUIEvent]:\n        return [\n            create_status_event(description=f\"Tool call started: {tool_name}\"),\n            create_notification_event(notification_type=\"info\", content=f\"Tool call started: {tool_name}\"),\n        ]\n\n    def on_tool_call_end(\n        self,\n        tool_name: str,\n        arguments: dict,\n        result: str,\n        error: bool,  # noqa: ARG002\n    ) -&gt; list[OpenWebUIEvent]:\n        return [\n            create_status_event(description=f\"Tool call ended: {tool_name}\", done=True),\n            create_notification_event(notification_type=\"success\", content=f\"Tool call ended: {tool_name}\"),\n            create_details_tag(\n                tool_name=tool_name,\n                summary=f\"Tool call result for {tool_name}\",\n                content=(f\"```\\nArguments:\\n{arguments}\\n\\nResponse:\\n{result}\\n```\"),\n            ),\n        ]\n\n    async def run_chat_completion_async(\n        self,\n        model: str,  # noqa: ARG002\n        messages: list[dict],\n        body: dict,  # noqa: ARG002\n    ) -&gt; AsyncGenerator[str, None]:\n        chat_messages = [ChatMessage.from_openai_dict_format(message) for message in messages]\n\n        return async_streaming_generator(\n            on_tool_call_start=self.on_tool_call_start,\n            on_tool_call_end=self.on_tool_call_end,\n            pipeline=self.agent,\n            pipeline_run_args={\"messages\": chat_messages},\n        )\n</code></pre>"},{"location":"concepts/agent-deployment/#next-steps","title":"Next Steps","text":"<ul> <li>PipelineWrapper Guide - Detailed implementation patterns</li> <li>Open WebUI Events Example - Interactive agent features</li> </ul>"},{"location":"concepts/pipeline-deployment/","title":"Pipeline Deployment","text":"<p>Hayhooks provides flexible options for deploying Haystack pipelines and agents. This section covers the core concepts of pipeline deployment.</p>"},{"location":"concepts/pipeline-deployment/#deployment-methods","title":"Deployment Methods","text":""},{"location":"concepts/pipeline-deployment/#1-pipelinewrapper-deployment-recommended","title":"1. PipelineWrapper Deployment (Recommended)","text":"<p>The most flexible approach using a <code>PipelineWrapper</code> class that encapsulates your pipeline logic. This method provides maximum flexibility for initialization, custom execution logic, OpenAI-compatible endpoint support, streaming capabilities, and file upload handling.</p> <p>See PipelineWrapper Details for complete implementation guide and examples.</p>"},{"location":"concepts/pipeline-deployment/#2-yaml-pipeline-deployment","title":"2. YAML Pipeline Deployment","text":"<p>Deploy pipelines directly from YAML definitions with automatic schema generation. This approach offers simple deployment from YAML files with automatic request/response schema generation, no wrapper code required, making it perfect for straightforward pipelines. The YAML must include <code>inputs</code> and <code>outputs</code> sections with properly defined pipeline components.</p> <p>For a complete YAML example and detailed requirements, see YAML Pipeline Deployment.</p> <p>YAML Pipeline Limitations</p> <p>YAML-deployed pipelines do not support OpenAI-compatible chat endpoints or streaming. For chat/streaming (e.g., Open WebUI), use a <code>PipelineWrapper</code> and implement <code>run_chat_completion</code>/<code>run_chat_completion_async</code> (see OpenAI Compatibility).</p>"},{"location":"concepts/pipeline-deployment/#core-components","title":"Core Components","text":""},{"location":"concepts/pipeline-deployment/#basepipelinewrapper-class","title":"BasePipelineWrapper Class","text":"<p>All pipeline wrappers inherit from <code>BasePipelineWrapper</code>:</p>"},{"location":"concepts/pipeline-deployment/#required-methods","title":"Required Methods","text":"<p><code>setup()</code> is called once when the pipeline is deployed to initialize your pipeline instance and set up any required resources.</p> <p><code>run_api()</code> is called for each API request to define your custom execution logic and return the pipeline result.</p>"},{"location":"concepts/pipeline-deployment/#optional-methods","title":"Optional Methods","text":"<p><code>run_api_async()</code> is the async version of <code>run_api()</code>, providing better performance for concurrent requests and supporting async pipeline execution.</p> <p><code>run_chat_completion()</code> enables OpenAI-compatible chat endpoints for Open WebUI integration with chat completion format support (see OpenAI Compatibility).</p> <p><code>run_chat_completion_async()</code> provides async chat completion with streaming support for chat interfaces and better performance for concurrent chat requests (see OpenAI Compatibility).</p>"},{"location":"concepts/pipeline-deployment/#inputoutput-handling","title":"Input/Output Handling","text":"<p>Hayhooks automatically handles request validation using Pydantic models, JSON serialization of responses, multipart/form-data requests for file uploads (see File Upload Support), and automatic type conversion between JSON and Python types.</p>"},{"location":"concepts/pipeline-deployment/#lifecycle-management","title":"Lifecycle Management","text":""},{"location":"concepts/pipeline-deployment/#pipeline-registration","title":"Pipeline Registration","text":"<p>When you deploy a pipeline, Hayhooks validates the wrapper implementation, creates the pipeline instance using <code>setup()</code>, registers the pipeline with the server, generates API endpoints and schemas, and creates OpenAI-compatible endpoints if implemented.</p>"},{"location":"concepts/pipeline-deployment/#pipeline-execution","title":"Pipeline Execution","text":"<p>For each request, Hayhooks validates the request against the schema, calls the appropriate method (<code>run_api</code>, <code>run_chat_completion</code>, etc.), handles errors and exceptions, and returns the response in the correct format.</p>"},{"location":"concepts/pipeline-deployment/#pipeline-undeployment","title":"Pipeline Undeployment","text":"<p>When you undeploy a pipeline, Hayhooks removes the pipeline from the registry, deletes the pipeline files if saved, unregisters all API endpoints, and cleans up resources.</p>"},{"location":"concepts/pipeline-deployment/#mcp-integration","title":"MCP Integration","text":"<p>All deployed pipelines can be exposed as MCP tools. Pipelines are automatically listed as available tools with input schemas generated from method signatures. Tools can be called from MCP clients (see MCP Support).</p>"},{"location":"concepts/pipeline-deployment/#next-steps","title":"Next Steps","text":"<ul> <li>PipelineWrapper Details - Learn about PipelineWrapper implementation</li> <li>YAML Pipeline Deployment - Deploy from YAML files</li> </ul>"},{"location":"concepts/pipeline-wrapper/","title":"PipelineWrapper","text":"<p>The <code>PipelineWrapper</code> class is the core component for deploying Haystack pipelines with Hayhooks. It provides maximum flexibility for pipeline initialization and execution.</p>"},{"location":"concepts/pipeline-wrapper/#why-pipelinewrapper","title":"Why PipelineWrapper?","text":"<p>The pipeline wrapper provides a flexible foundation for deploying Haystack pipelines, agents or any other component by allowing users to:</p> <ul> <li>Choose their preferred initialization method (YAML files, Haystack templates, or inline code)</li> <li>Define custom execution logic with configurable inputs and outputs</li> <li>Optionally expose OpenAI-compatible chat endpoints with streaming support for integration with interfaces like open-webui</li> </ul>"},{"location":"concepts/pipeline-wrapper/#basic-structure","title":"Basic Structure","text":"<pre><code>from collections.abc import AsyncGenerator, Generator\nfrom pathlib import Path\n\nfrom haystack import AsyncPipeline, Pipeline\n\nfrom hayhooks import BasePipelineWrapper, get_last_user_message, async_streaming_generator, streaming_generator\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def setup(self) -&gt; None:\n        pipeline_yaml = (Path(__file__).parent / \"pipeline.yml\").read_text()\n        self.pipeline = Pipeline.loads(pipeline_yaml)\n\n    def run_api(self, urls: list[str], question: str) -&gt; str:\n        result = self.pipeline.run({\"fetcher\": {\"urls\": urls}, \"prompt\": {\"query\": question}})\n        return result[\"llm\"][\"replies\"][0]\n</code></pre>"},{"location":"concepts/pipeline-wrapper/#required-methods","title":"Required Methods","text":""},{"location":"concepts/pipeline-wrapper/#setup","title":"setup()","text":"<p>The <code>setup()</code> method is called once when the pipeline is deployed. It should initialize the <code>self.pipeline</code> attribute as a Haystack pipeline.</p> <pre><code>def setup(self) -&gt; None:\n    # Initialize your pipeline here\n    pass\n</code></pre> <p>Initialization patterns:</p>"},{"location":"concepts/pipeline-wrapper/#1-programmatic-initialization-recommended","title":"1. Programmatic Initialization (Recommended)","text":"<p>Define your pipeline directly in code for maximum flexibility and control:</p> <pre><code>def setup(self) -&gt; None:\n    from haystack import Pipeline\n    from haystack.components.fetchers import LinkContentFetcher\n    from haystack.components.converters import HTMLToDocument\n    from haystack.components.builders import ChatPromptBuilder\n    from haystack.components.generators.chat import OpenAIChatGenerator\n\n    # Create components\n    fetcher = LinkContentFetcher()\n    converter = HTMLToDocument()\n    prompt_builder = ChatPromptBuilder(\n        template=\"\"\"{% message role=\"user\" %}\n        According to the contents of this website:\n        {% for document in documents %}\n            {{document.content}}\n        {% endfor %}\n        Answer the given question: {{query}}\n        {% endmessage %}\"\"\",\n        required_variables=\"*\"\n    )\n    llm = OpenAIChatGenerator(model=\"gpt-4o-mini\")\n\n    # Build pipeline\n    self.pipeline = Pipeline()\n    self.pipeline.add_component(\"fetcher\", fetcher)\n    self.pipeline.add_component(\"converter\", converter)\n    self.pipeline.add_component(\"prompt\", prompt_builder)\n    self.pipeline.add_component(\"llm\", llm)\n\n    # Connect components\n    self.pipeline.connect(\"fetcher.streams\", \"converter.sources\")\n    self.pipeline.connect(\"converter.documents\", \"prompt.documents\")\n    self.pipeline.connect(\"prompt.prompt\", \"llm.messages\")\n</code></pre> <p>Benefits of Programmatic Initialization</p> <ul> <li> Full IDE support with autocomplete and type checking</li> <li> Easier debugging and testing</li> <li> Better refactoring capabilities</li> <li> Dynamic component configuration based on runtime conditions</li> </ul>"},{"location":"concepts/pipeline-wrapper/#2-load-from-yaml","title":"2. Load from YAML","text":"<p>Load an existing YAML pipeline file:</p> <pre><code>def setup(self) -&gt; None:\n    from pathlib import Path\n    from haystack import Pipeline\n\n    pipeline_yaml = (Path(__file__).parent / \"pipeline.yml\").read_text()\n    self.pipeline = Pipeline.loads(pipeline_yaml)\n</code></pre> <p>When to use:</p> <ul> <li>You already have a YAML pipeline definition</li> <li>You want to version control pipeline structure separately</li> <li>You need to share pipeline definitions across different deployments</li> </ul> <p>Consider YAML-only deployment</p> <p>If your pipeline is simple and doesn't need custom logic, consider using YAML Pipeline Deployment instead, which doesn't require a wrapper at all.</p>"},{"location":"concepts/pipeline-wrapper/#run_api","title":"run_api()","text":"<p>The <code>run_api()</code> method is called for each API request to the <code>{pipeline_name}/run</code> endpoint.</p> <pre><code>def run_api(self, urls: list[str], question: str) -&gt; str:\n    result = self.pipeline.run({\"fetcher\": {\"urls\": urls}, \"prompt\": {\"query\": question}})\n    return result[\"llm\"][\"replies\"][0]\n</code></pre> <p>Key features:</p> <ul> <li>Flexible Input: You can define any input arguments you need</li> <li>Automatic Validation: Hayhooks creates Pydantic models for request validation</li> <li>Type Safety: Use proper type hints for better validation</li> <li>Error Handling: Implement proper error handling for production use</li> </ul> <p>Input argument rules:</p> <ul> <li>Arguments must be JSON-serializable</li> <li>Use proper type hints (<code>list[str]</code>, <code>int | None</code>, etc.)</li> <li>Default values are supported</li> <li>Complex types like <code>dict[str, Any]</code> are allowed</li> </ul>"},{"location":"concepts/pipeline-wrapper/#streaming-responses","title":"Streaming responses","text":"<p>Hayhooks can stream results from <code>run_api()</code> or <code>run_api_async()</code> when you return a generator instead of a fully materialized value. This is especially useful when you already rely on <code>streaming_generator()</code> / <code>async_streaming_generator()</code> inside chat endpoints and want the same behaviour on the generic <code>/run</code> route.</p> <pre><code>from collections.abc import Generator\nfrom hayhooks import streaming_generator\n\ndef run_api(self, query: str) -&gt; Generator:\n    return streaming_generator(\n        pipeline=self.pipeline,\n        pipeline_run_args={\"prompt\": {\"query\": query}},\n    )\n</code></pre> <p>For async pipelines:</p> <pre><code>from collections.abc import AsyncGenerator\nfrom hayhooks import async_streaming_generator\n\nasync def run_api_async(self, query: str) -&gt; AsyncGenerator:\n    return async_streaming_generator(\n        pipeline=self.pipeline,\n        pipeline_run_args={\"prompt\": {\"query\": query}},\n    )\n</code></pre> <p>When a generator is detected, Hayhooks automatically wraps it in a FastAPI <code>StreamingResponse</code> using the <code>text/plain</code> media type. The behaviour is identical for both <code>run_api()</code> and <code>run_api_async()</code>\u2014the only difference is whether the underlying generator is sync or async.</p> Method What you return Response media type Notes <code>run_api()</code> generator / <code>streaming_generator(...)</code> <code>text/plain</code> Works out of the box with existing clients (CLI, curl). <code>run_api_async()</code> async generator / <code>async_streaming_generator(...)</code> <code>text/plain</code> Same payload as sync version, emitted from an async task. Either <code>SSEStream(...)</code> around the generator <code>text/event-stream</code> Opt-in Server-Sent Events with automatic frame wrapping. <p>If you need SSE (for browsers, EventSource, or anything expecting <code>text/event-stream</code>), wrap the generator in <code>SSEStream(...)</code>. The wrapper is a tiny dataclass\u2014the only thing it does is annotate \u201cthis stream should be served as SSE\u201d, so you still write your pipeline logic exactly as before.</p> <pre><code>from hayhooks import SSEStream, streaming_generator\n\ndef run_api(self, query: str):\n    return SSEStream(\n        streaming_generator(\n            pipeline=self.pipeline,\n            pipeline_run_args={\"prompt\": {\"query\": query}},\n        )\n    )\n</code></pre> <p>For async pipelines:</p> <pre><code>from hayhooks import SSEStream, async_streaming_generator\n\nasync def run_api_async(self, query: str):\n    return SSEStream(\n        async_streaming_generator(\n            pipeline=self.pipeline,\n            pipeline_run_args={\"prompt\": {\"query\": query}},\n        )\n    )\n</code></pre>"},{"location":"concepts/pipeline-wrapper/#file-responses","title":"File responses","text":"<p>Hayhooks can return binary files (images, PDFs, audio, etc.) directly from <code>run_api()</code> when you return a FastAPI <code>Response</code> object instead of a JSON-serializable value. This is useful for pipelines that generate images, documents, or other binary content.</p> <pre><code>import tempfile\nfrom fastapi.responses import FileResponse\n\ndef run_api(self, prompt: str) -&gt; FileResponse:\n    image = self.generate_image(prompt)\n\n    with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmp:\n        image.save(tmp, format=\"PNG\")\n\n    return FileResponse(path=tmp.name, media_type=\"image/png\", filename=\"result.png\")\n</code></pre> <p>Any <code>Response</code> subclass works \u2014 <code>FileResponse</code> for files on disk, <code>Response</code> for in-memory bytes, or <code>StreamingResponse</code> for large content. When Hayhooks detects a <code>Response</code> return type, it registers the endpoint with <code>response_model=None</code> (skipping JSON schema generation) and returns the response directly at runtime.</p> Method What you return Response media type Notes <code>run_api()</code> <code>FileResponse</code> Set by <code>media_type</code> Serves a file from disk. <code>run_api()</code> <code>Response(content=bytes)</code> Set by <code>media_type</code> Returns in-memory binary content. <code>run_api()</code> <code>StreamingResponse</code> Set by <code>media_type</code> Streams large content on the fly. <p>For a full working example, see the Image Generation example and the File Response Support feature documentation.</p>"},{"location":"concepts/pipeline-wrapper/#optional-methods","title":"Optional Methods","text":""},{"location":"concepts/pipeline-wrapper/#run_api_async","title":"run_api_async()","text":"<p>The asynchronous version of <code>run_api()</code> for better performance under high load.</p> <pre><code>async def run_api_async(self, urls: list[str], question: str) -&gt; str:\n    result = await self.pipeline.run_async({\"fetcher\": {\"urls\": urls}, \"prompt\": {\"query\": question}})\n    return result[\"llm\"][\"replies\"][0]\n</code></pre> <p>When to use <code>run_api_async</code>:</p> <ul> <li>Working with <code>AsyncPipeline</code> instances</li> <li>Handling many concurrent requests</li> <li>Integrating with async-compatible components</li> <li>Better performance for I/O-bound operations</li> </ul>"},{"location":"concepts/pipeline-wrapper/#run_chat_completion","title":"run_chat_completion()","text":"<p>Enable OpenAI-compatible chat endpoints for integration with chat interfaces.</p> <pre><code>def run_chat_completion(self, model: str, messages: list[dict], body: dict) -&gt; str | Generator:\n    question = get_last_user_message(messages)\n    result = self.pipeline.run({\"prompt\": {\"query\": question}})\n    return result[\"llm\"][\"replies\"][0]\n</code></pre> <p>Fixed signature:</p> <ul> <li><code>model</code>: The pipeline name</li> <li><code>messages</code>: OpenAI-format message list</li> <li><code>body</code>: Full request body (for additional parameters)</li> </ul>"},{"location":"concepts/pipeline-wrapper/#run_chat_completion_async","title":"run_chat_completion_async()","text":"<p>Async version of chat completion with streaming support.</p> <pre><code>async def run_chat_completion_async(self, model: str, messages: list[dict], body: dict) -&gt; AsyncGenerator:\n    question = get_last_user_message(messages)\n    return async_streaming_generator(\n        pipeline=self.pipeline,\n        pipeline_run_args={\"prompt\": {\"query\": question}},\n    )\n</code></pre>"},{"location":"concepts/pipeline-wrapper/#hybrid-streaming-mixing-async-and-sync-components","title":"Hybrid Streaming: Mixing Async and Sync Components","text":"<p>Compatibility for Legacy Components</p> <p>When working with legacy pipelines or components that only support sync streaming callbacks (like <code>OpenAIGenerator</code>), use <code>allow_sync_streaming_callbacks=True</code> to enable hybrid mode. For new code, prefer async-compatible components and use the default strict mode.</p> <p>Some Haystack components only support synchronous streaming callbacks and don't have async equivalents. Examples include:</p> <ul> <li><code>OpenAIGenerator</code> - Legacy OpenAI text generation (\u26a0\ufe0f Note: <code>OpenAIChatGenerator</code> IS async-compatible)</li> <li>Other components without <code>run_async()</code> support</li> </ul>"},{"location":"concepts/pipeline-wrapper/#the-problem","title":"The Problem","text":"<p>By default, <code>async_streaming_generator</code> requires all streaming components to support async callbacks:</p> <pre><code>async def run_chat_completion_async(self, model: str, messages: list[dict], body: dict) -&gt; AsyncGenerator:\n    # This will FAIL if pipeline contains OpenAIGenerator\n    return async_streaming_generator(\n        pipeline=self.pipeline,  # AsyncPipeline with OpenAIGenerator\n        pipeline_run_args={\"prompt\": {\"query\": question}},\n    )\n</code></pre> <p>Error:</p> <pre><code>ValueError: Component 'llm' of type 'OpenAIGenerator' seems to not support\nasync streaming callbacks...\n</code></pre>"},{"location":"concepts/pipeline-wrapper/#the-solution-hybrid-streaming-mode","title":"The Solution: Hybrid Streaming Mode","text":"<p>Enable hybrid streaming mode to automatically handle both async and sync components:</p> <pre><code>async def run_chat_completion_async(self, model: str, messages: list[dict], body: dict) -&gt; AsyncGenerator:\n    question = get_last_user_message(messages)\n    return async_streaming_generator(\n        pipeline=self.pipeline,\n        pipeline_run_args={\"prompt\": {\"query\": question}},\n        allow_sync_streaming_callbacks=True  # \u2705 Auto-detect and enable hybrid mode\n    )\n</code></pre>"},{"location":"concepts/pipeline-wrapper/#what-allow_sync_streaming_callbackstrue-does","title":"What <code>allow_sync_streaming_callbacks=True</code> Does","text":"<p>When you set <code>allow_sync_streaming_callbacks=True</code>, the system enables intelligent auto-detection:</p> <ol> <li>Scans Components: Automatically inspects all streaming components in your pipeline</li> <li>Detects Capabilities: Checks if each component has <code>run_async()</code> support</li> <li>Enables Hybrid Mode Only If Needed:</li> <li>\u2705 If all components support async \u2192 Uses pure async mode (no overhead)</li> <li>\u2705 If any component is sync-only \u2192 Automatically enables hybrid mode</li> <li>Bridges Sync to Async: For sync-only components, wraps their callbacks to work seamlessly with the async event loop</li> <li>Zero Configuration: You don't need to know which components are sync/async - it figures it out automatically</li> </ol> <p>Smart Behavior</p> <p>Setting <code>allow_sync_streaming_callbacks=True</code> does NOT force hybrid mode. It only enables it when actually needed. If your pipeline is fully async-capable, you get pure async performance with no overhead!</p>"},{"location":"concepts/pipeline-wrapper/#configuration-options","title":"Configuration Options","text":"<pre><code># Option 1: Strict mode (Default - Recommended)\nallow_sync_streaming_callbacks=False\n# \u2192 Raises error if sync-only components found\n# \u2192 Best for: New code, ensuring proper async components, best performance\n\n# Option 2: Auto-detection (Compatibility mode)\nallow_sync_streaming_callbacks=True\n# \u2192 Automatically detects and enables hybrid mode only when needed\n# \u2192 Best for: Legacy pipelines, components without async support, gradual migration\n</code></pre>"},{"location":"concepts/pipeline-wrapper/#example-legacy-openai-generator-with-async-pipeline","title":"Example: Legacy OpenAI Generator with Async Pipeline","text":"<pre><code>from collections.abc import AsyncGenerator\n\nfrom haystack import AsyncPipeline\nfrom haystack.components.builders import PromptBuilder\nfrom haystack.components.generators import OpenAIGenerator\nfrom haystack.utils import Secret\nfrom hayhooks import BasePipelineWrapper, get_last_user_message, async_streaming_generator\n\nclass LegacyOpenAIWrapper(BasePipelineWrapper):\n    def setup(self) -&gt; None:\n        # OpenAIGenerator only supports sync streaming (legacy component)\n        llm = OpenAIGenerator(\n            api_key=Secret.from_env_var(\"OPENAI_API_KEY\"),\n            model=\"gpt-4o-mini\"\n        )\n\n        prompt_builder = PromptBuilder(\n            template=\"Answer this question: {{question}}\"\n        )\n\n        self.pipeline = AsyncPipeline()\n        self.pipeline.add_component(\"prompt\", prompt_builder)\n        self.pipeline.add_component(\"llm\", llm)\n        self.pipeline.connect(\"prompt.prompt\", \"llm.prompt\")\n\n    async def run_chat_completion_async(\n        self, model: str, messages: list[dict], body: dict\n    ) -&gt; AsyncGenerator:\n        question = get_last_user_message(messages)\n\n        # Enable hybrid mode for OpenAIGenerator\n        return async_streaming_generator(\n            pipeline=self.pipeline,\n            pipeline_run_args={\"prompt\": {\"question\": question}},\n            allow_sync_streaming_callbacks=True  # \u2705 Handles sync component\n        )\n</code></pre>"},{"location":"concepts/pipeline-wrapper/#when-to-use-each-mode","title":"When to Use Each Mode","text":"<p>Use strict mode (default) when:</p> <ul> <li>Building new pipelines (recommended default)</li> <li>You want to ensure all components are async-compatible</li> <li>Performance is critical (pure async is ~1-2\u03bcs faster per chunk)</li> <li>You're building a production system with controlled dependencies</li> </ul> <p>Use <code>allow_sync_streaming_callbacks=True</code> when:</p> <ul> <li>Working with legacy pipelines that use <code>OpenAIGenerator</code> or other sync-only components</li> <li>Deploying YAML pipelines with unknown/legacy component types</li> <li>Migrating old code that doesn't have async equivalents yet</li> <li>Third-party components without async support</li> </ul>"},{"location":"concepts/pipeline-wrapper/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Pure async pipeline: No overhead</li> <li>Hybrid mode (auto-detected): Minimal overhead (~1-2 microseconds per streaming chunk for sync components)</li> <li>Network-bound operations: The overhead is negligible compared to LLM generation time</li> </ul> <p>Best Practice</p> <p>For new code: Use the default strict mode (<code>allow_sync_streaming_callbacks=False</code>) to ensure you're using proper async components.</p> <p>For legacy/compatibility: Use <code>allow_sync_streaming_callbacks=True</code> when working with older pipelines or components that don't support async streaming yet.</p>"},{"location":"concepts/pipeline-wrapper/#streaming-from-multiple-components","title":"Streaming from Multiple Components","text":"<p>Smart Streaming Behavior</p> <p>By default, Hayhooks streams only the last streaming-capable component in your pipeline. This is usually what you want - the final output streaming to users.</p> <p>For advanced use cases, you can control which components stream using the <code>streaming_components</code> parameter.</p> <p>When your pipeline contains multiple components that support streaming (e.g., multiple LLMs), you can control which ones stream their outputs as the pipeline executes.</p>"},{"location":"concepts/pipeline-wrapper/#default-behavior-stream-only-the-last-component","title":"Default Behavior: Stream Only the Last Component","text":"<p>By default, only the last streaming-capable component will stream:</p> <pre><code>class MultiLLMWrapper(BasePipelineWrapper):\n    def setup(self) -&gt; None:\n        from haystack.components.builders import ChatPromptBuilder\n        from haystack.components.generators.chat import OpenAIChatGenerator\n        from haystack.dataclasses import ChatMessage\n\n        self.pipeline = Pipeline()\n\n        # First LLM - initial answer\n        self.pipeline.add_component(\n            \"prompt_1\",\n            ChatPromptBuilder(\n                template=[\n                    ChatMessage.from_system(\"You are a helpful assistant.\"),\n                    ChatMessage.from_user(\"{{query}}\")\n                ]\n            )\n        )\n        self.pipeline.add_component(\"llm_1\", OpenAIChatGenerator(model=\"gpt-4o-mini\"))\n\n        # Second LLM - refines the answer using Jinja2 to access ChatMessage attributes\n        self.pipeline.add_component(\n            \"prompt_2\",\n            ChatPromptBuilder(\n                template=[\n                    ChatMessage.from_system(\"You are a helpful assistant that refines responses.\"),\n                    ChatMessage.from_user(\n                        \"Previous response: {{previous_response[0].text}}\\n\\nRefine this.\"\n                    )\n                ]\n            )\n        )\n        self.pipeline.add_component(\"llm_2\", OpenAIChatGenerator(model=\"gpt-4o-mini\"))\n\n        # Connect components - LLM 1's replies go directly to prompt_2\n        self.pipeline.connect(\"prompt_1.prompt\", \"llm_1.messages\")\n        self.pipeline.connect(\"llm_1.replies\", \"prompt_2.previous_response\")\n        self.pipeline.connect(\"prompt_2.prompt\", \"llm_2.messages\")\n\n    def run_chat_completion(self, model: str, messages: list[dict], body: dict) -&gt; Generator:\n        question = get_last_user_message(messages)\n\n        # By default, only llm_2 (the last streaming component) will stream\n        return streaming_generator(\n            pipeline=self.pipeline,\n            pipeline_run_args={\"prompt_1\": {\"query\": question}}\n        )\n</code></pre> <p>What happens: Only <code>llm_2</code> (the last streaming-capable component) streams its responses token by token. The first LLM (<code>llm_1</code>) executes normally without streaming, and only the final refined output streams to the user.</p>"},{"location":"concepts/pipeline-wrapper/#advanced-stream-multiple-components-with-streaming_components","title":"Advanced: Stream Multiple Components with <code>streaming_components</code>","text":"<p>For advanced use cases where you want to see outputs from multiple components, use the <code>streaming_components</code> parameter:</p> <pre><code>def run_chat_completion(self, model: str, messages: list[dict], body: dict) -&gt; Generator:\n    question = get_last_user_message(messages)\n\n    # Enable streaming for BOTH LLMs\n    return streaming_generator(\n        pipeline=self.pipeline,\n        pipeline_run_args={\"prompt_1\": {\"query\": question}},\n        streaming_components=[\"llm_1\", \"llm_2\"]  # Stream both components\n    )\n</code></pre> <p>What happens: Both LLMs stream their responses token by token. First you'll see the initial answer from <code>llm_1</code> streaming, then the refined answer from <code>llm_2</code> streaming.</p> <p>You can also selectively enable streaming for specific components:</p> <pre><code># Stream only the first LLM\nstreaming_components=[\"llm_1\"]\n\n# Stream only the second LLM (same as default)\nstreaming_components=[\"llm_2\"]\n\n# Stream ALL capable components (shorthand)\nstreaming_components=\"all\"\n\n#\u00a0Stream ALL capable components (specific list)\nstreaming_components=[\"llm_1\", \"llm_2\"]\n</code></pre>"},{"location":"concepts/pipeline-wrapper/#using-the-all-keyword","title":"Using the \"all\" Keyword","text":"<p>The <code>\"all\"</code> keyword is a convenient shorthand to enable streaming for all capable components:</p> <pre><code>return streaming_generator(\n    pipeline=self.pipeline,\n    pipeline_run_args={...},\n    streaming_components=\"all\"  # Enable all streaming components\n)\n</code></pre> <p>This is equivalent to explicitly enabling every streaming-capable component in your pipeline.</p>"},{"location":"concepts/pipeline-wrapper/#global-configuration-via-environment-variable","title":"Global Configuration via Environment Variable","text":"<p>You can set a global default using the <code>HAYHOOKS_STREAMING_COMPONENTS</code> environment variable. This applies to all pipelines unless overridden:</p> <pre><code># Stream all components by default\nexport HAYHOOKS_STREAMING_COMPONENTS=\"all\"\n\n# Stream specific components (comma-separated)\nexport HAYHOOKS_STREAMING_COMPONENTS=\"llm_1,llm_2\"\n</code></pre> <p>Priority order:</p> <ol> <li>Explicit <code>streaming_components</code> parameter (highest priority)</li> <li><code>HAYHOOKS_STREAMING_COMPONENTS</code> environment variable</li> <li>Default behavior: stream only last component (lowest priority)</li> </ol> <p>When to Use Each Approach</p> <ul> <li>Default (last component only): Best for most use cases - users see only the final output</li> <li>\"all\" keyword: Useful for debugging, demos, or transparent multi-step workflows</li> <li>List of components: Enable multiple specific components by name</li> <li>Environment variable: For deployment-wide defaults without code changes</li> </ul> <p>Async Streaming</p> <p>All streaming_components options work identically with <code>async_streaming_generator()</code> for async pipelines.</p>"},{"location":"concepts/pipeline-wrapper/#yaml-pipeline-streaming-configuration","title":"YAML Pipeline Streaming Configuration","text":"<p>You can also specify streaming configuration in YAML pipeline definitions:</p> <pre><code>components:\n  prompt_1:\n    init_parameters:\n      required_variables: \"*\"\n      template: |\n        {% message role=\"user\" %}\n        Answer this question: {{query}}\n        Answer:\n        {% endmessage %}\n    type: haystack.components.builders.chat_prompt_builder.ChatPromptBuilder\n\n  llm_1:\n    init_parameters:\n      ...\n    type: haystack.components.generators.chat.openai.OpenAIChatGenerator\n\n  prompt_2:\n    init_parameters:\n      required_variables: \"*\"\n      template: |\n        {% message role=\"user\" %}\n        Refine this response: {{previous_reply[0].text}}\n        Improved answer:\n        {% endmessage %}\n    type: haystack.components.builders.chat_prompt_builder.ChatPromptBuilder\n\n  llm_2:\n    init_parameters:\n      ...\n    type: haystack.components.generators.chat.openai.OpenAIChatGenerator\n\nconnections:\n  - receiver: llm_1.messages\n    sender: prompt_1.prompt\n  - receiver: prompt_2.previous_reply\n    sender: llm_1.replies\n  - receiver: llm_2.messages\n    sender: prompt_2.prompt\n\nmetadata: {}\n\ninputs:\n  query: prompt_1.query\n\noutputs:\n  replies: llm_2.replies\n\n# Option 1: List specific components\nstreaming_components:\n  - llm_1\n  - llm_2\n\n# Option 2: Stream all components\n# streaming_components: all\n</code></pre> <p>YAML configuration follows the same priority rules: YAML setting &gt; environment variable &gt; default.</p> <p>See the Multi-LLM Streaming Example for a complete working implementation.</p>"},{"location":"concepts/pipeline-wrapper/#accessing-intermediate-outputs-with-include_outputs_from","title":"Accessing Intermediate Outputs with <code>include_outputs_from</code>","text":"<p>Understanding Pipeline Outputs</p> <p>By default, Haystack pipelines only return outputs from leaf components (final components with no downstream connections). Use <code>include_outputs_from</code> to also get outputs from intermediate components like retrievers, preprocessors, or parallel branches.</p>"},{"location":"concepts/pipeline-wrapper/#streaming-with-on_pipeline_end-callback","title":"Streaming with <code>on_pipeline_end</code> Callback","text":"<p>For streaming responses, pass <code>include_outputs_from</code> to <code>streaming_generator()</code> or <code>async_streaming_generator()</code>, and use the <code>on_pipeline_end</code> callback to access intermediate outputs. For example:</p> <pre><code>    def run_chat_completion(self, model: str, messages: List[dict], body: dict) -&gt; Generator:\n        question = get_last_user_message(messages)\n\n        # Store retrieved documents for citations\n        self.retrieved_docs = []\n\n        def on_pipeline_end(result: dict[str, Any]) -&gt; None:\n            # Access intermediate outputs here\n            if \"retriever\" in result:\n                self.retrieved_docs = result[\"retriever\"][\"documents\"]\n                # Use for citations, logging, analytics, etc.\n\n        return streaming_generator(\n            pipeline=self.pipeline,\n            pipeline_run_args={\n                \"retriever\": {\"query\": question},\n                \"prompt_builder\": {\"query\": question}\n            },\n            include_outputs_from={\"retriever\"},  # Make retriever outputs available\n            on_pipeline_end=on_pipeline_end\n        )\n</code></pre> <p>What happens: The <code>on_pipeline_end</code> callback receives both <code>llm</code> and <code>retriever</code> outputs in the <code>result</code> dict, allowing you to access retrieved documents alongside the generated response.</p> <p>The same pattern works with async streaming:</p> <pre><code>async def run_chat_completion_async(self, model: str, messages: List[dict], body: dict) -&gt; AsyncGenerator:\n    question = get_last_user_message(messages)\n\n    def on_pipeline_end(result: dict[str, Any]) -&gt; None:\n        if \"retriever\" in result:\n            self.retrieved_docs = result[\"retriever\"][\"documents\"]\n\n    return async_streaming_generator(\n        pipeline=self.async_pipeline,\n        pipeline_run_args={\n            \"retriever\": {\"query\": question},\n            \"prompt_builder\": {\"query\": question}\n        },\n        include_outputs_from={\"retriever\"},\n        on_pipeline_end=on_pipeline_end\n    )\n</code></pre>"},{"location":"concepts/pipeline-wrapper/#non-streaming-api","title":"Non-Streaming API","text":"<p>For non-streaming <code>run_api</code> or <code>run_api_async</code> endpoints, pass <code>include_outputs_from</code> directly to <code>pipeline.run()</code> or <code>pipeline.run_async()</code>. For example:</p> <pre><code>def run_api(self, query: str) -&gt; dict:\n    result = self.pipeline.run(\n        data={\"retriever\": {\"query\": query}},\n        include_outputs_from={\"retriever\"}\n    )\n    # Build custom response with both answer and sources\n    return {\"answer\": result[\"llm\"][\"replies\"][0], \"sources\": result[\"retriever\"][\"documents\"]}\n</code></pre> <p>Same pattern for async:</p> <pre><code>async def run_api_async(self, query: str) -&gt; dict:\n    result = await self.async_pipeline.run_async(\n        data={\"retriever\": {\"query\": query}},\n        include_outputs_from={\"retriever\"}\n    )\n    return {\"answer\": result[\"llm\"][\"replies\"][0], \"sources\": result[\"retriever\"][\"documents\"]}\n</code></pre> <p>When to Use <code>include_outputs_from</code></p> <ul> <li>Streaming: Pass <code>include_outputs_from</code> to <code>streaming_generator()</code> or <code>async_streaming_generator()</code> and use <code>on_pipeline_end</code> callback to access the outputs</li> <li>Non-streaming: Pass <code>include_outputs_from</code> directly to <code>pipeline.run()</code> or <code>pipeline.run_async()</code></li> <li>YAML Pipelines: Automatically handled - see YAML Pipeline Deployment</li> </ul>"},{"location":"concepts/pipeline-wrapper/#external-event-queue","title":"External Event Queue","text":"<p>Both <code>streaming_generator</code> and <code>async_streaming_generator</code> support an optional <code>external_event_queue</code> parameter. This allows you to inject custom events (<code>dict</code>, <code>OpenWebUIEvent</code>, <code>str</code>, or <code>StreamingChunk</code>) into the streaming output alongside pipeline chunks. The external queue is checked before the internal queue in each iteration cycle, ensuring external events are delivered promptly.</p> <p>Typical Use Case</p> <p>A typical use case is passing the queue to pipeline components that need to emit events during execution\u2014for example, a Human-in-the-Loop (HITL) confirmation strategy that pushes approval request events to the queue while waiting for user input.</p>"},{"location":"concepts/pipeline-wrapper/#file-upload-support","title":"File Upload Support","text":"<p>Hayhooks can handle file uploads by adding a <code>files</code> parameter:</p> <pre><code>from fastapi import UploadFile\n\ndef run_api(self, files: list[UploadFile] | None = None, query: str = \"\") -&gt; str:\n    if files:\n        # Process uploaded files\n        filenames = [f.filename for f in files if f.filename]\n        file_contents = [f.file.read() for f in files]\n        return f\"Processed {len(files)} files: {', '.join(filenames)}\"\n\n    return \"No files uploaded\"\n</code></pre>"},{"location":"concepts/pipeline-wrapper/#pipelinewrapper-development","title":"PipelineWrapper Development","text":""},{"location":"concepts/pipeline-wrapper/#during-development","title":"During Development","text":"<p>Use the <code>--overwrite</code> flag for rapid development:</p> <pre><code>hayhooks pipeline deploy-files -n my_pipeline --overwrite ./path/to/pipeline\n</code></pre> <p>Development workflow:</p> <ol> <li>Make changes to your pipeline wrapper</li> <li>Redeploy with <code>--overwrite</code></li> <li>Test the changes</li> <li>Repeat as needed</li> </ol>"},{"location":"concepts/pipeline-wrapper/#for-even-faster-iterations","title":"For even faster iterations","text":"<p>Combine <code>--overwrite</code> with <code>--skip-saving-files</code>:</p> <pre><code>hayhooks pipeline deploy-files -n my_pipeline --overwrite --skip-saving-files ./path/to/pipeline\n</code></pre> <p>This avoids writing files to disk and speeds up development.</p>"},{"location":"concepts/pipeline-wrapper/#additional-dependencies","title":"Additional Dependencies","text":"<p>Your pipeline wrapper may require additional dependencies:</p> <pre><code># pipeline_wrapper.py\nimport trafilatura  # Additional dependency\n\ndef run_api(self, urls: list[str], question: str) -&gt; str:\n    # Use additional library\n    content = trafilatura.fetch(urls[0])\n    # ... rest of pipeline logic\n</code></pre> <p>Install dependencies:</p> <pre><code>pip install trafilatura\n</code></pre> <p>Debugging tip: Enable tracebacks to see full error messages:</p> <pre><code>HAYHOOKS_SHOW_TRACEBACKS=true hayhooks run\n</code></pre>"},{"location":"concepts/pipeline-wrapper/#error-handling","title":"Error Handling","text":"<p>Implement proper error handling in production:</p> <pre><code>from hayhooks import log\nfrom fastapi import HTTPException\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def setup(self) -&gt; None:\n        try:\n            self.pipeline = self._create_pipeline()\n        except Exception as e:\n            log.error(\"Failed to initialize pipeline: {}\", e)\n            raise\n\n    def run_api(self, query: str) -&gt; str:\n        if not query or not query.strip():\n            raise HTTPException(status_code=400, detail=\"Query cannot be empty\")\n\n        try:\n            result = self.pipeline.run({\"prompt\": {\"query\": query}})\n            return result[\"llm\"][\"replies\"][0]\n        except Exception as e:\n            log.error(\"Pipeline execution failed: {}\", e)\n            raise HTTPException(status_code=500, detail=\"Pipeline execution failed\")\n</code></pre>"},{"location":"concepts/pipeline-wrapper/#mcp-tool-configuration","title":"MCP Tool Configuration","text":""},{"location":"concepts/pipeline-wrapper/#skip-mcp-tool-listing","title":"Skip MCP Tool Listing","text":"<p>To skip MCP tool registration:</p> <pre><code>class PipelineWrapper(BasePipelineWrapper):\n    skip_mcp = True  # This pipeline won't be listed as an MCP tool\n\n    def setup(self) -&gt; None:\n        ...\n\n    def run_api(self, ...) -&gt; str:\n        ...\n</code></pre>"},{"location":"concepts/pipeline-wrapper/#mcp-tool-description","title":"MCP Tool Description","text":"<p>Use docstrings to provide MCP tool descriptions:</p> <pre><code>def run_api(self, urls: list[str], question: str) -&gt; str:\n    \"\"\"\n    Ask questions about website content.\n\n    Args:\n        urls: List of website URLs to analyze\n        question: Question to ask about the content\n\n    Returns:\n        Answer to the question based on the website content\n    \"\"\"\n    result = self.pipeline.run({\"fetcher\": {\"urls\": urls}, \"prompt\": {\"query\": question}})\n    return result[\"llm\"][\"replies\"][0]\n</code></pre>"},{"location":"concepts/pipeline-wrapper/#examples","title":"Examples","text":"<p>For complete, working examples see:</p> <ul> <li>Chat with Website (Streaming) - Pipeline with streaming chat completion support</li> <li>Async Question Answer - Async pipeline patterns with streaming</li> <li>Image Generation (File Response) - Returning binary files (images) from <code>run_api</code></li> <li>RAG Indexing &amp; Query - Complete RAG system with file uploads and Elasticsearch</li> </ul>"},{"location":"concepts/pipeline-wrapper/#next-steps","title":"Next Steps","text":"<ul> <li>YAML Pipeline Deployment - Alternative deployment method</li> <li>Agent Deployment - Deploy Haystack agents</li> </ul>"},{"location":"concepts/yaml-pipeline-deployment/","title":"YAML Pipeline Deployment","text":"<p>Hayhooks supports deploying Haystack pipelines directly from YAML definitions. This approach builds request/response schemas automatically from the YAML-declared <code>inputs</code> and <code>outputs</code>.</p>"},{"location":"concepts/yaml-pipeline-deployment/#overview","title":"Overview","text":"<p>YAML pipeline deployment is ideal for:</p> <ul> <li>Simple pipelines with clear inputs and outputs</li> <li>Quick deployment without wrapper code</li> <li>Automatic schema generation</li> <li>CI/CD pipeline deployments</li> </ul> <p>Converting Existing Pipelines</p> <p>If you already have a Haystack <code>Pipeline</code> instance, you can serialize it with <code>pipeline.dumps()</code> and then manually add the required <code>inputs</code> and <code>outputs</code> sections before deploying.</p>"},{"location":"concepts/yaml-pipeline-deployment/#requirements","title":"Requirements","text":""},{"location":"concepts/yaml-pipeline-deployment/#yaml-structure","title":"YAML Structure","text":"<p>Your YAML file must include both <code>inputs</code> and <code>outputs</code> sections:</p> <pre><code>components:\n  converter:\n    type: haystack.components.converters.html.HTMLToDocument\n    init_parameters:\n      extraction_kwargs: null\n\n  fetcher:\n    init_parameters:\n      raise_on_failure: true\n      retry_attempts: 2\n      timeout: 3\n      user_agents:\n        - haystack/LinkContentFetcher/2.0.0b8\n    type: haystack.components.fetchers.link_content.LinkContentFetcher\n\n  llm:\n    init_parameters:\n      api_key:\n        env_vars:\n          - OPENAI_API_KEY\n        strict: true\n        type: env_var\n      generation_kwargs: {}\n      model: gpt-4o-mini\n    type: haystack.components.generators.chat.openai.OpenAIChatGenerator\n\n  prompt:\n    init_parameters:\n      template: |\n        {% message role=\"user\" %}\n        According to the contents of this website:\n        {% for document in documents %}\n          {{document.content}}\n        {% endfor %}\n        Answer the given question: {{query}}\n        {% endmessage %}\n      required_variables: \"*\"\n    type: haystack.components.builders.chat_prompt_builder.ChatPromptBuilder\n\nconnections:\n  - receiver: converter.sources\n    sender: fetcher.streams\n  - receiver: prompt.documents\n    sender: converter.documents\n  - receiver: llm.messages\n    sender: prompt.prompt\n\ninputs:\n  urls: fetcher.urls\n  query: prompt.query\n\noutputs:\n  replies: llm.replies\n</code></pre>"},{"location":"concepts/yaml-pipeline-deployment/#key-requirements","title":"Key Requirements","text":"<ol> <li><code>inputs</code> Section: Maps friendly names to pipeline component fields</li> <li><code>outputs</code> Section: Maps pipeline outputs to response fields</li> <li>Valid Components: All components must be properly defined</li> <li>Valid Connections: All connections must reference existing components</li> </ol>"},{"location":"concepts/yaml-pipeline-deployment/#deployment-methods","title":"Deployment Methods","text":"CLIHTTP APIPython <pre><code># Deploy with default settings\nhayhooks pipeline deploy-yaml pipelines/chat_pipeline.yml\n\n# Deploy with custom name\nhayhooks pipeline deploy-yaml -n my_chat_pipeline pipelines/chat_pipeline.yml\n\n# Deploy with description\nhayhooks pipeline deploy-yaml -n my_chat_pipeline --description \"Chat pipeline for Q&amp;A\" pipelines/chat_pipeline.yml\n\n# Overwrite existing pipeline\nhayhooks pipeline deploy-yaml -n my_chat_pipeline --overwrite pipelines/chat_pipeline.yml\n</code></pre> <pre><code>curl -X POST \\\n  http://localhost:1416/deploy-yaml \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"name\": \"my_chat_pipeline\",\n    \"description\": \"Chat pipeline for Q&amp;A\",\n    \"source_code\": \"...\",\n    \"overwrite\": false\n  }'\n</code></pre> <pre><code>import requests\n\nresponse = requests.post(\n    \"http://localhost:1416/deploy-yaml\",\n    json={\n        \"name\": \"my_chat_pipeline\",\n        \"description\": \"Chat pipeline for Q&amp;A\",\n        \"source_code\": \"...\",  # Your YAML content as string\n        \"overwrite\": False\n    }\n)\nprint(response.json())\n</code></pre>"},{"location":"concepts/yaml-pipeline-deployment/#cli-options","title":"CLI Options","text":"Option Short Description Default <code>--name</code> <code>-n</code> Override the pipeline name YAML file stem <code>--description</code> Human-readable description Pipeline name <code>--overwrite</code> <code>-o</code> Overwrite if pipeline exists <code>false</code> <code>--skip-mcp</code> Skip MCP tool registration <code>false</code> <code>--save-file</code> Save YAML to server <code>true</code> <code>--no-save-file</code> Don't save YAML to server <code>false</code>"},{"location":"concepts/yaml-pipeline-deployment/#inputoutput-mapping","title":"Input/Output Mapping","text":""},{"location":"concepts/yaml-pipeline-deployment/#input-mapping","title":"Input Mapping","text":"<p>The <code>inputs</code> section maps friendly names to pipeline component fields:</p> <pre><code>inputs:\n  # friendly_name: component.field\n  urls: fetcher.urls\n  query: prompt_builder.query\n</code></pre> <p>Mapping rules:</p> <ul> <li>Use <code>component.field</code> syntax</li> <li>Field must exist in the component</li> <li>Multiple inputs can map to the same component field</li> <li>Input names become API parameters</li> <li>Use a YAML list when the same external field should feed multiple component inputs</li> </ul> <pre><code>inputs:\n  query:\n    - chat_summary_prompt_builder.query\n    - answer_builder.query\n</code></pre> <p>How multi-target inputs are resolved</p> <p>Hayhooks normalizes list-declared inputs by looking at the first valid target to derive type metadata and marks the input as required regardless of component-level metadata. At runtime the resolved value is fanned out to all listed component inputs, so the example above sends the same payload to both <code>chat_summary_prompt_builder.query</code> and <code>answer_builder.query</code> even if the external parameter is named differently (for example <code>a_query</code>). This ensures downstream components get the expected inputs while the API still exposes a single friendly field.</p>"},{"location":"concepts/yaml-pipeline-deployment/#output-mapping","title":"Output Mapping","text":"<p>The <code>outputs</code> section maps pipeline outputs to response fields:</p> <pre><code>outputs:\n  # response_field: component.field\n  replies: llm.replies\n  documents: fetcher.documents\n</code></pre> <p>Mapping rules:</p> <ul> <li>Use <code>component.field</code> syntax</li> <li>Field must exist in the component</li> <li>Response fields are serialized to JSON</li> <li>Complex objects are automatically serialized</li> </ul> <p>Automatic <code>include_outputs_from</code> Derivation</p> <p>Hayhooks automatically derives the <code>include_outputs_from</code> parameter from your <code>outputs</code> section. This ensures that all components referenced in the outputs are included in the pipeline results, even if they're not leaf components.</p> <p>Example: If your outputs reference <code>retriever.documents</code> and <code>llm.replies</code>, Hayhooks automatically sets <code>include_outputs_from={\"retriever\", \"llm\"}</code> when running the pipeline.</p> <p>What this means: You don't need to configure anything extra - just declare your outputs in the YAML, and Hayhooks ensures those component outputs are available in the results!</p> <p>Comparison with PipelineWrapper</p> <p>YAML Pipelines (this page): <code>include_outputs_from</code> is automatic - derived from your <code>outputs</code> section</p> <p>PipelineWrapper: <code>include_outputs_from</code> must be manually passed:</p> <ul> <li>For streaming: Pass to <code>streaming_generator()</code> / <code>async_streaming_generator()</code></li> <li>For non-streaming: Pass to <code>pipeline.run()</code> / <code>pipeline.run_async()</code></li> </ul> <p>See PipelineWrapper: include_outputs_from for examples.</p>"},{"location":"concepts/yaml-pipeline-deployment/#api-usage","title":"API Usage","text":""},{"location":"concepts/yaml-pipeline-deployment/#after-deployment","title":"After Deployment","text":"<p>Once deployed, your pipeline is available at:</p> <ul> <li>Run Endpoint: <code>/{pipeline_name}/run</code></li> <li>Schema: <code>/{pipeline_name}/schema</code></li> <li>OpenAPI: <code>/openapi.json</code></li> </ul>"},{"location":"concepts/yaml-pipeline-deployment/#example-request","title":"Example Request","text":"<pre><code>curl -X POST \\\n  http://localhost:1416/my_chat_pipeline/run \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"urls\": [\"https://haystack.deepset.ai\"],\n    \"query\": \"What is Haystack?\"\n  }'\n</code></pre>"},{"location":"concepts/yaml-pipeline-deployment/#example-response","title":"Example Response","text":"<pre><code>{\n  \"replies\": [\"Haystack is an open source framework...\"],\n  \"documents\": [...],\n  \"metadata\": {...}\n}\n</code></pre>"},{"location":"concepts/yaml-pipeline-deployment/#schema-generation","title":"Schema Generation","text":"<p>Hayhooks automatically generates:</p>"},{"location":"concepts/yaml-pipeline-deployment/#request-schema","title":"Request Schema","text":"<pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"urls\": {\n      \"type\": \"array\",\n      \"items\": {\"type\": \"string\"}\n    },\n    \"query\": {\"type\": \"string\"}\n  },\n  \"required\": [\"urls\", \"query\"]\n}\n</code></pre>"},{"location":"concepts/yaml-pipeline-deployment/#response-schema","title":"Response Schema","text":"<pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"replies\": {\"type\": \"array\"},\n    \"documents\": {\"type\": \"array\"},\n    \"metadata\": {\"type\": \"object\"}\n  }\n}\n</code></pre>"},{"location":"concepts/yaml-pipeline-deployment/#obtaining-yaml-from-existing-pipelines","title":"Obtaining YAML from Existing Pipelines","text":"<p>You can obtain YAML from existing Haystack pipelines:</p> <pre><code>from haystack import Pipeline\n\n# Create or load your pipeline\npipeline = Pipeline()\n# ... add components and connections ...\n\n# Get YAML representation\nyaml_content = pipeline.dumps()\n\n# Save to file\nwith open(\"pipeline.yml\", \"w\") as f:\n    f.write(yaml_content)\n</code></pre> <p>Note</p> <p>You'll need to manually add the <code>inputs</code> and <code>outputs</code> sections to the generated YAML.</p>"},{"location":"concepts/yaml-pipeline-deployment/#limitations","title":"Limitations","text":"<p>YAML Pipeline Limitations</p> <p>YAML-deployed pipelines have the following limitations:</p> <ul> <li> No OpenAI Compatibility: Don't support OpenAI-compatible chat endpoints</li> <li> No Streaming: Streaming responses are not supported</li> <li> No File Uploads: File upload handling is not available</li> <li> Async Only: Pipelines are run as <code>AsyncPipeline</code> instances</li> </ul> <p>Using PipelineWrapper for Advanced Features</p> <p>For advanced features, use <code>PipelineWrapper</code> instead:</p> <pre><code># For OpenAI compatibility\nclass PipelineWrapper(BasePipelineWrapper):\n    def run_chat_completion(self, model: str, messages: list[dict], body: dict) -&gt; str | Generator:\n        ...\n\n# For file uploads\nclass PipelineWrapper(BasePipelineWrapper):\n    def run_api(self, files: list[UploadFile] | None = None, query: str = \"\") -&gt; str:\n        ...\n\n# For streaming\nclass PipelineWrapper(BasePipelineWrapper):\n    def run_chat_completion_async(self, model: str, messages: list[dict], body: dict) -&gt; AsyncGenerator:\n        ...\n</code></pre>"},{"location":"concepts/yaml-pipeline-deployment/#example","title":"Example","text":"<p>For a complete working example of a YAML pipeline with proper <code>inputs</code> and <code>outputs</code>, see:</p> <ul> <li>tests/test_files/yaml/inputs_outputs_pipeline.yml</li> </ul> <p>This example demonstrates:</p> <ul> <li>Complete pipeline structure with components and connections</li> <li>Proper <code>inputs</code> mapping to component fields</li> <li>Proper <code>outputs</code> mapping from component results</li> <li>Real-world usage with <code>LinkContentFetcher</code>, <code>HTMLToDocument</code>, <code>ChatPromptBuilder</code>, and <code>OpenAIChatGenerator</code></li> </ul>"},{"location":"concepts/yaml-pipeline-deployment/#next-steps","title":"Next Steps","text":"<ul> <li>PipelineWrapper - For advanced features like streaming and chat completion</li> <li>Examples - See working examples</li> </ul>"},{"location":"deployment/deployment_guidelines/","title":"Deployment Guidelines","text":"<p>This guide describes how to deploy Hayhooks in production environments.</p> <p>Since Hayhooks is a FastAPI application, you can deploy it using any standard ASGI server deployment strategy. For comprehensive deployment concepts, see the FastAPI deployment documentation.</p> <p>This guide focuses on Hayhooks-specific considerations for production deployments.</p>"},{"location":"deployment/deployment_guidelines/#quick-recommendations","title":"Quick Recommendations","text":"<ul> <li>Use <code>HAYHOOKS_PIPELINES_DIR</code> to deploy pipelines in production environments</li> <li>Start with a single worker for I/O-bound pipelines, use multiple workers for CPU-bound workloads</li> <li>Implement async methods (<code>run_api_async</code>) for better I/O performance</li> <li>Configure health checks for container orchestration</li> <li>Set appropriate resource limits and environment variables</li> <li>Review security settings (CORS, tracebacks, logging levels)</li> </ul> <p>Configuration Resources</p> <p>Review Configuration and Environment Variables Reference before deploying.</p>"},{"location":"deployment/deployment_guidelines/#pipeline-deployment-strategy","title":"Pipeline Deployment Strategy","text":"<p>For production deployments, use <code>HAYHOOKS_PIPELINES_DIR</code> to deploy pipelines at startup.</p>"},{"location":"deployment/deployment_guidelines/#using-hayhooks_pipelines_dir","title":"Using HAYHOOKS_PIPELINES_DIR","text":"<p>Set the environment variable to point to a directory containing your pipeline definitions:</p> <pre><code>export HAYHOOKS_PIPELINES_DIR=/app/pipelines\nhayhooks run\n</code></pre> <p>When Hayhooks starts, it automatically loads all pipelines from this directory.</p> <p>Benefits:</p> <ul> <li>Pipelines are available immediately on startup</li> <li>Consistent across all workers/instances</li> <li>No runtime deployment API calls needed</li> <li>Simple to version control and deploy</li> </ul> <p>Directory structure:</p> <pre><code>pipelines/\n\u251c\u2500\u2500 my_pipeline/\n\u2502   \u251c\u2500\u2500 pipeline_wrapper.py\n\u2502   \u2514\u2500\u2500 pipeline.yml\n\u2514\u2500\u2500 another_pipeline/\n    \u251c\u2500\u2500 pipeline_wrapper.py\n    \u2514\u2500\u2500 pipeline.yml\n</code></pre> <p>See YAML Pipeline Deployment and PipelineWrapper for file structure details.</p> <p>Development vs Production</p> <p>For local development, you can use CLI commands (<code>hayhooks pipeline deploy-files</code>) or API endpoints (<code>POST /deploy-files</code>). For production, always use <code>HAYHOOKS_PIPELINES_DIR</code>.</p>"},{"location":"deployment/deployment_guidelines/#performance-tuning","title":"Performance Tuning","text":""},{"location":"deployment/deployment_guidelines/#single-worker-vs-multiple-workers","title":"Single Worker vs Multiple Workers","text":"<p>Single Worker Environment:</p> <pre><code>hayhooks run\n</code></pre> <p>Best for:</p> <ul> <li>Development and testing</li> <li>I/O-bound pipelines (HTTP requests, file operations, database queries)</li> <li>Low to moderate concurrent requests</li> <li>Simpler deployment and debugging</li> </ul> <p>Multiple Workers Environment:</p> <pre><code>hayhooks run --workers 4\n</code></pre> <p>Best for:</p> <ul> <li>CPU-bound pipelines (embedding generation, heavy computation)</li> <li>High concurrent request volumes</li> <li>Production environments with available CPU cores</li> </ul> <p>Worker Count Formula</p> <p>A common starting point: <code>workers = (2 x CPU_cores) + 1</code>. Adjust based on your workload - I/O-bound: More workers can help; CPU-bound: Match CPU cores to avoid context switching overhead.</p>"},{"location":"deployment/deployment_guidelines/#concurrency-behavior","title":"Concurrency Behavior","text":"<p>Pipeline <code>run()</code> methods execute synchronously but are wrapped in <code>run_in_threadpool</code> to avoid blocking the async event loop.</p> <p>I/O-bound pipelines (HTTP requests, file operations, database queries):</p> <ul> <li>Can handle concurrent requests effectively in a single worker</li> <li>Worker switches between tasks during I/O waits</li> <li>Consider implementing async methods for even better performance</li> </ul> <p>CPU-bound pipelines (embedding generation, heavy computation):</p> <ul> <li>Limited by Python's Global Interpreter Lock (GIL)</li> <li>Requests are queued and processed sequentially in a single worker</li> <li>Use multiple workers or horizontal scaling to improve throughput</li> </ul>"},{"location":"deployment/deployment_guidelines/#async-pipelines","title":"Async Pipelines","text":"<p>Implement async methods for better I/O-bound performance:</p> <pre><code>from haystack import AsyncPipeline\nfrom hayhooks import BasePipelineWrapper\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def setup(self) -&gt; None:\n        self.pipeline = AsyncPipeline.loads(\n            (Path(__file__).parent / \"pipeline.yml\").read_text()\n        )\n\n    async def run_api_async(self, query: str) -&gt; str:\n        result = await self.pipeline.run_async({\"prompt\": {\"query\": query}})\n        return result[\"llm\"][\"replies\"][0]\n</code></pre>"},{"location":"deployment/deployment_guidelines/#streaming","title":"Streaming","text":"<p>Use streaming for chat endpoints to reduce perceived latency:</p> <pre><code>from hayhooks import async_streaming_generator, get_last_user_message\n\nasync def run_chat_completion_async(self, model: str, messages: list[dict], body: dict):\n    question = get_last_user_message(messages)\n    return async_streaming_generator(\n        pipeline=self.pipeline,\n        pipeline_run_args={\"prompt\": {\"query\": question}},\n    )\n</code></pre> <p>See OpenAI Compatibility for more details on streaming.</p>"},{"location":"deployment/deployment_guidelines/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>Deploy multiple instances behind a load balancer for increased throughput.</p> <p>Key considerations:</p> <ul> <li>Use <code>HAYHOOKS_PIPELINES_DIR</code> to ensure all instances have the same pipelines</li> <li>Configure session affinity if using stateful components</li> <li>Distribute traffic evenly across instances</li> <li>Monitor individual instance health</li> </ul> <p>Example setup (Docker Swarm, Kubernetes, or cloud load balancers):</p> <pre><code># Each instance should use the same pipeline directory\nexport HAYHOOKS_PIPELINES_DIR=/app/pipelines\nhayhooks run\n</code></pre> <p>GIL Limitations</p> <p>Even with multiple workers, individual workers have GIL limitations. CPU-bound pipelines benefit more from horizontal scaling (multiple instances) than vertical scaling (multiple workers per instance).</p>"},{"location":"deployment/deployment_guidelines/#docker-deployment","title":"Docker Deployment","text":""},{"location":"deployment/deployment_guidelines/#single-container","title":"Single Container","text":"<pre><code>docker run -d \\\n  -p 1416:1416 \\\n  -e HAYHOOKS_HOST=0.0.0.0 \\\n  -e HAYHOOKS_PIPELINES_DIR=/app/pipelines \\\n  -v \"$PWD/pipelines:/app/pipelines:ro\" \\\n  deepset/hayhooks:main\n</code></pre>"},{"location":"deployment/deployment_guidelines/#docker-compose","title":"Docker Compose","text":"<pre><code>version: '3.8'\nservices:\n  hayhooks:\n    image: deepset/hayhooks:main\n    ports:\n      - \"1416:1416\"\n    environment:\n      HAYHOOKS_HOST: 0.0.0.0\n      HAYHOOKS_PIPELINES_DIR: /app/pipelines\n      LOG: INFO\n    volumes:\n      - ./pipelines:/app/pipelines:ro\n    restart: unless-stopped\n</code></pre> <p>See Quick Start with Docker Compose for a complete example with Open WebUI integration.</p>"},{"location":"deployment/deployment_guidelines/#health-checks","title":"Health Checks","text":"<p>Add health checks to monitor container health:</p> <pre><code>services:\n  hayhooks:\n    image: deepset/hayhooks:main\n    ports:\n      - \"1416:1416\"\n    environment:\n      HAYHOOKS_HOST: 0.0.0.0\n      HAYHOOKS_PIPELINES_DIR: /app/pipelines\n    volumes:\n      - ./pipelines:/app/pipelines:ro\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:1416/status\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 40s\n    restart: unless-stopped\n</code></pre> <p>The <code>/status</code> endpoint returns the server status and can be used for health monitoring.</p>"},{"location":"deployment/deployment_guidelines/#production-deployment-options","title":"Production Deployment Options","text":""},{"location":"deployment/deployment_guidelines/#docker","title":"Docker","text":"<p>Deploy Hayhooks using Docker containers for consistent, portable deployments across environments. Docker provides isolation, easy versioning, and simplified dependency management. See the Docker documentation for container deployment best practices.</p>"},{"location":"deployment/deployment_guidelines/#kubernetes","title":"Kubernetes","text":"<p>Deploy Hayhooks on Kubernetes for automated scaling, self-healing, and advanced orchestration capabilities. Use Deployments, Services, and ConfigMaps to manage pipeline definitions and configuration. See the Kubernetes documentation for deployment strategies.</p>"},{"location":"deployment/deployment_guidelines/#servervps-deployment","title":"Server/VPS Deployment","text":"<p>Deploy Hayhooks directly on a server or VPS using systemd or process managers like supervisord for production reliability. This approach offers full control over the environment and is suitable for dedicated workloads. See the FastAPI deployment documentation for manual deployment guidance.</p>"},{"location":"deployment/deployment_guidelines/#aws-ecs","title":"AWS ECS","text":"<p>Deploy Hayhooks on AWS Elastic Container Service for managed container orchestration in the AWS ecosystem. ECS handles container scheduling, load balancing, and integrates seamlessly with other AWS services. See the AWS ECS documentation for deployment details.</p>"},{"location":"deployment/deployment_guidelines/#production-best-practices","title":"Production Best Practices","text":""},{"location":"deployment/deployment_guidelines/#environment-variables","title":"Environment Variables","text":"<p>Store sensitive configuration in environment variables or secrets:</p> <pre><code># Use a .env file\nHAYHOOKS_PIPELINES_DIR=/app/pipelines\nLOG=INFO\nHAYHOOKS_SHOW_TRACEBACKS=false\n</code></pre> <p>See Environment Variables Reference for all options.</p>"},{"location":"deployment/deployment_guidelines/#logging","title":"Logging","text":"<p>Configure appropriate log levels for production:</p> <pre><code># Production: INFO or WARNING\nexport LOG=INFO\n\n# Development: DEBUG\nexport LOG=DEBUG\n</code></pre> <p>See Logging for details.</p>"},{"location":"deployment/deployment_guidelines/#cors-configuration","title":"CORS Configuration","text":"<p>Configure CORS for production environments:</p> <pre><code># Restrict to specific origins\nexport HAYHOOKS_CORS_ALLOW_ORIGINS='[\"https://yourdomain.com\"]'\nexport HAYHOOKS_CORS_ALLOW_CREDENTIALS=true\n</code></pre>"},{"location":"deployment/deployment_guidelines/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/deployment_guidelines/#pipeline-not-available","title":"Pipeline Not Available","text":"<p>If pipelines aren't available after startup:</p> <ol> <li>Check <code>HAYHOOKS_PIPELINES_DIR</code> is correctly set</li> <li>Verify pipeline files exist in the directory</li> <li>Check logs for deployment errors: <code>docker logs &lt;container_id&gt;</code></li> <li>Verify pipeline wrapper syntax and imports</li> </ol>"},{"location":"deployment/deployment_guidelines/#high-memory-usage","title":"High Memory Usage","text":"<p>For memory-intensive pipelines:</p> <ol> <li>Increase container memory limits in Docker Compose</li> <li>Profile pipeline components for memory leaks</li> <li>Optimize component initialization and caching</li> <li>Consider using smaller models or batch sizes</li> </ol>"},{"location":"deployment/deployment_guidelines/#slow-response-times","title":"Slow Response Times","text":"<p>For performance issues:</p> <ol> <li>Check component initialization in <code>setup()</code> vs <code>run_api()</code></li> <li>Verify pipeline directory is mounted correctly</li> <li>Review logs for errors or warnings</li> <li>Consider implementing async methods or adding workers (see Performance Tuning above)</li> </ol>"},{"location":"deployment/deployment_guidelines/#next-steps","title":"Next Steps","text":"<ul> <li>Advanced Configuration - Custom routes, middleware, and programmatic customization</li> <li>Environment Variables Reference - Complete configuration reference</li> <li>Pipeline Deployment - Pipeline deployment concepts</li> <li>Quick Start with Docker Compose - Complete Docker Compose example</li> </ul>"},{"location":"examples/async-operations/","title":"Async Operations Example","text":"<p>Patterns for async pipelines in Hayhooks: streaming responses, concurrency, and background work. Use these patterns when you need high throughput or to avoid blocking.</p>"},{"location":"examples/async-operations/#where-is-the-code","title":"Where is the code?","text":"<ul> <li>Async wrapper: async_question_answer</li> <li>See main docs for async <code>run_api_async</code> and <code>run_chat_completion_async</code></li> </ul>"},{"location":"examples/async-operations/#deploy-example","title":"Deploy (example)","text":"<pre><code>hayhooks pipeline deploy-files -n async-question-answer examples/pipeline_wrappers/async_question_answer\n</code></pre>"},{"location":"examples/async-operations/#run","title":"Run","text":"<ul> <li>OpenAI-compatible chat (async streaming):</li> </ul> <pre><code>curl -X POST http://localhost:1416/v1/chat/completions \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"model\": \"async-question-answer\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke about programming\"}]\n  }'\n</code></pre> <p>Best Practices</p> <ul> <li>Prefer <code>run_chat_completion_async</code> for streaming and concurrency</li> <li>Use async-compatible components (e.g., <code>OpenAIChatGenerator</code>) for best performance</li> <li>For legacy pipelines with sync-only components (like <code>OpenAIGenerator</code>), use <code>allow_sync_streaming_callbacks=True</code> to enable hybrid mode</li> <li>See Hybrid Streaming for handling legacy components</li> </ul>"},{"location":"examples/async-operations/#related","title":"Related","text":"<ul> <li>General guide: Main docs</li> <li>Examples index: Examples Overview</li> </ul>"},{"location":"examples/chat-with-website/","title":"Chat with Website Example","text":"<p>Build a pipeline that answers questions about one or more websites. Uses fetching, cleaning and an LLM to generate answers, and supports streaming via OpenAI-compatible chat endpoints when implemented in the wrapper.</p>"},{"location":"examples/chat-with-website/#where-is-the-code","title":"Where is the code?","text":"<ul> <li>Wrapper example directory: examples/pipeline_wrappers/chat_with_website_streaming</li> <li>See the main docs for <code>PipelineWrapper</code> basics and OpenAI compatibility</li> </ul>"},{"location":"examples/chat-with-website/#deploy","title":"Deploy","text":"<pre><code>hayhooks pipeline deploy-files -n chat_with_website examples/pipeline_wrappers/chat_with_website_streaming\n</code></pre>"},{"location":"examples/chat-with-website/#run","title":"Run","text":"<ul> <li>API mode:</li> </ul> <pre><code>curl -X POST http://localhost:1416/chat_with_website/run \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"query\": \"What is this website about?\", \"urls\": [\"https://python.org\"]}'\n</code></pre> <ul> <li>Chat (OpenAI-compatible), when <code>run_chat_completion</code>/<code>_async</code> is implemented:</li> </ul> <pre><code>curl -X POST http://localhost:1416/v1/chat/completions \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"model\": \"chat_with_website\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Tell me about https://github.com\"}]\n  }'\n</code></pre> <p>Development Tips</p> <ul> <li>For development, use <code>--overwrite</code> to redeploy a changed wrapper: <code>hayhooks pipeline deploy-files -n chat_with_website --overwrite &lt;dir&gt;</code></li> <li>Some examples may require extra Python packages (e.g., <code>trafilatura</code>). Install as needed.</li> </ul>"},{"location":"examples/chat-with-website/#related","title":"Related","text":"<ul> <li>General guide: Main docs</li> <li>Examples index: Examples Overview</li> </ul>"},{"location":"examples/openwebui-events/","title":"Open WebUI Events Example","text":"<p>Send status updates and UI events to Open WebUI during streaming, and optionally intercept tool calls for richer feedback.</p>"},{"location":"examples/openwebui-events/#where-is-the-code","title":"Where is the code?","text":"<ul> <li>Event examples: open_webui_agent_events, open_webui_agent_on_tool_calls</li> <li>See the main docs \u2192 Open WebUI integration and event hooks</li> </ul>"},{"location":"examples/openwebui-events/#deploy-example","title":"Deploy (example)","text":"<pre><code>hayhooks pipeline deploy-files -n agent_events examples/pipeline_wrappers/open_webui_agent_events\n</code></pre>"},{"location":"examples/openwebui-events/#run","title":"Run","text":"<ul> <li>OpenAI-compatible chat (events stream to Open WebUI):</li> </ul> <pre><code>curl -X POST http://localhost:1416/v1/chat/completions \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"model\": \"agent_events\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Tell me about machine learning\"}]\n  }'\n</code></pre> <p>Working with Events</p> <ul> <li>Use helpers from <code>hayhooks.open_webui</code>: <code>create_status_event</code>, <code>create_message_event</code>, <code>create_replace_event</code>, <code>create_source_event</code>, <code>create_notification_event</code>, <code>create_details_tag</code></li> <li>Intercept tool calls via <code>on_tool_call_start</code>/<code>on_tool_call_end</code> with <code>streaming_generator</code>/<code>async_streaming_generator</code></li> <li>For recommended Open WebUI settings, see the Open WebUI Integration guide</li> </ul>"},{"location":"examples/openwebui-events/#related","title":"Related","text":"<ul> <li>General guide: Main docs</li> <li>Examples index: Examples Overview</li> </ul>"},{"location":"examples/overview/","title":"Examples Overview","text":"<p>This page lists all maintained Hayhooks examples with detailed descriptions and links to the source code.</p>"},{"location":"examples/overview/#pipeline-wrapper-examples","title":"Pipeline wrapper examples","text":"Example Docs Code Description Chat with Website (Streaming) chat-with-website.md GitHub Website Q&amp;A with streaming Chat with Website (basic) \u2014 GitHub Minimal website Q&amp;A wrapper Chat with Website (MCP) \u2014 GitHub Exposes website Q&amp;A as MCP Tool Async Question Answer (Streaming) async-operations.md GitHub Async pipeline and streaming patterns Open WebUI Agent Events openwebui-events.md GitHub UI events and status updates Open WebUI Agent on Tool Calls openwebui-events.md GitHub Tool call interception &amp; feedback Image Generation (File Response) file-response-support.md GitHub Returning binary files (images) from <code>run_api</code> Shared Code Between Wrappers \u2014 GitHub Reusing code across wrappers"},{"location":"examples/overview/#end-to-end-examples-patterns","title":"End-to-end examples &amp; patterns","text":"Example Docs Code Description RAG: Indexing and Query with Elasticsearch rag-system.md GitHub Full indexing/query pipelines with Elasticsearch"},{"location":"examples/overview/#how-to-use-examples","title":"How to use examples","text":"<p>Prerequisites: - Install Hayhooks: <code>pip install hayhooks</code> (additional deps per example may apply)</p> <p>Deployment: - Pipeline wrappers: deploy directly with <code>hayhooks pipeline deploy-files -n &lt;name&gt; &lt;example_dir&gt;</code> and run via API (<code>POST /&lt;name&gt;/run</code>) or OpenAI-compatible chat endpoints if implemented - End-to-end examples: follow the example's documentation for full setup (services like Elasticsearch, multi-pipeline deployment, datasets, etc.)</p> <p>For general usage and CLI commands, see the Getting Started Guide.</p>"},{"location":"examples/overview/#try-all-examples-with-docker-compose","title":"Try all examples with Docker Compose","text":"<p>The repository includes a <code>compose.yml</code> file that automatically deploys all pipeline wrapper examples:</p> <pre><code># Clone the repository\ngit clone https://github.com/deepset-ai/hayhooks.git\ncd hayhooks\n\n# Set your API key (required for most examples)\nexport OPENAI_API_KEY=your-api-key\n\n# Start Hayhooks with all examples\ndocker compose up -d\n</code></pre> <p>This will:</p> <ul> <li>Start Hayhooks on port <code>1416</code> (API) and <code>1417</code> (MCP server)</li> <li>Auto-deploy all pipeline wrappers from <code>examples/pipeline_wrappers/</code></li> <li>Install required dependencies like <code>trafilatura</code></li> </ul> <p>Check deployed pipelines:</p> <pre><code>curl localhost:1416/status\n</code></pre> <p>Access the API documentation at http://localhost:1416/docs.</p>"},{"location":"examples/rag-system/","title":"RAG System Example","text":"<p>Build a Retrieval-Augmented Generation flow: ingest documents, embed and store them, retrieve by similarity, and answer questions with an LLM.</p>"},{"location":"examples/rag-system/#where-is-the-code","title":"Where is the code?","text":"<ul> <li>End-to-end example: examples/rag_indexing_query</li> <li><code>indexing_pipeline/</code> - Handles document upload and indexing</li> <li><code>query_pipeline/</code> - Retrieves and generates answers</li> </ul>"},{"location":"examples/rag-system/#quick-start-from-repository-root","title":"Quick Start (from repository root)","text":"<pre><code># 1) Enter the example\ncd examples/rag_indexing_query\n\n# 2) (Recommended) Create and activate a virtual env, then install deps\npython -m venv .venv &amp;&amp; source .venv/bin/activate\npip install -r requirements.txt\n\n# 3) Launch Hayhooks (in a separate terminal if you prefer)\nhayhooks run\n\n# 4) Launch Elasticsearch\ndocker compose up\n\n# 5) Deploy the pipelines\nhayhooks pipeline deploy-files -n indexing indexing_pipeline\nhayhooks pipeline deploy-files -n query query_pipeline\n\n# 6) Index sample files\nhayhooks pipeline run indexing --dir files_to_index\n\n# 7) Ask a question\nhayhooks pipeline run query --param 'question=\"is this recipe vegan?\"'\n\n# Optional: check API docs\n# http://localhost:1416/docs\n</code></pre> <p>Additional Information</p> <ul> <li>See File Upload Support for wrapper signature and request format</li> <li>Choose appropriate embedding models and document stores for your scale</li> </ul>"},{"location":"examples/rag-system/#related","title":"Related","text":"<ul> <li>General guide: Main docs</li> <li>Examples index: Examples Overview</li> </ul>"},{"location":"features/chainlit-integration/","title":"Chainlit Integration","text":"<p>Hayhooks provides optional integration with Chainlit, allowing you to embed a chat UI directly within your Hayhooks server. This provides a zero-configuration frontend for interacting with your deployed Haystack pipelines.</p>"},{"location":"features/chainlit-integration/#overview","title":"Overview","text":"<p>The Chainlit integration offers:</p> <ul> <li>Single Deployment: Run both backend and frontend in one process/container</li> <li>Zero Configuration: Works out-of-the-box with Hayhooks' OpenAI-compatible endpoints</li> <li>Streaming Support: Real-time streaming responses in the chat interface</li> <li>Pipeline Selection: Automatically discovers and lists deployed pipelines</li> </ul>"},{"location":"features/chainlit-integration/#installation","title":"Installation","text":"<p>Install Hayhooks with the <code>chainlit</code> extra:</p> <pre><code>pip install \"hayhooks[chainlit]\"\n</code></pre>"},{"location":"features/chainlit-integration/#quick-start","title":"Quick Start","text":""},{"location":"features/chainlit-integration/#using-cli","title":"Using CLI","text":"<p>The simplest way to enable the Chainlit UI is via the <code>--with-chainlit</code> flag:</p> <pre><code>hayhooks run --with-chainlit\n</code></pre> <p>This starts Hayhooks with the embedded Chainlit UI available at <code>http://localhost:1416/chat</code>.</p>"},{"location":"features/chainlit-integration/#custom-ui-path","title":"Custom UI Path","text":"<p>You can customize the URL path where the UI is mounted:</p> <pre><code>hayhooks run --with-chainlit --chainlit-path /my-chat\n</code></pre> <p>Now the UI will be available at <code>http://localhost:1416/my-chat</code>.</p>"},{"location":"features/chainlit-integration/#using-environment-variables","title":"Using Environment Variables","text":"<p>You can also configure the UI via environment variables:</p> <pre><code>export HAYHOOKS_CHAINLIT_ENABLED=true\nexport HAYHOOKS_CHAINLIT_PATH=/chat\n\nhayhooks run\n</code></pre>"},{"location":"features/chainlit-integration/#configuration","title":"Configuration","text":""},{"location":"features/chainlit-integration/#environment-variables","title":"Environment Variables","text":"Variable Description Default <code>HAYHOOKS_CHAINLIT_ENABLED</code> Enable/disable the Chainlit UI <code>false</code> <code>HAYHOOKS_CHAINLIT_PATH</code> URL path where UI is mounted <code>/chat</code> <code>HAYHOOKS_CHAINLIT_APP</code> Custom Chainlit app file path (uses default)"},{"location":"features/chainlit-integration/#additional-settings","title":"Additional Settings","text":"Variable Description Default <code>HAYHOOKS_CHAINLIT_DEFAULT_MODEL</code> Default pipeline to auto-select (auto-select if only one) <code>HAYHOOKS_CHAINLIT_REQUEST_TIMEOUT</code> Timeout (seconds) for chat requests <code>120.0</code>"},{"location":"features/chainlit-integration/#how-it-works","title":"How It Works","text":"<pre><code>flowchart LR\n    subgraph server [\"Hayhooks Server\"]\n        UI[\"Chainlit UI\\n(mounted at /chat)\"]\n        API[\"/v1/chat/completions\\n(OpenAI-compatible)\"]\n        Pipelines[\"Haystack Pipelines\\n(with chat support)\"]\n\n        UI -- \"HTTP streaming\" --&gt; API\n        API --&gt; Pipelines\n    end\n\n    Browser[\"\ud83c\udf10 Browser\"] --&gt; UI</code></pre> <p>The Chainlit UI is mounted as a FastAPI sub-application and communicates with your pipelines through Hayhooks' OpenAI-compatible endpoints. This means:</p> <ol> <li>Your pipelines must implement <code>run_chat_completion</code> or <code>run_chat_completion_async</code></li> <li>The UI automatically discovers available pipelines via <code>/v1/models</code></li> <li>Streaming is supported out-of-the-box</li> </ol>"},{"location":"features/chainlit-integration/#example-complete-setup","title":"Example: Complete Setup","text":""},{"location":"features/chainlit-integration/#1-create-a-pipeline-wrapper","title":"1. Create a Pipeline Wrapper","text":"<pre><code># pipelines/my_chat/pipeline_wrapper.py\nfrom typing import Generator\n\nfrom haystack import Pipeline\nfrom haystack.components.builders import ChatPromptBuilder\nfrom haystack.components.generators.chat import OpenAIChatGenerator\nfrom haystack.dataclasses import ChatMessage\n\nfrom hayhooks import BasePipelineWrapper, streaming_generator\n\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def setup(self) -&gt; None:\n        self.system_message = ChatMessage.from_system(\"You are a helpful assistant.\")\n        chat_prompt_builder = ChatPromptBuilder()\n        llm = OpenAIChatGenerator(model=\"gpt-4o-mini\")\n\n        self.pipeline = Pipeline()\n        self.pipeline.add_component(\"chat_prompt_builder\", chat_prompt_builder)\n        self.pipeline.add_component(\"llm\", llm)\n        self.pipeline.connect(\"chat_prompt_builder.prompt\", \"llm.messages\")\n\n    def run_chat_completion(self, model: str, messages: list[dict], body: dict) -&gt; Generator:\n        chat_messages = [self.system_message] + [\n            ChatMessage.from_openai_dict_format(msg) for msg in messages\n        ]\n        return streaming_generator(\n            pipeline=self.pipeline,\n            pipeline_run_args={\"chat_prompt_builder\": {\"template\": chat_messages}},\n        )\n</code></pre>"},{"location":"features/chainlit-integration/#2-run-hayhooks-with-ui","title":"2. Run Hayhooks with UI","text":"<pre><code>hayhooks run --with-chainlit --pipelines-dir ./pipelines\n</code></pre>"},{"location":"features/chainlit-integration/#3-open-the-ui","title":"3. Open the UI","text":"<p>Navigate to <code>http://localhost:1416/chat</code> in your browser. You'll see your deployed pipeline and can start chatting!</p>"},{"location":"features/chainlit-integration/#custom-chainlit-app","title":"Custom Chainlit App","text":"<p>You can provide your own Chainlit app for more customization:</p> <pre><code>hayhooks run --with-chainlit\n</code></pre> <p>With environment variable:</p> <pre><code>export HAYHOOKS_CHAINLIT_APP=/path/to/my_chainlit_app.py\nhayhooks run --with-chainlit\n</code></pre>"},{"location":"features/chainlit-integration/#example-custom-app","title":"Example Custom App","text":"<pre><code># my_chainlit_app.py\nimport os\nimport chainlit as cl\nimport httpx\n\nHAYHOOKS_URL = os.getenv(\"HAYHOOKS_BASE_URL\", \"http://localhost:1416\")\n\n@cl.on_chat_start\nasync def start():\n    await cl.Message(content=\"Welcome! How can I help you today?\").send()\n\n@cl.on_message\nasync def main(message: cl.Message):\n    response_msg = cl.Message(content=\"\")\n    await response_msg.send()\n\n    async with httpx.AsyncClient() as client:\n        async with client.stream(\n            \"POST\",\n            f\"{HAYHOOKS_URL}/v1/chat/completions\",\n            json={\n                \"model\": \"my_pipeline\",  # Your pipeline name\n                \"messages\": [{\"role\": \"user\", \"content\": message.content}],\n                \"stream\": True,\n            },\n            timeout=120.0,\n        ) as response:\n            async for line in response.aiter_lines():\n                if line.startswith(\"data: \") and line[6:] != \"[DONE]\":\n                    import json\n                    chunk = json.loads(line[6:])\n                    content = chunk[\"choices\"][0].get(\"delta\", {}).get(\"content\", \"\")\n                    if content:\n                        await response_msg.stream_token(content)\n\n    await response_msg.update()\n</code></pre>"},{"location":"features/chainlit-integration/#custom-elements","title":"Custom Elements","text":"<p>Chainlit supports custom elements -- JSX components that pipelines can push to the UI at runtime for rich, interactive widgets (charts, cards, maps, etc.).</p> <p>The chainlit_weather_agent example includes a <code>WeatherCard</code> element that demonstrates this feature. To use your own elements, point Hayhooks at a directory of <code>.jsx</code> files:</p>"},{"location":"features/chainlit-integration/#using-cli_1","title":"Using CLI","text":"<pre><code>hayhooks run --with-chainlit --chainlit-custom-elements-dir ./my_elements\n</code></pre>"},{"location":"features/chainlit-integration/#using-environment-variable","title":"Using Environment Variable","text":"<pre><code>export HAYHOOKS_CHAINLIT_CUSTOM_ELEMENTS_DIR=./my_elements\nhayhooks run --with-chainlit\n</code></pre> <p>At startup, Hayhooks copies every <code>.jsx</code> file from that directory into the Chainlit <code>public/elements/</code> folder. The file name (minus <code>.jsx</code>) becomes the element name you reference from your pipeline:</p> <pre><code>my_elements/\n  StockChart.jsx\n  MapView.jsx\n</code></pre> <pre><code># In your pipeline wrapper\nfrom hayhooks.chainlit_events import create_custom_element_event\n\ndef on_tool_call_end(self, tool_name, arguments, result, error):\n    return [create_custom_element_event(name=\"StockChart\", props={\"symbol\": \"AAPL\", ...})]\n</code></pre>"},{"location":"features/chainlit-integration/#jsx-requirements","title":"JSX Requirements","text":"<p>Custom elements must:</p> <ul> <li>Be written in JSX (not TSX) and export a default component</li> <li>Access data via the global <code>props</code> object (not function arguments)</li> <li>Use Tailwind CSS classes for styling (shadcn + tailwind environment)</li> </ul> <p>Available imports include <code>@/components/ui/*</code> (shadcn), <code>lucide-react</code>, <code>react</code>, and <code>recharts</code>. See the Chainlit custom element docs for the full list.</p> <p>Tip</p> <p>If a custom file has the same name as a built-in element, the custom version takes precedence. A warning is logged when this happens.</p>"},{"location":"features/chainlit-integration/#branding-theme","title":"Branding &amp; Theme","text":"<p>The default Chainlit UI ships with Hayhooks logos, favicons, and a <code>theme.json</code>. To override any of these, place your own files in a <code>public/</code> directory next to your custom Chainlit app:</p> <pre><code>my_chainlit/\n  my_app.py               # Custom Chainlit app\n  public/\n    logo_dark.png          # Overrides the default dark-mode logo\n    theme.json             # Overrides the default theme\n</code></pre> <pre><code>export HAYHOOKS_CHAINLIT_APP=my_chainlit/my_app.py\nhayhooks run --with-chainlit\n</code></pre> <p>When <code>HAYHOOKS_CHAINLIT_APP</code> is set, Hayhooks uses the parent directory of that file as the Chainlit app root. Built-in assets (logos, favicons, theme) are automatically seeded into the <code>public/</code> directory, so you only need to provide the files you want to override -- anything you don't provide keeps the default.</p> <p>The built-in public assets are:</p> File Description <code>logo_dark.png</code> Logo displayed in dark mode <code>logo_light.png</code> Logo displayed in light mode <code>favicon.ico</code> Browser tab icon (ICO) <code>favicon.png</code> Browser tab icon (PNG) <code>favicon.svg</code> Browser tab icon (SVG) <code>apple-touch-icon.png</code> iOS home screen icon <code>theme.json</code> Chainlit theme (colors, fonts, etc.) <p>For theme configuration options, see the Chainlit theme documentation.</p>"},{"location":"features/chainlit-integration/#why-embedded-chainlit","title":"Why Embedded Chainlit","text":"<p>The embedded Chainlit UI is designed for simplicity and fast iteration:</p> <ul> <li>Single process deployment: No extra containers or services to manage</li> <li>Minimal setup: Just <code>pip install \"hayhooks[chainlit]\"</code> and <code>--with-chainlit</code></li> <li>Zero configuration: Automatically discovers deployed pipelines via <code>/v1/models</code></li> <li>Streaming out of the box: Real-time token streaming with tool call visualization</li> </ul> <p>For production deployments requiring persistent conversation history, multi-user authentication, or advanced features, consider pairing Hayhooks with an external frontend like Open WebUI which connects through the same OpenAI-compatible endpoints.</p>"},{"location":"features/chainlit-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/chainlit-integration/#ui-not-loading","title":"UI Not Loading","text":"<ol> <li>Ensure Chainlit is installed: <code>pip install \"hayhooks[chainlit]\"</code></li> <li>Check that <code>--with-chainlit</code> flag is set or <code>HAYHOOKS_CHAINLIT_ENABLED=true</code></li> <li>Verify the UI path in logs (default: <code>/chat</code>)</li> </ol>"},{"location":"features/chainlit-integration/#no-pipelines-available","title":"No Pipelines Available","text":"<p>The UI requires at least one deployed pipeline with chat completion support:</p> <ol> <li>Ensure your pipeline implements <code>run_chat_completion</code> or <code>run_chat_completion_async</code></li> <li>Check that pipelines are deployed: <code>curl http://localhost:1416/v1/models</code></li> </ol>"},{"location":"features/chainlit-integration/#streaming-not-working","title":"Streaming Not Working","text":"<ol> <li>Ensure your pipeline returns a <code>Generator</code> or <code>AsyncGenerator</code></li> <li>Use <code>streaming_generator</code> or <code>async_streaming_generator</code> helpers</li> <li>Check browser console for WebSocket errors</li> </ol>"},{"location":"features/chainlit-integration/#assets-not-loading-behind-a-reverse-proxy","title":"Assets Not Loading Behind a Reverse Proxy","text":"<p>When Hayhooks is served behind a reverse proxy with a path prefix (<code>root_path</code>), Chainlit assets (logos, theme, favicon) may fail to load because their URLs don't include the prefix. If you experience this:</p> <ol> <li>Verify that <code>HAYHOOKS_ROOT_PATH</code> is set correctly</li> <li>Check your reverse proxy is forwarding requests to the correct paths</li> <li>As a workaround, consider serving static assets directly from the reverse proxy</li> </ol>"},{"location":"features/chainlit-integration/#production-notes","title":"Production Notes","text":""},{"location":"features/chainlit-integration/#multiple-workers-and-sticky-sessions","title":"Multiple Workers and Sticky Sessions","text":"<p>Chainlit uses WebSockets via socket.io, which keeps session state in-memory. Running with <code>--workers &gt; 1</code> requires sticky sessions (session affinity) on your load balancer so that all requests from the same client hit the same worker.</p> <p>Even with sticky sessions, some load balancers struggle to consistently route WebSocket upgrades. If you experience intermittent disconnects, set <code>transports = [\"websocket\"]</code> in your <code>.chainlit/config.toml</code> to skip the HTTP long-polling fallback:</p> <pre><code>[project]\n# Use WebSocket transport only (recommended behind load balancers with sticky sessions)\ntransports = [\"websocket\"]\n</code></pre>"},{"location":"features/chainlit-integration/#authentication","title":"Authentication","text":"<p>The embedded Chainlit UI is public by default -- anyone with network access to the URL can use it. To restrict access:</p> <ul> <li>Set the <code>CHAINLIT_AUTH_SECRET</code> environment variable (generate one with <code>chainlit create-secret</code>)</li> <li>Implement an authentication callback in a custom Chainlit app via <code>HAYHOOKS_CHAINLIT_APP</code></li> <li>Or place Hayhooks behind a reverse proxy with its own authentication layer</li> </ul>"},{"location":"features/chainlit-integration/#conversation-persistence","title":"Conversation Persistence","text":"<p>By default, conversation history lives only in the server's memory and is lost on page refresh or server restart. Chainlit supports pluggable data layers for persistent storage:</p> <ul> <li>SQLite (file-based, zero infrastructure) or PostgreSQL via the community SQLAlchemy data layer</li> <li>DynamoDB for cloud-native deployments</li> <li>Custom implementations via Chainlit's <code>BaseDataLayer</code> API</li> </ul> <p>To configure a data layer, provide a custom Chainlit app (<code>HAYHOOKS_CHAINLIT_APP</code>) that registers the data layer at startup.</p> <p>Note</p> <p>Chainlit sessions are server-side only. Fully client-side storage (cookies, localStorage) is not supported by Chainlit's architecture.</p>"},{"location":"features/chainlit-integration/#endpoint-ordering","title":"Endpoint Ordering","text":"<p><code>mount_chainlit()</code> must be called after all other FastAPI routes are registered -- it captures all unmatched URL space. Hayhooks handles this correctly in <code>create_app()</code>, but if you add custom middleware or routes, ensure they are registered before the Chainlit mount.</p>"},{"location":"features/chainlit-integration/#limitations","title":"Limitations","text":"<ul> <li>Single worker without sticky sessions: Use <code>--workers 1</code> (the default) when running with <code>--with-chainlit</code>, or configure sticky sessions on your load balancer. See Production Notes above.</li> <li>No conversation persistence out of the box: History is in-memory only. Configure a data layer for persistence.</li> <li>No built-in authentication: The UI is public by default. See Authentication above.</li> <li>No client-side session storage: Chainlit's architecture requires server-side sessions over WebSocket. Page refreshes create a new session.</li> </ul>"},{"location":"features/chainlit-integration/#next-steps","title":"Next Steps","text":"<ul> <li>OpenAI Compatibility - Learn about chat completion implementation</li> <li>Open WebUI Integration - For a more feature-rich frontend</li> <li>Examples - Working pipeline examples</li> </ul>"},{"location":"features/cli-commands/","title":"CLI Commands","text":"<p>Hayhooks provides a comprehensive command-line interface for managing pipelines and the server. This section covers all available CLI commands and their usage.</p>"},{"location":"features/cli-commands/#overview","title":"Overview","text":"<p>The Hayhooks CLI allows you to:</p> <ul> <li>Start and manage the Hayhooks server</li> <li>Deploy and undeploy pipelines</li> <li>Run pipelines with custom inputs</li> <li>Monitor server status and health</li> <li>Manage MCP server operations</li> </ul>"},{"location":"features/cli-commands/#installation","title":"Installation","text":"<p>The CLI is automatically installed with the Hayhooks package:</p> <pre><code>pip install hayhooks\n</code></pre>"},{"location":"features/cli-commands/#global-commands","title":"Global Commands","text":""},{"location":"features/cli-commands/#help","title":"Help","text":"<p>Get help for any command:</p> <pre><code># Show main help\nhayhooks --help\n\n# Show help for specific command\nhayhooks run --help\nhayhooks pipeline --help\n</code></pre>"},{"location":"features/cli-commands/#version","title":"Version","text":"<p>Check the installed version:</p> <pre><code>hayhooks --version\n</code></pre>"},{"location":"features/cli-commands/#server-commands","title":"Server Commands","text":""},{"location":"features/cli-commands/#run-http-vs-cli-example","title":"run (HTTP vs CLI example)","text":"<p>Start the Hayhooks server:</p> <pre><code># Basic server start\nhayhooks run\n\n# With custom host and port\nhayhooks run --host 0.0.0.0 --port 1416\n\n# With multiple workers\nhayhooks run --workers 4\n\n# With custom pipelines directory\nhayhooks run --pipelines-dir ./my_pipelines\n\n# With additional Python path\nhayhooks run --additional-python-path ./custom_code\n\n# Reload on changes (development)\nhayhooks run --reload\n</code></pre>"},{"location":"features/cli-commands/#options-for-run","title":"Options for <code>run</code>","text":"Option Short Description Default <code>--host</code> Host to bind to <code>localhost</code> <code>--port</code> Port to listen on <code>1416</code> <code>--workers</code> Number of worker processes <code>1</code> <code>--pipelines-dir</code> Directory for pipeline definitions <code>./pipelines</code> <code>--additional-python-path</code> Additional Python path <code>None</code> <code>--root-path</code> Root path for API <code>/</code> <code>--reload</code> Reload on code changes (development) <code>false</code>"},{"location":"features/cli-commands/#mcp-run","title":"mcp run","text":"<p>Start the MCP server:</p> <pre><code># Start MCP server\nhayhooks mcp run\n\n# With custom host and port\nhayhooks mcp run --host 0.0.0.0 --port 1417\n</code></pre>"},{"location":"features/cli-commands/#options-for-mcp-run","title":"Options for <code>mcp run</code>","text":"Option Short Description Default <code>--host</code> MCP server host <code>localhost</code> <code>--port</code> MCP server port <code>1417</code> <code>--pipelines-dir</code> Directory for pipeline definitions <code>./pipelines</code> <code>--additional-python-path</code> Additional Python path <code>None</code>"},{"location":"features/cli-commands/#pipeline-management-commands","title":"Pipeline Management Commands","text":""},{"location":"features/cli-commands/#pipeline-deploy-files","title":"pipeline deploy-files","text":"<p>Deploy a pipeline from wrapper files:</p> <pre><code># Basic deployment\nhayhooks pipeline deploy-files -n my_pipeline ./path/to/pipeline\n\n# With custom name and description\nhayhooks pipeline deploy-files -n my_pipeline --description \"My pipeline\" ./path/to/pipeline\n\n# Overwrite existing pipeline\nhayhooks pipeline deploy-files -n my_pipeline --overwrite ./path/to/pipeline\n\n# Skip saving files to server\nhayhooks pipeline deploy-files -n my_pipeline --skip-saving-files ./path/to/pipeline\n\n# Skip MCP tool registration\nhayhooks pipeline deploy-files -n my_pipeline --skip-mcp ./path/to/pipeline\n</code></pre>"},{"location":"features/cli-commands/#options-for-pipeline-deploy-files","title":"Options for <code>pipeline deploy-files</code>","text":"Option Short Description Default <code>--name</code> <code>-n</code> Pipeline name Required <code>--description</code> Human-readable description Pipeline name <code>--overwrite</code> <code>-o</code> Overwrite existing pipeline <code>false</code> <code>--skip-saving-files</code> Don't save files to server <code>false</code> <code>--skip-mcp</code> Skip MCP tool registration <code>false</code>"},{"location":"features/cli-commands/#pipeline-deploy-yaml","title":"pipeline deploy-yaml","text":"<p>Deploy a pipeline from YAML definition:</p> <pre><code># Deploy from YAML file\nhayhooks pipeline deploy-yaml pipelines/my_pipeline.yml\n\n# With custom name\nhayhooks pipeline deploy-yaml -n my_custom_name pipelines/my_pipeline.yml\n\n# With description\nhayhooks pipeline deploy-yaml -n my_pipeline --description \"YAML pipeline\" pipelines/my_pipeline.yml\n\n# Overwrite existing\nhayhooks pipeline deploy-yaml -n my_pipeline --overwrite pipelines/my_pipeline.yml\n\n# Don't save YAML file\nhayhooks pipeline deploy-yaml -n my_pipeline --no-save-file pipelines/my_pipeline.yml\n</code></pre>"},{"location":"features/cli-commands/#options-for-pipeline-deploy-yaml","title":"Options for <code>pipeline deploy-yaml</code>","text":"Option Short Description Default <code>--name</code> <code>-n</code> Pipeline name YAML file stem <code>--description</code> Human-readable description Pipeline name <code>--overwrite</code> <code>-o</code> Overwrite existing pipeline <code>false</code> <code>--skip-mcp</code> Skip MCP tool registration <code>false</code> <code>--save-file</code> Save YAML to server <code>true</code> <code>--no-save-file</code> Don't save YAML to server <code>false</code>"},{"location":"features/cli-commands/#pipeline-undeploy","title":"pipeline undeploy","text":"<p>Undeploy a pipeline:</p> <pre><code># Undeploy by name\nhayhooks pipeline undeploy my_pipeline\n\n# Force undeploy (ignore errors)\nhayhooks pipeline undeploy my_pipeline --force\n</code></pre>"},{"location":"features/cli-commands/#options-for-pipeline-undeploy","title":"Options for <code>pipeline undeploy</code>","text":"Option Short Description Default <code>--force</code> Force undeploy (if needed) <code>false</code>"},{"location":"features/cli-commands/#pipeline-run","title":"pipeline run","text":"<p>Run a deployed pipeline:</p> <pre><code># Run with JSON parameters\nhayhooks pipeline run my_pipeline --param 'query=\"What is Haystack?\"'\n\n# Run with multiple parameters\nhayhooks pipeline run my_pipeline --param 'query=\"What is Haystack?\"' --param 'max_results=5'\n\n# Upload files\nhayhooks pipeline run my_pipeline --file document.pdf --param 'query=\"Summarize this\"'\n\n# Upload directory\nhayhooks pipeline run my_pipeline --dir ./documents --param 'query=\"Analyze all documents\"'\n\n# Upload multiple files\nhayhooks pipeline run my_pipeline --file doc1.pdf --file doc2.txt --param 'query=\"Compare documents\"'\n</code></pre>"},{"location":"features/cli-commands/#options-for-pipeline-run","title":"Options for <code>pipeline run</code>","text":"Option Short Description Default <code>--file</code> Upload single file None <code>--dir</code> Upload directory None <code>--param</code> Pass parameters as JSON None"},{"location":"features/cli-commands/#status-and-monitoring-commands","title":"Status and Monitoring Commands","text":""},{"location":"features/cli-commands/#status","title":"status","text":"<p>Check server and pipeline status:</p> <pre><code># Check server status\nhayhooks status\n</code></pre> <p>Note</p> <p>There is no <code>hayhooks health</code> command in the CLI. Use <code>hayhooks status</code> or call HTTP endpoints directly.</p> <p>CLI Limitations</p> <ul> <li>Configuration is managed via environment variables and CLI flags. See Configuration.</li> <li>Development flows (testing, linting) are not exposed as CLI commands.</li> <li>Use your process manager or container logs to view logs; Hayhooks uses standard output.</li> <li>Advanced export/import/migrate commands are not provided by the Hayhooks CLI at this time.</li> </ul>"},{"location":"features/cli-commands/#http-api-commands","title":"HTTP API Commands","text":"<p>All CLI commands have corresponding HTTP API endpoints.</p> <p>Interactive API Documentation</p> <p>Explore and test all HTTP API endpoints interactively:</p> <ul> <li>Swagger UI: <code>http://localhost:1416/docs</code></li> <li>ReDoc: <code>http://localhost:1416/redoc</code></li> </ul> <p>See the API Reference for complete documentation.</p> <p>Common HTTP API equivalents:</p>"},{"location":"features/cli-commands/#deploy-files","title":"deploy-files","text":"<pre><code># CLI\nhayhooks pipeline deploy-files -n my_pipeline ./path/to/pipeline\n\n# HTTP API\ncurl -X POST http://localhost:1416/deploy_files \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"name\": \"my_pipeline\",\n    \"description\": \"My pipeline\",\n    \"files\": [...],\n    \"overwrite\": false\n  }'\n</code></pre>"},{"location":"features/cli-commands/#deploy-yaml","title":"deploy-yaml","text":"<pre><code># CLI\nhayhooks pipeline deploy-yaml -n my_pipeline pipelines/my_pipeline.yml\n\n# HTTP API\ncurl -X POST http://localhost:1416/deploy-yaml \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"name\": \"my_pipeline\",\n    \"description\": \"My pipeline\",\n    \"source_code\": \"...\",\n    \"overwrite\": false\n  }'\n</code></pre>"},{"location":"features/cli-commands/#run","title":"run","text":"<pre><code># CLI\nhayhooks pipeline run my_pipeline --param 'query=\"What is Haystack?\"'\n\n# HTTP API\ncurl -X POST http://localhost:1416/my_pipeline/run \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"query\": \"What is Haystack?\"}'\n</code></pre>"},{"location":"features/cli-commands/#configuration-files","title":"Configuration Files","text":""},{"location":"features/cli-commands/#env-file","title":".env File","text":"<p>Create a <code>.env</code> file for configuration:</p> <pre><code># .env\nHAYHOOKS_HOST=0.0.0.0\nHAYHOOKS_PORT=1416\nHAYHOOKS_MCP_PORT=1417\nHAYHOOKS_PIPELINES_DIR=./pipelines\nLOG=INFO\n</code></pre>"},{"location":"features/cli-commands/#error-handling","title":"Error Handling","text":""},{"location":"features/cli-commands/#common-errors","title":"Common Errors","text":"<ol> <li>Server already running</li> </ol> <pre><code># Check if server is running\nhayhooks status\n\n# Kill existing process\npkill -f \"hayhooks run\"\n</code></pre> <ol> <li>Pipeline deployment failed</li> </ol> <pre><code># Check server logs with your process manager or container runtime\n\n# Enable debug logging\nLOG=DEBUG hayhooks run\n</code></pre> <ol> <li>Permission denied</li> </ol> <pre><code># Check file permissions\nls -la ./path/to/pipeline\n\n# Fix permissions if needed\nchmod +x ./path/to/pipeline/pipeline_wrapper.py\n</code></pre>"},{"location":"features/cli-commands/#debug-mode","title":"Debug Mode","text":"<p>Enable debug mode for troubleshooting:</p> <pre><code># Set debug logging\nexport LOG=DEBUG\n\n# Start server with debug logging\nhayhooks run\n</code></pre>"},{"location":"features/cli-commands/#examples","title":"Examples","text":""},{"location":"features/cli-commands/#basic-workflow","title":"Basic Workflow","text":"<pre><code># 1. Start server\nhayhooks run --port 1416\n\n# 2. In another terminal, deploy pipeline\nhayhooks pipeline deploy-files -n chat_pipeline ./pipelines/chat\n\n# 3. Check status\nhayhooks status\n\n# 4. Run pipeline\nhayhooks pipeline run chat_pipeline --param 'query=\"Hello!\"'\n\n# 5. Check logs\n# Use your process manager or container logs\n</code></pre>"},{"location":"features/cli-commands/#production-deployment","title":"Production Deployment","text":"<pre><code># 1. Set environment variables\nexport HAYHOOKS_HOST=0.0.0.0\nexport HAYHOOKS_PORT=1416\nexport LOG=INFO\n\n# 2. Start server with multiple workers\nhayhooks run --workers 4\n\n# 3. Deploy pipelines\nhayhooks pipeline deploy-files -n production_pipeline ./pipelines/production\n\n# 4. Monitor status\nhayhooks status\n</code></pre>"},{"location":"features/cli-commands/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - Configuration options</li> <li>Examples - Working examples</li> </ul>"},{"location":"features/file-response-support/","title":"File Response Support","text":"<p>Hayhooks supports returning binary files (images, PDFs, audio, etc.) directly from <code>run_api</code> endpoints. When <code>run_api</code> returns a FastAPI <code>Response</code> object, Hayhooks passes it straight to the client \u2014 bypassing JSON serialization entirely. See also the FastAPI docs on custom responses.</p>"},{"location":"features/file-response-support/#overview","title":"Overview","text":"<p>File response support enables you to:</p> <ul> <li>Return images, PDFs, audio files, or any binary content from pipelines</li> <li>Use FastAPI's <code>FileResponse</code>, <code>StreamingResponse</code>, or plain <code>Response</code></li> <li>Serve generated content (e.g. AI-generated images) directly to clients</li> <li>Set custom headers like <code>Content-Disposition</code> for download behavior</li> </ul>"},{"location":"features/file-response-support/#basic-implementation","title":"Basic Implementation","text":"<p>To return a file from your pipeline, return a FastAPI <code>Response</code> (or any subclass) from <code>run_api</code>:</p> <pre><code>from fastapi.responses import FileResponse\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def setup(self) -&gt; None:\n        pass\n\n    def run_api(self, prompt: str) -&gt; FileResponse:\n        # Generate or retrieve a file...\n        image_path = generate_image(prompt)\n\n        return FileResponse(\n            path=image_path,\n            media_type=\"image/png\",\n            filename=\"result.png\",\n        )\n</code></pre>"},{"location":"features/file-response-support/#response-types","title":"Response Types","text":"<p>You can use any FastAPI/Starlette response class:</p>"},{"location":"features/file-response-support/#fileresponse","title":"FileResponse","text":"<p>Best for serving files from disk:</p> <pre><code>from fastapi.responses import FileResponse\n\ndef run_api(self, document_id: str) -&gt; FileResponse:\n    path = self.get_document_path(document_id)\n    return FileResponse(\n        path=path,\n        media_type=\"application/pdf\",\n        filename=\"document.pdf\",\n    )\n</code></pre>"},{"location":"features/file-response-support/#response","title":"Response","text":"<p>Best for returning in-memory bytes:</p> <pre><code>from fastapi.responses import Response\n\ndef run_api(self, prompt: str) -&gt; Response:\n    image_bytes = self.generate_image(prompt)\n    return Response(\n        content=image_bytes,\n        media_type=\"image/png\",\n        headers={\"Content-Disposition\": 'inline; filename=\"image.png\"'},\n    )\n</code></pre>"},{"location":"features/file-response-support/#streamingresponse","title":"StreamingResponse","text":"<p>Best for large files or on-the-fly generation:</p> <pre><code>import io\nfrom fastapi.responses import StreamingResponse\n\ndef run_api(self, query: str) -&gt; StreamingResponse:\n    audio_buffer = self.generate_audio(query)\n    return StreamingResponse(\n        content=io.BytesIO(audio_buffer),\n        media_type=\"audio/wav\",\n        headers={\"Content-Disposition\": 'attachment; filename=\"audio.wav\"'},\n    )\n</code></pre>"},{"location":"features/file-response-support/#how-it-works","title":"How It Works","text":"<p>When Hayhooks deploys a pipeline whose <code>run_api</code> return type is a <code>Response</code> subclass (or a generator), three things happen at deploy time:</p> <ol> <li> <p><code>response_model=None</code>: <code>create_response_model_from_callable</code> detects the <code>Response</code> return type and returns <code>None</code> instead of a Pydantic model. This tells FastAPI to skip response validation and not generate a JSON schema for this endpoint.</p> </li> <li> <p><code>response_class</code>: <code>get_response_class_from_callable</code> returns the concrete response class (e.g. <code>FileResponse</code>, <code>StreamingResponse</code>) so that OpenAPI docs show the correct Content-Type for the endpoint instead of <code>application/json</code>.</p> </li> <li> <p>At runtime: The endpoint handler checks if the result is a <code>Response</code> instance and returns it directly, skipping JSON wrapping:</p> <pre><code># From deploy_utils.py\nif isinstance(result, Response):\n    return result\n</code></pre> </li> </ol> <p>This is the same mechanism used for streaming generators \u2014 both bypass Pydantic serialization. Generators additionally get <code>StreamingResponse</code> as their <code>response_class</code>.</p>"},{"location":"features/file-response-support/#api-usage","title":"API Usage","text":""},{"location":"features/file-response-support/#curl","title":"curl","text":"<pre><code># Generate and download an image\ncurl -X POST \"http://localhost:1416/image_pipeline/run\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"prompt\": \"A sunset over mountains\"}' \\\n  --output result.png\n</code></pre>"},{"location":"features/file-response-support/#python","title":"Python","text":"<pre><code>import requests\n\nresponse = requests.post(\n    \"http://localhost:1416/image_pipeline/run\",\n    json={\"prompt\": \"A sunset over mountains\"},\n)\n\n# Save the binary content\nwith open(\"result.png\", \"wb\") as f:\n    f.write(response.content)\n</code></pre>"},{"location":"features/file-response-support/#browser","title":"Browser","text":"<p>File responses with <code>Content-Disposition: inline</code> will display directly in the browser. Use <code>Content-Disposition: attachment</code> to trigger a download.</p>"},{"location":"features/file-response-support/#complete-example-image-generation","title":"Complete Example: Image Generation","text":"<p>This example uses the Hugging Face Inference API to generate images from text prompts:</p> <pre><code>import tempfile\n\nfrom fastapi.responses import FileResponse\n\nfrom hayhooks import BasePipelineWrapper, log\n\nDEFAULT_MODEL = \"black-forest-labs/FLUX.1-schnell\"\n\n\nclass PipelineWrapper(BasePipelineWrapper):\n    \"\"\"Generate images from text prompts using Hugging Face Inference API.\"\"\"\n\n    def setup(self) -&gt; None:\n        from huggingface_hub import InferenceClient\n\n        self.client = InferenceClient()\n\n    def run_api(\n        self,\n        prompt: str,\n        width: int = 512,\n        height: int = 512,\n        model: str = DEFAULT_MODEL,\n    ) -&gt; FileResponse:\n        \"\"\"Generate an image from a text prompt and return it as a PNG file.\"\"\"\n        log.info(\"Generating image for prompt: '{}'\", prompt)\n\n        image = self.client.text_to_image(\n            prompt=prompt, model=model, width=width, height=height,\n        )\n\n        with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmp:\n            image.save(tmp, format=\"PNG\")\n\n        return FileResponse(\n            path=tmp.name,\n            media_type=\"image/png\",\n            filename=\"generated_image.png\",\n        )\n</code></pre> <p>Deploy and test:</p> <pre><code># Deploy\nhayhooks run --pipelines-dir examples/pipeline_wrappers/image_generation\n\n# Generate an image\ncurl -X POST \"http://localhost:1416/image_generation/run\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"prompt\": \"A cat sitting on a rainbow\"}' \\\n  --output generated_image.png\n</code></pre> <p>For the full example code, see examples/pipeline_wrappers/image_generation.</p>"},{"location":"features/file-response-support/#next-steps","title":"Next Steps","text":"<ul> <li>PipelineWrapper - Learn about wrapper implementation</li> <li>File Upload Support - Accept file uploads in pipelines</li> <li>Examples - See working examples</li> </ul>"},{"location":"features/file-upload-support/","title":"File Upload Support","text":"<p>Hayhooks provides built-in support for handling file uploads in your pipelines, making it easy to create applications that process documents, images, and other files.</p>"},{"location":"features/file-upload-support/#overview","title":"Overview","text":"<p>File upload support enables you to:</p> <ul> <li>Accept file uploads through REST APIs</li> <li>Process multiple files in a single request</li> <li>Combine file uploads with other parameters</li> <li>Build document processing and RAG pipelines</li> </ul>"},{"location":"features/file-upload-support/#basic-implementation","title":"Basic Implementation","text":"<p>To accept file uploads in your pipeline, add a <code>files</code> parameter to your <code>run_api</code> method:</p> <pre><code>from fastapi import UploadFile\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def run_api(self, files: list[UploadFile] | None = None, query: str = \"\") -&gt; str:\n        if not files:\n            return \"No files provided\"\n\n        # Process files here...\n        return f\"Processed {len(files)} files\"\n</code></pre> <p>For a complete implementation, see the RAG System Example.</p>"},{"location":"features/file-upload-support/#api-usage","title":"API Usage","text":""},{"location":"features/file-upload-support/#multipart-form-data-requests","title":"Multipart Form Data Requests","text":"<p>File uploads use <code>multipart/form-data</code> format:</p> <pre><code># Upload single file\ncurl -X POST \\\n  http://localhost:1416/my_pipeline/run \\\n  -F 'files=@document.pdf' \\\n  -F 'query=\"Summarize this document\"'\n\n# Upload multiple files\ncurl -X POST \\\n  http://localhost:1416/my_pipeline/run \\\n  -F 'files=@document1.pdf' \\\n  -F 'files=@document2.txt' \\\n  -F 'query=\"Compare these documents\"'\n\n# Upload with additional parameters\ncurl -X POST \\\n  http://localhost:1416/my_pipeline/run \\\n  -F 'files=@document.pdf' \\\n  -F 'query=\"Analyze this document\"'\n</code></pre>"},{"location":"features/file-upload-support/#python-client-example","title":"Python Client Example","text":"<pre><code>import requests\n\n# Upload files\nfiles = [\n    ('files', open('document.pdf', 'rb')),\n    ('files', open('notes.txt', 'rb'))\n]\n\ndata = {\n    'query': 'Analyze these documents'\n}\n\nresponse = requests.post(\n    'http://localhost:1416/my_pipeline/run',\n    files=files,\n    data=data\n)\n\nprint(response.json())\n</code></pre>"},{"location":"features/file-upload-support/#cli-usage","title":"CLI Usage","text":"<p>Hayhooks CLI supports file uploads:</p> <pre><code># Upload single file\nhayhooks pipeline run my_pipeline --file document.pdf --param 'query=\"Summarize this\"'\n\n# Upload directory\nhayhooks pipeline run my_pipeline --dir ./documents --param 'query=\"Analyze all documents\"'\n\n# Upload multiple files\nhayhooks pipeline run my_pipeline --file doc1.pdf --file doc2.txt --param 'query=\"Compare documents\"'\n\n# Upload with parameters\nhayhooks pipeline run my_pipeline --file document.pdf --param 'query=\"Analyze\"'\n</code></pre>"},{"location":"features/file-upload-support/#combining-files-with-other-parameters","title":"Combining Files with Other Parameters","text":"<p>You can handle both files and parameters in the same request by adding them as arguments to the <code>run_api</code> method:</p> <pre><code>from fastapi import UploadFile\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def run_api(\n        self,\n        files: list[UploadFile] | None = None,\n        query: str = \"\",\n        additional_param: str = \"default\"\n    ) -&gt; str:\n        if files and len(files) &gt; 0:\n            filenames = [f.filename for f in files if f.filename is not None]\n            return f\"Received files: {', '.join(filenames)} with query: {query}\"\n\n        return \"No files received\"\n</code></pre>"},{"location":"features/file-upload-support/#complete-example-rag-system-with-file-upload","title":"Complete Example: RAG System with File Upload","text":"<p>For a complete, production-ready example of a RAG system with file uploads, including document indexing and querying with Elasticsearch, see:</p> <ul> <li>RAG System Example - Full RAG implementation guide</li> <li>examples/rag_indexing_query - Complete working code with:</li> <li>Document indexing pipeline with file upload support</li> <li>Query pipeline for retrieving and generating answers</li> <li>Elasticsearch integration</li> <li>Support for PDF, Markdown, and text files</li> </ul>"},{"location":"features/file-upload-support/#next-steps","title":"Next Steps","text":"<ul> <li>PipelineWrapper - Learn about wrapper implementation</li> <li>Examples - See working examples</li> <li>CLI Commands - CLI usage for file uploads</li> </ul>"},{"location":"features/mcp-support/","title":"MCP Support","text":"<p>Hayhooks supports the Model Context Protocol and can act as an MCP Server, exposing pipelines and agents as MCP tools for use in AI development environments.</p>"},{"location":"features/mcp-support/#overview","title":"Overview","text":"<p>The Hayhooks MCP Server:</p> <ul> <li>Exposes Core Tools for controlling Hayhooks directly from IDEs</li> <li>Exposes deployed Haystack pipelines as usable MCP Tools</li> <li>Supports both Server-Sent Events (SSE) and Streamable HTTP transports</li> <li>Integrates with AI development environments like Cursor and Claude Desktop</li> </ul>"},{"location":"features/mcp-support/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+ for MCP support</li> <li>Install with <code>pip install hayhooks[mcp]</code></li> </ul>"},{"location":"features/mcp-support/#getting-started","title":"Getting Started","text":""},{"location":"features/mcp-support/#install-with-mcp-support","title":"Install with MCP Support","text":"<pre><code>pip install hayhooks[mcp]\n</code></pre>"},{"location":"features/mcp-support/#start-mcp-server","title":"Start MCP Server","text":"<pre><code>hayhooks mcp run\n</code></pre> <p>This starts the MCP server on <code>HAYHOOKS_MCP_HOST:HAYHOOKS_MCP_PORT</code> (default: <code>localhost:1417</code>).</p>"},{"location":"features/mcp-support/#configuration","title":"Configuration","text":"<p>Environment variables for MCP server:</p> <pre><code>HAYHOOKS_MCP_HOST=localhost    # MCP server host\nHAYHOOKS_MCP_PORT=1417         # MCP server port\n</code></pre>"},{"location":"features/mcp-support/#transports","title":"Transports","text":""},{"location":"features/mcp-support/#streamable-http-transport","title":"Streamable HTTP (Recommended)","text":"<p>The preferred transport for modern MCP clients:</p> <pre><code>import mcp\n\nclient = mcp.Client(\"http://localhost:1417/mcp\")\n</code></pre>"},{"location":"features/mcp-support/#sse-transport","title":"Server-Sent Events (SSE)","text":"<p>Legacy transport maintained for backward compatibility:</p> <pre><code>import mcp\n\nclient = mcp.Client(\"http://localhost:1417/sse\")\n</code></pre>"},{"location":"features/mcp-support/#core-mcp-tools","title":"Core MCP Tools","text":"<p>Hayhooks provides core tools for managing pipelines:</p>"},{"location":"features/mcp-support/#get_all_pipeline_statuses","title":"get_all_pipeline_statuses","text":"<p>Get status of all deployed pipelines:</p> <pre><code>result = await client.call_tool(\"get_all_pipeline_statuses\")\n</code></pre>"},{"location":"features/mcp-support/#get_pipeline_status","title":"get_pipeline_status","text":"<p>Get status of a specific pipeline:</p> <pre><code>result = await client.call_tool(\"get_pipeline_status\", {\"pipeline_name\": \"my_pipeline\"})\n</code></pre>"},{"location":"features/mcp-support/#deploy_pipeline","title":"deploy_pipeline","text":"<p>Deploy a pipeline from files:</p> <pre><code>result = await client.call_tool(\"deploy_pipeline\", {\n    \"name\": \"my_pipeline\",\n    \"files\": [\n        {\"name\": \"pipeline_wrapper.py\", \"content\": \"...\"},\n        {\"name\": \"pipeline.yml\", \"content\": \"...\"}\n    ],\n    \"save_files\": True,\n    \"overwrite\": False\n})\n</code></pre>"},{"location":"features/mcp-support/#undeploy_pipeline","title":"undeploy_pipeline","text":"<p>Undeploy a pipeline:</p> <pre><code>result = await client.call_tool(\"undeploy_pipeline\", {\"pipeline_name\": \"my_pipeline\"})\n</code></pre>"},{"location":"features/mcp-support/#pipeline-tools","title":"Pipeline Tools","text":""},{"location":"features/mcp-support/#pipelinewrapper-as-mcp-tool","title":"PipelineWrapper as MCP Tool","text":"<p>When you deploy a pipeline with <code>PipelineWrapper</code>, it's automatically exposed as an MCP tool.</p> <p>MCP Tool Requirements:</p> <p>A MCP Tool requires:</p> <ul> <li><code>name</code>: The name of the tool</li> <li><code>description</code>: The description of the tool</li> <li><code>inputSchema</code>: JSON Schema describing the tool's input parameters</li> </ul> <p>How Hayhooks Creates MCP Tools:</p> <p>For each deployed pipeline, Hayhooks will:</p> <ul> <li>Use the pipeline wrapper <code>name</code> as MCP Tool <code>name</code> (always present)</li> <li>Parse <code>run_api</code> method docstring:</li> <li>If you use Google-style or reStructuredText-style docstrings, use the first line as MCP Tool <code>description</code> and the rest as <code>parameters</code> (if present)</li> <li>Each parameter description will be used as the <code>description</code> of the corresponding Pydantic model field (if present)</li> <li>Generate a Pydantic model from the <code>inputSchema</code> using the <code>run_api</code> method arguments as fields</li> </ul> <p>Example:</p> <pre><code>from pathlib import Path\nfrom haystack import Pipeline\nfrom hayhooks import BasePipelineWrapper\n\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def setup(self) -&gt; None:\n        pipeline_yaml = (Path(__file__).parent / \"chat_with_website.yml\").read_text()\n        self.pipeline = Pipeline.loads(pipeline_yaml)\n\n    def run_api(self, urls: list[str], question: str) -&gt; str:\n        #\n        # NOTE: The following docstring will be used as MCP Tool description\n        #\n        \"\"\"\n        Ask a question about one or more websites using a Haystack pipeline.\n        \"\"\"\n        result = self.pipeline.run({\"fetcher\": {\"urls\": urls}, \"prompt\": {\"query\": question}})\n        return result[\"llm\"][\"replies\"][0]\n</code></pre>"},{"location":"features/mcp-support/#yaml-pipeline-as-mcp-tool","title":"YAML Pipeline as MCP Tool","text":"<p>YAML-deployed pipelines are also automatically exposed as MCP tools. When you deploy via <code>hayhooks pipeline deploy-yaml</code>, the pipeline becomes available as an MCP tool with its input schema derived from the YAML <code>inputs</code> section.</p> <p>For complete examples and detailed information, see YAML Pipeline Deployment.</p>"},{"location":"features/mcp-support/#skip-mcp-tool-listing","title":"Skip MCP Tool Listing","text":"<p>To prevent a pipeline from being listed as an MCP tool:</p> <pre><code>class PipelineWrapper(BasePipelineWrapper):\n    skip_mcp = True  # This pipeline won't be listed as an MCP tool\n\n    def setup(self) -&gt; None:\n        ...\n\n    def run_api(self, ...) -&gt; str:\n        ...\n</code></pre>"},{"location":"features/mcp-support/#ide-integration","title":"IDE Integration","text":""},{"location":"features/mcp-support/#cursor-integration","title":"Cursor Integration","text":"<p>Add Hayhooks MCP Server in Cursor Settings \u2192 MCP:</p> <pre><code>{\n  \"mcpServers\": {\n    \"hayhooks\": {\n      \"url\": \"http://localhost:1417/mcp\"\n    }\n  }\n}\n</code></pre> <p>Once configured, you can deploy, manage, and run pipelines directly from Cursor chat using the Core MCP Tools.</p> <p>For more information about MCP in Cursor, see the Cursor MCP Documentation.</p>"},{"location":"features/mcp-support/#claude-desktop-integration","title":"Claude Desktop Integration","text":"<p>Configure Claude Desktop to connect to Hayhooks MCP Server:</p> <p>Claude Desktop Tiers</p> Free TierPro/Max/Teams/Enterprise <p>Use supergateway to bridge the connection</p> <pre><code>{\n  \"mcpServers\": {\n    \"hayhooks\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"supergateway\", \"--streamableHttp\", \"http://localhost:1417/mcp\"]\n    }\n  }\n}\n</code></pre> <p>Direct connection via Streamable HTTP or SSE</p> <pre><code>{\n  \"mcpServers\": {\n    \"hayhooks\": {\n      \"url\": \"http://localhost:1417/mcp\"\n    }\n  }\n}\n</code></pre>"},{"location":"features/mcp-support/#development-workflow","title":"Development Workflow","text":"<p>Basic workflow:</p> <ol> <li>Start Hayhooks server: <code>hayhooks run</code></li> <li>Start MCP server: <code>hayhooks mcp run</code> (in another terminal)</li> <li>Configure your IDE to connect to the MCP server</li> <li>Deploy and manage pipelines through your IDE using natural language</li> </ol>"},{"location":"features/mcp-support/#tool-development","title":"Tool Development","text":""},{"location":"features/mcp-support/#custom-tool-descriptions","title":"Custom Tool Descriptions","text":"<p>Use docstrings to provide better tool descriptions:</p> <pre><code>def run_api(self, urls: list[str], question: str) -&gt; str:\n    \"\"\"\n    Ask questions about website content using AI.\n\n    This tool analyzes website content and provides answers to user questions.\n    It's perfect for research, content analysis, and information extraction.\n\n    Args:\n        urls: List of website URLs to analyze\n        question: Question to ask about the content\n\n    Returns:\n        Answer to the question based on the website content\n    \"\"\"\n    result = self.pipeline.run({\"fetcher\": {\"urls\": urls}, \"prompt\": {\"query\": question}})\n    return result[\"llm\"][\"replies\"][0]\n</code></pre>"},{"location":"features/mcp-support/#input-validation","title":"Input Validation","text":"<p>Hayhooks automatically validates inputs based on your method signature:</p> <pre><code>def run_api(\n    self,\n    urls: list[str],           # Required: List of URLs\n    question: str,             # Required: User question\n    max_tokens: int = 1000     # Optional: Max tokens\n) -&gt; str:\n    ...\n</code></pre>"},{"location":"features/mcp-support/#security-considerations","title":"Security Considerations","text":""},{"location":"features/mcp-support/#authentication","title":"Authentication","text":"<p>Currently, Hayhooks MCP server doesn't include built-in authentication. Consider:</p> <ul> <li>Running behind a reverse proxy with authentication</li> <li>Using network-level security (firewalls, VPNs)</li> <li>Implementing custom middleware for authentication</li> </ul>"},{"location":"features/mcp-support/#resource-management","title":"Resource Management","text":"<ul> <li>Monitor tool execution for resource usage</li> <li>Implement timeouts for long-running operations</li> <li>Consider rate limiting for production deployments</li> </ul>"},{"location":"features/mcp-support/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/mcp-support/#common-issues","title":"Common Issues","text":""},{"location":"features/mcp-support/#connection-refused","title":"Connection Refused","text":"<p>If you cannot connect to the MCP server, ensure the MCP server is running with <code>hayhooks mcp run</code>. Check that the port configuration matches (default is <code>1417</code>), and verify network connectivity between the client and server.</p>"},{"location":"features/mcp-support/#tool-not-found","title":"Tool Not Found","text":"<p>If an MCP tool is not showing up, verify that the pipeline is properly deployed using <code>hayhooks status</code>. Check if the <code>skip_mcp</code> class attribute is set to <code>True</code> in your <code>PipelineWrapper</code>, which would prevent it from being listed. Ensure the <code>run_api</code> method is properly implemented with correct type hints.</p>"},{"location":"features/mcp-support/#input-validation-errors","title":"Input Validation Errors","text":"<p>If you're getting validation errors when calling tools, check that your method signatures match the expected input types. Verify that all required parameters are being passed and that data types match the type hints in your <code>run_api</code> method signature. Review the MCP tool's <code>inputSchema</code> to ensure parameter names and types are correct.</p>"},{"location":"features/mcp-support/#debug-commands","title":"Debug Commands","text":"<p>The MCP server exposes the following endpoints:</p> <ul> <li>Streamable HTTP endpoint: <code>http://localhost:1417/mcp</code> - Main MCP protocol endpoint</li> <li>SSE endpoint: <code>http://localhost:1417/sse</code> - Server-Sent Events transport (deprecated)</li> <li>Status/Health Check: <code>http://localhost:1417/status</code> - Returns <code>{\"status\": \"ok\"}</code> for health monitoring</li> </ul>"},{"location":"features/mcp-support/#testing-the-health-endpoint","title":"Testing the health endpoint","text":"<pre><code># Check if MCP server is running\ncurl http://localhost:1417/status\n\n# Expected response:\n# {\"status\":\"ok\"}\n</code></pre> <p>This status endpoint is useful for:</p> <ul> <li>Container health checks in Docker/Kubernetes deployments</li> <li>Load balancer health probes</li> <li>Monitoring and alerting systems</li> <li>Verifying the MCP server is running before connecting clients</li> </ul> <p>Use an MCP-capable client like supergateway, Cursor, or Claude Desktop to list and call tools. Example supergateway usage is shown above.</p>"},{"location":"features/mcp-support/#next-steps","title":"Next Steps","text":"<ul> <li>PipelineWrapper Guide - Learn how to create MCP-compatible pipeline wrappers</li> <li>Examples - See working examples of deployed pipelines</li> </ul>"},{"location":"features/openai-compatibility/","title":"OpenAI Compatibility","text":"<p>Hayhooks provides OpenAI-compatible endpoints for Haystack pipelines and agents, enabling integration with OpenAI-compatible tools and frameworks.</p> <p>Open WebUI Integration</p> <p>Looking to integrate with Open WebUI? Check out the complete Open WebUI Integration guide for detailed setup instructions, event handling, and advanced features.</p>"},{"location":"features/openai-compatibility/#overview","title":"Overview","text":"<p>Hayhooks can automatically generate OpenAI-compatible endpoints if you implement the <code>run_chat_completion</code> or <code>run_chat_completion_async</code> method in your pipeline wrapper. This makes Hayhooks compatible with any OpenAI-compatible client or tool, including chat interfaces, agent frameworks, and custom applications.</p>"},{"location":"features/openai-compatibility/#key-features","title":"Key Features","text":"<ul> <li>Automatic Endpoint Generation: OpenAI-compatible endpoints are created automatically</li> <li>Streaming Support: Real-time streaming responses for chat interfaces</li> <li>Async Support: High-performance async chat completion</li> <li>Multiple Integration Options: Works with various OpenAI-compatible clients</li> <li>Open WebUI Ready: Full support for Open WebUI with events and tool call interception</li> </ul>"},{"location":"features/openai-compatibility/#implementation","title":"Implementation","text":""},{"location":"features/openai-compatibility/#basic-chat-completion","title":"Basic Chat Completion","text":"<pre><code>from pathlib import Path\nfrom typing import Union, Generator\nfrom haystack import Pipeline\nfrom hayhooks import get_last_user_message, BasePipelineWrapper, log\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def setup(self) -&gt; None:\n        # Initialize your pipeline\n        pipeline_yaml = (Path(__file__).parent / \"pipeline.yml\").read_text()\n        self.pipeline = Pipeline.loads(pipeline_yaml)\n\n    def run_chat_completion(self, model: str, messages: list[dict], body: dict) -&gt; str | Generator:\n        log.trace(\"Running pipeline with model: {}, messages: {}, body: {}\", model, messages, body)\n\n        question = get_last_user_message(messages)\n        log.trace(\"Question: {}\", question)\n\n        # Pipeline run, returns a string\n        result = self.pipeline.run({\"prompt\": {\"query\": question}})\n        return result[\"llm\"][\"replies\"][0]\n</code></pre>"},{"location":"features/openai-compatibility/#async-chat-completion-with-streaming","title":"Async Chat Completion with Streaming","text":"<pre><code>from collections.abc import AsyncGenerator\n\nfrom hayhooks import async_streaming_generator, get_last_user_message, log\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def setup(self) -&gt; None:\n        # Initialize async pipeline\n        pipeline_yaml = (Path(__file__).parent / \"pipeline.yml\").read_text()\n        self.pipeline = AsyncPipeline.loads(pipeline_yaml)\n\n    async def run_chat_completion_async(self, model: str, messages: list[dict], body: dict) -&gt; AsyncGenerator:\n        log.trace(\"Running pipeline with model: {}, messages: {}, body: {}\", model, messages, body)\n\n        question = get_last_user_message(messages)\n        log.trace(\"Question: {}\", question)\n\n        # Async streaming pipeline run\n        return async_streaming_generator(\n            pipeline=self.pipeline,\n            pipeline_run_args={\"prompt\": {\"query\": question}},\n        )\n</code></pre>"},{"location":"features/openai-compatibility/#method-signatures","title":"Method Signatures","text":""},{"location":"features/openai-compatibility/#run_chat_completion","title":"run_chat_completion(...)","text":"<pre><code>def run_chat_completion(self, model: str, messages: list[dict], body: dict) -&gt; str | Generator:\n    \"\"\"\n    Run the pipeline for OpenAI-compatible chat completion.\n\n    Args:\n        model: The pipeline name\n        messages: List of messages in OpenAI format\n        body: Full request body with additional parameters\n\n    Returns:\n        str: Non-streaming response\n        Generator: Streaming response generator\n    \"\"\"\n</code></pre>"},{"location":"features/openai-compatibility/#run_chat_completion_async","title":"run_chat_completion_async(...)","text":"<pre><code>async def run_chat_completion_async(self, model: str, messages: list[dict], body: dict) -&gt; str | AsyncGenerator:\n    \"\"\"\n    Async version of run_chat_completion.\n\n    Args:\n        model: The pipeline name\n        messages: List of messages in OpenAI format\n        body: Full request body with additional parameters\n\n    Returns:\n        str: Non-streaming response\n        AsyncGenerator: Streaming response generator\n    \"\"\"\n</code></pre>"},{"location":"features/openai-compatibility/#generated-endpoints","title":"Generated Endpoints","text":"<p>When you implement chat completion methods, Hayhooks automatically creates:</p>"},{"location":"features/openai-compatibility/#chat-endpoints","title":"Chat Endpoints","text":"<ul> <li><code>/{pipeline_name}/chat</code> - Direct chat endpoint for a specific pipeline</li> <li><code>/chat/completions</code> - OpenAI-compatible endpoint (routes to the model specified in request)</li> <li><code>/v1/chat/completions</code> - OpenAI API v1 compatible endpoint</li> </ul> <p>All endpoints support the standard OpenAI chat completion request format:</p> <pre><code>{\n  \"model\": \"pipeline_name\",\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"Your message\"}\n  ],\n  \"stream\": false\n}\n</code></pre>"},{"location":"features/openai-compatibility/#available-models","title":"Available Models","text":"<p>Use the <code>/v1/models</code> endpoint to list all deployed pipelines that support chat completion:</p> <pre><code>curl http://localhost:1416/v1/models\n</code></pre>"},{"location":"features/openai-compatibility/#streaming-responses","title":"Streaming Responses","text":""},{"location":"features/openai-compatibility/#streaming-generator","title":"Streaming Generator","text":"<pre><code>from hayhooks import streaming_generator\n\ndef run_chat_completion(self, model: str, messages: list[dict], body: dict) -&gt; Generator:\n    question = get_last_user_message(messages)\n\n    return streaming_generator(\n        pipeline=self.pipeline,\n        pipeline_run_args={\"prompt\": {\"query\": question}},\n    )\n</code></pre>"},{"location":"features/openai-compatibility/#async-streaming-generator","title":"Async Streaming Generator","text":"<pre><code>from hayhooks import async_streaming_generator\n\nasync def run_chat_completion_async(self, model: str, messages: list[dict], body: dict) -&gt; AsyncGenerator:\n    question = get_last_user_message(messages)\n\n    return async_streaming_generator(\n        pipeline=self.pipeline,\n        pipeline_run_args={\"prompt\": {\"query\": question}},\n    )\n</code></pre>"},{"location":"features/openai-compatibility/#using-hayhooks-with-haystacks-openaichatgenerator","title":"Using Hayhooks with Haystack's OpenAIChatGenerator","text":"<p>Hayhooks' OpenAI-compatible endpoints can be used as a backend for Haystack's <code>OpenAIChatGenerator</code>, enabling you to create pipelines that consume other Hayhooks-deployed pipelines:</p> <pre><code>from haystack.components.generators.chat import OpenAIChatGenerator\nfrom haystack.utils import Secret\nfrom haystack.dataclasses import ChatMessage\n\n# Connect to a Hayhooks-deployed pipeline\nclient = OpenAIChatGenerator(\n    model=\"chat_with_website\",  # Your deployed pipeline name\n    api_key=Secret.from_token(\"not-used\"),  # Hayhooks doesn't require authentication\n    api_base_url=\"http://localhost:1416/v1/\",\n    streaming_callback=lambda chunk: print(chunk.content, end=\"\")\n)\n\n# Use it like any OpenAI client\nresult = client.run([ChatMessage.from_user(\"What is Haystack?\")])\nprint(result[\"replies\"][0].content)\n</code></pre> <p>This enables powerful use cases:</p> <ul> <li>Pipeline Composition: Chain multiple Hayhooks pipelines together</li> <li>Testing: Test your pipelines using Haystack's testing tools</li> <li>Hybrid Deployments: Mix local and remote pipeline execution</li> </ul> <p>Limitations</p> <p>If you customize your Pipeline wrapper to emit Open WebUI Events, it may break out-of-the-box compatibility with Haystack's <code>OpenAIChatGenerator</code>.</p>"},{"location":"features/openai-compatibility/#examples","title":"Examples","text":""},{"location":"features/openai-compatibility/#sync-chat-pipeline-non-streaming","title":"Sync Chat Pipeline (Non-Streaming)","text":"<pre><code>class SyncChatWrapper(BasePipelineWrapper):\n    def setup(self) -&gt; None:\n        from haystack.components.builders import ChatPromptBuilder\n        from haystack.components.generators.chat import OpenAIChatGenerator\n        from haystack.dataclasses import ChatMessage\n\n        template = [ChatMessage.from_user(\"Answer: {{query}}\")]\n        chat_prompt_builder = ChatPromptBuilder(template=template)\n        llm = OpenAIChatGenerator(model=\"gpt-4o-mini\")\n\n        self.pipeline = Pipeline()\n        self.pipeline.add_component(\"chat_prompt_builder\", chat_prompt_builder)\n        self.pipeline.add_component(\"llm\", llm)\n        self.pipeline.connect(\"chat_prompt_builder.prompt\", \"llm.messages\")\n\n    def run_chat_completion(self, model: str, messages: list[dict], body: dict) -&gt; str:\n        question = get_last_user_message(messages)\n        result = self.pipeline.run({\"chat_prompt_builder\": {\"query\": question}})\n        return result[\"llm\"][\"replies\"][0].content\n</code></pre>"},{"location":"features/openai-compatibility/#async-streaming-pipeline","title":"Async Streaming Pipeline","text":"<pre><code>class AsyncStreamingWrapper(BasePipelineWrapper):\n    def setup(self) -&gt; None:\n        from haystack import AsyncPipeline\n        from haystack.components.builders import ChatPromptBuilder\n        from haystack.components.generators.chat import OpenAIChatGenerator\n        from haystack.dataclasses import ChatMessage\n\n        template = [ChatMessage.from_user(\"Answer: {{query}}\")]\n        chat_prompt_builder = ChatPromptBuilder(template=template)\n        llm = OpenAIChatGenerator(model=\"gpt-4o\")\n\n        self.pipeline = AsyncPipeline()\n        self.pipeline.add_component(\"chat_prompt_builder\", chat_prompt_builder)\n        self.pipeline.add_component(\"llm\", llm)\n        self.pipeline.connect(\"chat_prompt_builder.prompt\", \"llm.messages\")\n\n    async def run_chat_completion_async(self, model: str, messages: list[dict], body: dict) -&gt; AsyncGenerator:\n        question = get_last_user_message(messages)\n        return async_streaming_generator(\n            pipeline=self.pipeline,\n            pipeline_run_args={\"chat_prompt_builder\": {\"query\": question}},\n        )\n</code></pre>"},{"location":"features/openai-compatibility/#request-parameters","title":"Request Parameters","text":"<p>The OpenAI-compatible endpoints support standard parameters from the <code>body</code> argument:</p> <pre><code>def run_chat_completion(self, model: str, messages: list[dict], body: dict) -&gt; str:\n    # Access additional parameters\n    temperature = body.get(\"temperature\", 0.7)\n    max_tokens = body.get(\"max_tokens\", 150)\n    stream = body.get(\"stream\", False)\n\n    # Use them in your pipeline\n    result = self.pipeline.run({\n        \"llm\": {\n            \"generation_kwargs\": {\n                \"temperature\": temperature,\n                \"max_tokens\": max_tokens\n            }\n        }\n    })\n    return result[\"llm\"][\"replies\"][0].content\n</code></pre> <p>Common parameters include:</p> <ul> <li><code>temperature</code>: Controls randomness (0.0 to 2.0)</li> <li><code>max_tokens</code>: Maximum number of tokens to generate</li> <li><code>stream</code>: Enable streaming responses</li> <li><code>stop</code>: Stop sequences</li> <li><code>top_p</code>: Nucleus sampling parameter</li> </ul> <p>See the OpenAI API reference for the complete list of parameters.</p>"},{"location":"features/openai-compatibility/#next-steps","title":"Next Steps","text":"<ul> <li>Open WebUI Integration - Use Hayhooks with Open WebUI chat interface</li> <li>Examples - Working examples and use cases</li> <li>File Upload Support - Handle file uploads in pipelines</li> </ul>"},{"location":"features/openwebui-integration/","title":"Open WebUI Integration","text":"<p>Hayhooks provides seamless integration with Open WebUI, enabling you to use Haystack pipelines and agents as chat completion backends with full feature support.</p>"},{"location":"features/openwebui-integration/#overview","title":"Overview","text":"<p>Open WebUI integration allows you to:</p> <ul> <li>Use Haystack pipelines as OpenAI-compatible chat backends</li> <li>Support streaming responses in real-time</li> <li>Send status events to enhance user experience</li> <li>Intercept tool calls for better feedback</li> </ul>"},{"location":"features/openwebui-integration/#getting-started","title":"Getting Started","text":""},{"location":"features/openwebui-integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Open WebUI instance running</li> <li>Hayhooks server running</li> <li>Pipeline with chat completion support</li> </ul>"},{"location":"features/openwebui-integration/#configuration","title":"Configuration","text":""},{"location":"features/openwebui-integration/#1-install-open-webui","title":"1. Install Open WebUI","text":"<p>Please follow the Open WebUI installation guide to install Open WebUI.</p> <p>We recommend using Docker to install Open WebUI.</p> <p>A quick command to install Open WebUI using Docker is:</p> <pre><code>docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -e WEBUI_AUTH=False -v open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:main\n</code></pre> <p>This will start Open WebUI on local port 3000, with no authentication, and with the data stored in the <code>open-webui</code> volume. It's the easiest way to get started.</p>"},{"location":"features/openwebui-integration/#2-disable-auto-generated-content","title":"2. Disable Auto-generated Content","text":"<p>Open WebUI automatically generates content for your pipelines. More precisely, it calls your pipelines to generate tags, title and follow-up messages. Depending on your pipeline, this may not be suitable for this use case.</p> <p>We recommend disabling those features as a starting point, then you can enable them if you need them.</p> <p>Go to Admin Settings \u2192 Interface and turn off the following features:</p> <p></p>"},{"location":"features/openwebui-integration/#3-add-hayhooks-as-an-openai-compatible-api-endpoint","title":"3. Add Hayhooks as an OpenAI compatible API endpoint","text":"<p>You have two options to connect Hayhooks to Open WebUI:</p>"},{"location":"features/openwebui-integration/#option-1-direct-connection-recommended","title":"Option 1: Direct Connection (Recommended)","text":"<p>First, enable Direct Connections in Open WebUI.</p> <p>Go to Admin Settings \u2192 Connections and enable Direct Connections:</p> <p></p> <p>Then go to Settings \u2192 Connections and add a new connection:</p> <ul> <li>API Base URL: <code>http://localhost:1416</code></li> <li>API Key: <code>any-value</code> (or leave it empty, it's not used by Hayhooks)</li> </ul> <p></p>"},{"location":"features/openwebui-integration/#option-2-openai-api-connection","title":"Option 2: OpenAI API Connection","text":"<p>Alternatively, you can add Hayhooks as an additional OpenAI API Connection from Admin Settings \u2192 Connections:</p> <p></p> <p>In both cases, remember to fill a random value as API key (Hayhooks doesn't require authentication).</p>"},{"location":"features/openwebui-integration/#pipeline-implementation","title":"Pipeline Implementation","text":"<p>To make your pipeline work with Open WebUI, implement the <code>run_chat_completion</code> or <code>run_chat_completion_async</code> method in your <code>PipelineWrapper</code>. See the OpenAI Compatibility guide for detailed implementation examples.</p>"},{"location":"features/openwebui-integration/#non-streaming-example","title":"Non-Streaming Example","text":"<p>Here's how a non-streaming chat completion looks in Open WebUI:</p> <p></p>"},{"location":"features/openwebui-integration/#streaming-example","title":"Streaming Example","text":"<p>With streaming enabled, responses appear in real-time:</p> <p></p>"},{"location":"features/openwebui-integration/#open-webui-events","title":"Open WebUI Events","text":"<p>Hayhooks supports sending events to Open WebUI for enhanced user experience:</p>"},{"location":"features/openwebui-integration/#available-events","title":"Available Events","text":"<ul> <li>status: Show progress updates and loading indicators</li> <li>message: Append content to the current message</li> <li>replace: Replace the current message content entirely</li> <li>notification: Show toast notifications (info, success, warning, error)</li> <li>source: Add references, citations, or code execution results</li> </ul> <p>Limitations</p> <p>Customizing your Pipeline wrapper to emit Open WebUI Events may break out-of-the-box compatibility with Haystack's <code>OpenAIChatGenerator</code>. Your pipeline will still function normally, but it may not be directly consumable through <code>OpenAIChatGenerator</code>.</p>"},{"location":"features/openwebui-integration/#event-implementation","title":"Event Implementation","text":"<pre><code>from typing import AsyncGenerator\nfrom hayhooks import async_streaming_generator, get_last_user_message, BasePipelineWrapper\nfrom hayhooks.open_webui import create_status_event, create_message_event, OpenWebUIEvent\n\nclass PipelineWrapper(BasePipelineWrapper):\n    async def run_chat_completion_async(self, model: str, messages: list[dict], body: dict) -&gt; AsyncGenerator[str | OpenWebUIEvent, None]:\n        # Indicate loading\n        yield create_status_event(\"Processing your request...\", done=False)\n\n        question = get_last_user_message(messages)\n\n        try:\n            # Stream model output alongside events\n            result = async_streaming_generator(\n                pipeline=self.pipeline,\n                pipeline_run_args={\"prompt_builder\": {\"query\": question}},\n            )\n\n            # Optional UI hint\n            yield create_message_event(\"\u270d\ufe0f Generating response...\")\n\n            async for chunk in result:\n                yield chunk\n\n            yield create_status_event(\"Request completed successfully\", done=True)\n        except Exception as e:\n            yield create_status_event(\"Request failed\", done=True)\n            yield create_message_event(f\"Error: {str(e)}\")\n            raise\n</code></pre> <p>Here's how Open WebUI events enhance the user experience:</p> <p></p>"},{"location":"features/openwebui-integration/#tool-call-interception","title":"Tool Call Interception","text":"<p>For agent pipelines, you can intercept tool calls to provide real-time feedback:</p> <pre><code>def on_tool_call_start(tool_name: str, arguments: dict, tool_id: str):\n    \"\"\"Called when a tool call starts\"\"\"\n    print(f\"Tool call started: {tool_name}\")\n\n\ndef on_tool_call_end(tool_name: str, arguments: dict, result: dict, error: bool):\n    \"\"\"Called when a tool call ends\"\"\"\n    print(f\"Tool call ended: {tool_name}, Error: {error}\")\n\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def run_chat_completion(self, model: str, messages: list[dict], body: dict) -&gt; Generator:\n        return streaming_generator(\n            pipeline=self.pipeline,\n            pipeline_run_args={\"messages\": messages},\n            on_tool_call_start=on_tool_call_start,\n            on_tool_call_end=on_tool_call_end,\n        )\n</code></pre> <p>Here's an example of tool call interception in action:</p> <p></p>"},{"location":"features/openwebui-integration/#streaming-the-final-pipeline-output","title":"Streaming the Final Pipeline Output","text":"<p>The <code>on_pipeline_end</code> callback lets you customize what happens after a pipeline finishes running when using <code>streaming_generator</code> or <code>async_streaming_generator</code>. You can use it to return the final output of the pipeline, or add additional information (e.g. adding sources).</p> <p>Here\u2019s how you can define and use <code>on_pipeline_end</code>:</p> <pre><code>def on_pipeline_end(result: dict):\n    \"\"\"Called after the pipeline completes. Can format or summarize results.\"\"\"\n    documents = result.get(\"documents\", [])\n    references_str = \"\\n\\n### All Sources:\\n\"\n    if documents:\n        for idx, doc in enumerate(documents):\n            references_str += f\"- [{idx + 1}] {doc.meta['url']}\\n\"\n    return references_str\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def run_chat_completion(self, model: str, messages: list[dict], body: dict) -&gt; Generator:\n        return streaming_generator(\n            pipeline=self.pipeline,\n            pipeline_run_args={\"messages\": messages},\n            on_pipeline_end=on_pipeline_end,\n        )\n</code></pre> <p>With this setup, after the pipeline finishes, the <code>on_pipeline_end</code> function is called with the pipeline\u2019s result. You can use it to append a list of sources, add a summary, or perform any other final formatting. This final output is then sent to Open WebUI as part of the chat response.</p> <p>Here\u2019s an example of post-processing with <code>on_pipeline_end</code> in action:</p> <p></p>"},{"location":"features/openwebui-integration/#openapi-tool-server","title":"OpenAPI Tool Server","text":"<p>Hayhooks can serve as an OpenAPI Tool Server for Open WebUI, exposing its core API endpoints as tools that can be used directly from the chat interface.</p> <p>OpenAPI Tool Servers are a standard way to integrate external tools and data sources into LLM agents using the widely-adopted OpenAPI specification. This approach offers several advantages:</p> <ul> <li>Standard Protocol: Uses the established OpenAPI specification - no proprietary protocols to learn</li> <li>Easy Integration: If you build REST APIs today, you're already familiar with the approach</li> <li>Secure: Built on HTTP/REST with standard authentication methods (OAuth, JWT, API Keys)</li> <li>Flexible Deployment: Can be hosted locally or externally without vendor lock-in</li> </ul> <p>Since Hayhooks exposes its OpenAPI schema at <code>/openapi.json</code>, Open WebUI can automatically discover and integrate all available Hayhooks endpoints as tools.</p>"},{"location":"features/openwebui-integration/#setup","title":"Setup","text":"<ol> <li>Go to Settings \u2192 Tools in Open WebUI</li> <li>Add OpenAPI Tool Server:</li> <li>Name: Hayhooks</li> <li>URL: <code>http://localhost:1416/openapi.json</code></li> </ol>"},{"location":"features/openwebui-integration/#available-tools","title":"Available Tools","text":"<p>Once configured, the following Hayhooks operations become available as tools in your Open WebUI chat:</p> <ul> <li>Deploy Pipeline: Deploy new pipelines from your chat interface</li> <li>Undeploy Pipeline: Remove existing pipelines</li> <li>Run Pipeline: Execute deployed pipelines with parameters</li> <li>Get Status: Check the status and list of all deployed pipelines</li> </ul> <p>This enables you to manage your entire Hayhooks deployment directly through natural language conversations.</p>"},{"location":"features/openwebui-integration/#example-deploy-a-haystack-pipeline-from-open-webui-chat-interface","title":"Example: Deploy a Haystack pipeline from <code>open-webui</code> chat interface","text":"<p>Here's a video example of how to deploy a Haystack pipeline from the <code>open-webui</code> chat interface:</p> <p></p>"},{"location":"features/openwebui-integration/#example-chat-with-website","title":"Example: Chat with Website","text":"<p>Here's a complete example for a website chat pipeline:</p> <pre><code>from typing import AsyncGenerator\nfrom haystack import AsyncPipeline\nfrom haystack.components.fetchers import LinkContentFetcher\nfrom haystack.components.converters import HTMLToDocument\nfrom haystack.components.builders import ChatPromptBuilder\nfrom haystack.components.generators.chat import OpenAIChatGenerator\nfrom haystack.dataclasses import ChatMessage\nfrom hayhooks import BasePipelineWrapper, async_streaming_generator, get_last_user_message\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def setup(self) -&gt; None:\n        fetcher = LinkContentFetcher()\n        converter = HTMLToDocument()\n\n        template = [\n            ChatMessage.from_user(\n                \"Based on this content: {{documents}}\\nAnswer: {{query}}\"\n            )\n        ]\n        chat_prompt_builder = ChatPromptBuilder(template=template)\n\n        llm = OpenAIChatGenerator(model=\"gpt-4o\")\n\n        self.pipeline = AsyncPipeline()\n        self.pipeline.add_component(\"fetcher\", fetcher)\n        self.pipeline.add_component(\"converter\", converter)\n        self.pipeline.add_component(\"chat_prompt_builder\", chat_prompt_builder)\n        self.pipeline.add_component(\"llm\", llm)\n        self.pipeline.connect(\"fetcher.content\", \"converter\")\n        self.pipeline.connect(\"converter.documents\", \"chat_prompt_builder.documents\")\n        self.pipeline.connect(\"chat_prompt_builder.prompt\", \"llm.messages\")\n\n    async def run_chat_completion_async(self, model: str, messages: list[dict], body: dict) -&gt; AsyncGenerator:\n        question = get_last_user_message(messages)\n\n        # Extract URLs from messages or use defaults\n        urls = [\"https://haystack.deepset.ai\"]  # Default URL\n\n        return async_streaming_generator(\n            pipeline=self.pipeline,\n            pipeline_run_args={\n                \"fetcher\": {\"urls\": urls},\n                \"chat_prompt_builder\": {\"query\": question}\n            },\n        )\n</code></pre>"},{"location":"features/openwebui-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/openwebui-integration/#common-issues","title":"Common Issues","text":"<ol> <li>Connection Failed</li> <li>Verify Hayhooks server is running</li> <li>Check API URL in Open WebUI settings</li> <li> <p>Ensure correct port (1416 by default)</p> </li> <li> <p>No Response</p> </li> <li>Check if pipeline implements <code>run_chat_completion</code></li> <li>Verify pipeline is deployed</li> <li> <p>Check server logs for errors</p> </li> <li> <p>Streaming Not Working</p> </li> <li>Ensure <code>streaming_callback</code> is set on generator</li> <li>Check if <code>run_chat_completion_async</code> is implemented</li> <li>Verify Open WebUI streaming is enabled</li> </ol>"},{"location":"features/openwebui-integration/#debug-commands","title":"Debug Commands","text":"<pre><code># Check Hayhooks status\nhayhooks status\n\n# Check deployed pipelines\ncurl http://localhost:1416/status\n\n# Test chat completion endpoint (OpenAI-compatible)\ncurl -X POST http://localhost:1416/v1/chat/completions \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"model\": \"my_pipeline\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"test message\"}],\n    \"stream\": false\n  }'\n\n# Test streaming chat completion\ncurl -X POST http://localhost:1416/v1/chat/completions \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"model\": \"my_pipeline\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"test message\"}],\n    \"stream\": true\n  }'\n</code></pre>"},{"location":"features/openwebui-integration/#next-steps","title":"Next Steps","text":"<ul> <li>OpenAI Compatibility - Implementation details for chat completion methods</li> <li>Open WebUI Events Example - Advanced UI integration patterns</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>Hayhooks can be configured through environment variables, command-line arguments, or <code>.env</code> files.</p>"},{"location":"getting-started/configuration/#configuration-methods","title":"Configuration Methods","text":""},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>Set environment variables before starting Hayhooks:</p> <pre><code>export HAYHOOKS_HOST=0.0.0.0\nexport HAYHOOKS_PORT=1416\nhayhooks run\n</code></pre>"},{"location":"getting-started/configuration/#env-file","title":".env File","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># .env\nHAYHOOKS_HOST=0.0.0.0\nHAYHOOKS_PORT=1416\nHAYHOOKS_PIPELINES_DIR=./pipelines\nLOG=INFO\n</code></pre>"},{"location":"getting-started/configuration/#command-line-arguments","title":"Command Line Arguments","text":"<p>Pass options directly to <code>hayhooks run</code>:</p> <pre><code>hayhooks run --host 0.0.0.0 --port 1416 --pipelines-dir ./pipelines\n</code></pre>"},{"location":"getting-started/configuration/#common-configuration-options","title":"Common Configuration Options","text":"<p>The most frequently used options:</p> <ul> <li><code>HAYHOOKS_HOST</code> - Host to bind to (default: <code>localhost</code>)</li> <li><code>HAYHOOKS_PORT</code> - Port to listen on (default: <code>1416</code>)</li> <li><code>HAYHOOKS_PIPELINES_DIR</code> - Pipeline directory for auto-deployment (default: <code>./pipelines</code>)</li> <li><code>LOG</code> - Log level: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code> (default: <code>INFO</code>)</li> </ul> <p>For the complete list of all environment variables and detailed descriptions, see the Environment Variables Reference.</p>"},{"location":"getting-started/configuration/#example-configurations","title":"Example Configurations","text":""},{"location":"getting-started/configuration/#development","title":"Development","text":"<pre><code># .env.development\nHAYHOOKS_HOST=localhost\nHAYHOOKS_PORT=1416\nLOG=DEBUG\nHAYHOOKS_SHOW_TRACEBACKS=true\n</code></pre>"},{"location":"getting-started/configuration/#production","title":"Production","text":"<pre><code># .env.production\nHAYHOOKS_HOST=0.0.0.0\nHAYHOOKS_PORT=1416\nLOG=INFO\nHAYHOOKS_SHOW_TRACEBACKS=false\n</code></pre>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Get started with basic usage</li> <li>Pipeline Deployment - Learn how to deploy pipelines</li> <li>Environment Variables Reference - Complete configuration reference</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide covers how to install Hayhooks and its dependencies.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python 3.10+</li> <li>Operating System: Linux, macOS, or Windows</li> <li>Memory: Minimum 512MB RAM, 2GB+ recommended</li> <li>Storage: Minimum 100MB free space</li> </ul>"},{"location":"getting-started/installation/#install-from-pypi","title":"Install from PyPI","text":"Standard InstallationWith MCP SupportFrom Source <pre><code>pip install hayhooks\n</code></pre> <p>This includes all core features for deploying and running pipelines.</p> <pre><code>pip install hayhooks[mcp]\n</code></pre> <p>Includes all standard features plus MCP Server support for integration with AI development tools like Cursor and Claude Desktop.</p> <p>Python 3.10+ Required</p> <p>You'll need to run at least Python 3.10+ to use the MCP Server.</p> <pre><code>git clone https://github.com/deepset-ai/hayhooks.git\ncd hayhooks\npip install -e .\n</code></pre> <p>Useful for development or testing the latest unreleased features.</p>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>After installation, verify that Hayhooks is installed correctly:</p> <pre><code># Check version\nhayhooks --version\n\n# Show help\nhayhooks --help\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>If you want to contribute to Hayhooks, we recommend using Hatch, the project's build and environment management tool:</p> <pre><code># Clone the repository\ngit clone https://github.com/deepset-ai/hayhooks.git\ncd hayhooks\n\n# Install Hatch (if not already installed)\npip install hatch\n\n# Run unit tests\nhatch run test:unit\n\n# Run integration tests\nhatch run test:integration\n\n# Run tests\nhatch run test:all\n\n# Format code\nhatch run fmt\n\n# Serve documentation locally\nhatch run docs:serve\n</code></pre> <p>Hatch automatically manages virtual environments and dependencies for you. See available commands in <code>pyproject.toml</code>.</p>"},{"location":"getting-started/installation/#alternative-manual-installation","title":"Alternative: Manual Installation","text":"<p>If you prefer manual setup:</p> <pre><code># Clone the repository\ngit clone https://github.com/deepset-ai/hayhooks.git\ncd hayhooks\n\n# Create a virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install in development mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#docker-installation","title":"Docker Installation","text":""},{"location":"getting-started/installation/#using-docker-hub","title":"Using Docker Hub","text":"<pre><code># Pull the image corresponding to Hayhooks main branch\ndocker pull deepset/hayhooks:main\n\n# Run Hayhooks\ndocker run -p 1416:1416 deepset/hayhooks:main\n</code></pre> <p>You can inspect all available images on Docker Hub.</p>"},{"location":"getting-started/installation/#building-from-source","title":"Building from Source","text":"<pre><code># Clone the repository\ngit clone https://github.com/deepset-ai/hayhooks.git\ncd hayhooks\n\n# Build with Docker Buildx Bake (current platform) and load into Docker\ncd docker\nIMAGE_NAME=hayhooks IMAGE_TAG_SUFFIX=local docker buildx bake --load\n\n# Run the image\ndocker run -p 1416:1416 hayhooks:local\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>After successful installation:</p> <ul> <li>Quick Start - Get started with basic usage</li> <li>Configuration - Configure Hayhooks for your needs</li> <li>Examples - Explore example implementations</li> </ul>"},{"location":"getting-started/quick-start-docker/","title":"Quick Start with Docker Compose","text":"<p>To quickly get started with Hayhooks, we provide a ready-to-use Docker Compose \ud83d\udc33 setup with pre-configured integration with Open WebUI.</p> <p>It's available in the Hayhooks + Open WebUI Docker Compose repository.</p>"},{"location":"getting-started/quick-start-docker/#setup-instructions","title":"Setup Instructions","text":""},{"location":"getting-started/quick-start-docker/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/deepset-ai/hayhooks-open-webui-docker-compose.git\ncd hayhooks-open-webui-docker-compose\n</code></pre>"},{"location":"getting-started/quick-start-docker/#2-start-the-services","title":"2. Start the Services","text":"<pre><code>docker-compose up -d\n</code></pre> <p>This will start:</p> <ul> <li>Hayhooks server on port 1416</li> <li>Hayhooks MCP server on port 1417</li> <li>Open WebUI on port 3000</li> </ul>"},{"location":"getting-started/quick-start-docker/#3-access-the-services","title":"3. Access the Services","text":"<ul> <li>Hayhooks API: http://localhost:1416</li> <li>Open WebUI: http://localhost:3000</li> <li>Hayhooks API Documentation: http://localhost:1416/docs</li> </ul>"},{"location":"getting-started/quick-start-docker/#4-deploy-example-pipelines","title":"4. Deploy Example Pipelines","text":"<p>Install Hayhooks locally to use the CLI:</p> <pre><code>pip install hayhooks\n</code></pre> <p>Then deploy example pipelines:</p> <pre><code># Deploy a sample pipeline\nhayhooks pipeline deploy-files -n chat_with_website pipelines/chat_with_website_streaming\n</code></pre> <p>Alternative: Deploy via API</p> <p>You can also deploy pipelines using the HTTP API endpoints: <code>POST /deploy_files</code> (PipelineWrapper files) or <code>POST /deploy-yaml</code> (YAML pipeline definition). See the API Reference for details.</p>"},{"location":"getting-started/quick-start-docker/#configuration-options","title":"Configuration Options","text":""},{"location":"getting-started/quick-start-docker/#environment-variables","title":"Environment Variables","text":"<p>The following environment variables can be configured in <code>.env</code>:</p> Variable Description Default <code>OPEN_WEBUI_PORT</code> Port for Open WebUI <code>3000</code> <code>WEBUI_DOCKER_TAG</code> Tag for Open WebUI image <code>main</code>"},{"location":"getting-started/quick-start-docker/#volume-mounts","title":"Volume Mounts","text":"<p>The Docker Compose setup includes the following volume mounts:</p> <ul> <li>Pipeline Directory: <code>/pipelines</code> \u2013 Directory mounted inside the Hayhooks container where your pipeline wrappers or YAML files live. Hayhooks auto-deploys anything it finds here at startup.</li> </ul>"},{"location":"getting-started/quick-start-docker/#integrating-with-open-webui","title":"Integrating with Open WebUI","text":"<p>The Docker Compose setup comes pre-configured to integrate Hayhooks with Open WebUI:</p>"},{"location":"getting-started/quick-start-docker/#1-configure-open-webui","title":"1. Configure Open WebUI","text":"<ol> <li>Access Open WebUI at http://localhost:3000</li> <li>Go to Settings \u2192 Connections</li> <li>Add a new connection with:</li> <li>API Base URL: <code>http://hayhooks:1416/v1</code></li> <li>API Key: <code>any-value</code> (not used by Hayhooks)</li> </ol>"},{"location":"getting-started/quick-start-docker/#2-deploy-a-pipeline","title":"2. Deploy a Pipeline","text":"<p>Deploy a pipeline that supports chat completion:</p> <pre><code>hayhooks pipeline deploy-files -n chat_with_website pipelines/chat_with_website_streaming\n</code></pre> <p>Alternatively, use the API endpoints (<code>POST /deploy_files</code> or <code>POST /deploy-yaml</code>).</p>"},{"location":"getting-started/quick-start-docker/#3-test-the-integration","title":"3. Test the Integration","text":"<ol> <li>In Open WebUI, select the Hayhooks backend</li> <li>Start a conversation with your deployed pipeline</li> <li>The pipeline will respond through the Open WebUI interface</li> </ol>"},{"location":"getting-started/quick-start-docker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quick-start-docker/#common-issues","title":"Common Issues","text":"<ol> <li>Port Conflicts: Ensure ports 1416, 1417, and 3000 are available</li> <li>Permission Issues: Make sure Docker has proper permissions</li> <li>Network Issues: Check that containers can communicate with each other</li> </ol>"},{"location":"getting-started/quick-start-docker/#logs","title":"Logs","text":"<p>Check logs for troubleshooting:</p> <pre><code># Hayhooks logs\ndocker-compose logs -f hayhooks\n\n# Open WebUI logs\ndocker-compose logs -f openwebui\n</code></pre>"},{"location":"getting-started/quick-start-docker/#cleanup","title":"Cleanup","text":"<p>To stop and remove all containers:</p> <pre><code>docker-compose down -v\n</code></pre>"},{"location":"getting-started/quick-start-docker/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - Learn about advanced configuration</li> <li>Examples - Explore more examples</li> <li>Open WebUI Integration - Deep dive into Open WebUI integration</li> </ul>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>This guide will help you get started with Hayhooks quickly.</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>A Haystack pipeline or agent to deploy</li> </ul>"},{"location":"getting-started/quick-start/#installation","title":"Installation","text":"<p>See Installation for detailed setup instructions.</p> <p>Quick install:</p> <pre><code>pip install hayhooks\n</code></pre>"},{"location":"getting-started/quick-start/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quick-start/#1-start-hayhooks","title":"1. Start Hayhooks","text":"<pre><code>hayhooks run\n</code></pre> <p>This will start the Hayhooks server on <code>http://localhost:1416</code> by default.</p>"},{"location":"getting-started/quick-start/#2-deploy-a-pipeline","title":"2. Deploy a Pipeline","text":"<p>Deploy a pipeline using the <code>deploy-files</code> command:</p> <pre><code>hayhooks pipeline deploy-files -n chat_with_website examples/pipeline_wrappers/chat_with_website_streaming\n</code></pre>"},{"location":"getting-started/quick-start/#3-check-status","title":"3. Check Status","text":"<p>Verify your pipeline is deployed:</p> <pre><code>hayhooks status\n</code></pre>"},{"location":"getting-started/quick-start/#4-run-your-pipeline","title":"4. Run Your Pipeline","text":"<p>Run your pipeline via the API:</p> cURLPythonHayhooks CLI <pre><code>curl -X POST \\\n  http://localhost:1416/chat_with_website/run \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"urls\": [\"https://haystack.deepset.ai\"], \"question\": \"What is Haystack?\"}'\n</code></pre> <pre><code>import requests\n\nresponse = requests.post(\n    \"http://localhost:1416/chat_with_website/run\",\n    json={\n        \"urls\": [\"https://haystack.deepset.ai\"],\n        \"question\": \"What is Haystack?\"\n    }\n)\nprint(response.json())\n</code></pre> <pre><code>hayhooks pipeline run chat_with_website \\\n  --param 'urls=[\"https://haystack.deepset.ai\"]' \\\n  --param 'question=\"What is Haystack?\"'\n</code></pre>"},{"location":"getting-started/quick-start/#quick-start-with-docker-compose","title":"Quick Start with Docker Compose","text":"<p>For the fastest setup with Open WebUI integration, see Quick Start with Docker Compose.</p>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Pipeline Deployment - Learn deployment methods</li> <li>Examples - Explore example implementations</li> </ul>"},{"location":"reference/api-reference/","title":"API Reference","text":"<p>Hayhooks provides a comprehensive REST API for managing and executing Haystack pipelines and agents.</p>"},{"location":"reference/api-reference/#base-url","title":"Base URL","text":"<pre><code>http://localhost:1416\n</code></pre>"},{"location":"reference/api-reference/#authentication","title":"Authentication","text":"<p>Currently, Hayhooks does not include built-in authentication. Consider implementing:</p> <ul> <li>Reverse proxy authentication</li> <li>Network-level security</li> <li>Custom middleware</li> </ul>"},{"location":"reference/api-reference/#endpoints","title":"Endpoints","text":""},{"location":"reference/api-reference/#pipeline-management","title":"Pipeline Management","text":""},{"location":"reference/api-reference/#deploy-pipeline-files","title":"Deploy Pipeline (files)","text":"<pre><code>POST /deploy_files\n</code></pre> <p>Request Body:</p> <pre><code>{\n  \"name\": \"pipeline_name\",\n  \"files\": {\n    \"pipeline_wrapper.py\": \"...file content...\",\n    \"other.py\": \"...\"\n  },\n  \"save_files\": true,\n  \"overwrite\": false\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"status\": \"success\",\n  \"message\": \"Pipeline deployed successfully\"\n}\n</code></pre>"},{"location":"reference/api-reference/#undeploy-pipeline","title":"Undeploy Pipeline","text":"<pre><code>POST /undeploy/{pipeline_name}\n</code></pre> <p>Remove a deployed pipeline.</p> <p>Response:</p> <pre><code>{\n  \"status\": \"success\",\n  \"message\": \"Pipeline undeployed successfully\"\n}\n</code></pre>"},{"location":"reference/api-reference/#get-pipeline-status","title":"Get Pipeline Status","text":"<pre><code>GET /status/{pipeline_name}\n</code></pre> <p>Check the status of a specific pipeline.</p> <p>Response:</p> <pre><code>{\n  \"status\": \"Up!\",\n  \"pipeline\": \"pipeline_name\"\n}\n</code></pre>"},{"location":"reference/api-reference/#get-all-pipeline-statuses","title":"Get All Pipeline Statuses","text":"<pre><code>GET /status\n</code></pre> <p>Get status of all deployed pipelines.</p> <p>Response:</p> <pre><code>{\n  \"pipelines\": [\n    \"pipeline1\",\n    \"pipeline2\"\n  ],\n  \"status\": \"Up!\"\n}\n</code></pre>"},{"location":"reference/api-reference/#pipeline-execution","title":"Pipeline Execution","text":""},{"location":"reference/api-reference/#run-pipeline","title":"Run Pipeline","text":"<pre><code>POST /{pipeline_name}/run\n</code></pre> <p>Execute a deployed pipeline.</p> <p>Request Body:</p> <pre><code>{\n  \"query\": \"What is the capital of France?\"\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"result\": \"The capital of France is Paris.\"\n}\n</code></pre>"},{"location":"reference/api-reference/#openai-compatibility","title":"OpenAI Compatibility","text":""},{"location":"reference/api-reference/#chat-completion","title":"Chat Completion","text":"<pre><code>POST /chat/completions\nPOST /v1/chat/completions\n</code></pre> <p>OpenAI-compatible chat completion endpoint.</p> <p>Request Body:</p> <pre><code>{\n  \"model\": \"pipeline_name\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello, how are you?\"\n    }\n  ],\n  \"stream\": false\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"id\": \"chat-123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1677652288,\n  \"model\": \"pipeline_name\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Hello! I'm doing well, thank you for asking.\"\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 12,\n    \"completion_tokens\": 20,\n    \"total_tokens\": 32\n  }\n}\n</code></pre>"},{"location":"reference/api-reference/#streaming-chat-completion","title":"Streaming Chat Completion","text":"<p>Use the same endpoints with <code>\"stream\": true</code>. Hayhooks streams chunks in OpenAI-compatible format.</p>"},{"location":"reference/api-reference/#mcp-server","title":"MCP Server","text":"<p>MCP runs in a separate Starlette app when invoked via <code>hayhooks mcp run</code>. Use the configured Streamable HTTP endpoint <code>/mcp</code> or SSE <code>/sse</code> depending on your client. See the MCP feature page for details.</p>"},{"location":"reference/api-reference/#interactive-api-documentation","title":"Interactive API Documentation","text":"<p>Hayhooks provides interactive API documentation for exploring and testing endpoints:</p> <ul> <li>Swagger UI: <code>http://localhost:1416/docs</code> - Interactive API explorer with built-in request testing</li> <li>ReDoc: <code>http://localhost:1416/redoc</code> - Clean, responsive API documentation</li> </ul>"},{"location":"reference/api-reference/#openapi-schema","title":"OpenAPI Schema","text":""},{"location":"reference/api-reference/#get-openapi-schema","title":"Get OpenAPI Schema","text":"<pre><code>GET /openapi.json\nGET /openapi.yaml\n</code></pre> <p>Get the complete OpenAPI specification for programmatic access or tooling integration.</p>"},{"location":"reference/api-reference/#error-handling","title":"Error Handling","text":""},{"location":"reference/api-reference/#error-response-format","title":"Error Response Format","text":"<pre><code>{\n  \"error\": {\n    \"message\": \"Error description\",\n    \"type\": \"invalid_request_error\",\n    \"code\": 400\n  }\n}\n</code></pre>"},{"location":"reference/api-reference/#common-error-codes","title":"Common Error Codes","text":"<ul> <li>400 Bad Request: Invalid request parameters</li> <li>404 Not Found: Pipeline or endpoint not found</li> <li>500 Internal Server Error: Server-side error</li> </ul>"},{"location":"reference/api-reference/#rate-limiting","title":"Rate Limiting","text":"<p>Currently, Hayhooks does not include built-in rate limiting. Consider implementing:</p> <ul> <li>Reverse proxy rate limiting</li> <li>Custom middleware</li> <li>Request throttling</li> </ul>"},{"location":"reference/api-reference/#examples","title":"Examples","text":""},{"location":"reference/api-reference/#running-a-pipeline","title":"Running a Pipeline","text":"cURLPythonHayhooks CLI <pre><code>curl -X POST http://localhost:1416/chat_pipeline/run \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"query\": \"Hello!\"}'\n</code></pre> <pre><code>import requests\n\nresponse = requests.post(\n    \"http://localhost:1416/chat_pipeline/run\",\n    json={\"query\": \"Hello!\"}\n)\nprint(response.json())\n</code></pre> <pre><code>hayhooks pipeline run chat_pipeline --param 'query=\"Hello!\"'\n</code></pre>"},{"location":"reference/api-reference/#openai-compatible-chat-completion","title":"OpenAI-Compatible Chat Completion","text":"cURLPythonOpenAI Python SDK <pre><code>curl -X POST http://localhost:1416/v1/chat/completions \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"model\": \"chat_pipeline\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n  }'\n</code></pre> <pre><code>import requests\n\nresponse = requests.post(\n    \"http://localhost:1416/v1/chat/completions\",\n    json={\n        \"model\": \"chat_pipeline\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Hello!\"}\n        ]\n    }\n)\nprint(response.json())\n</code></pre> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://localhost:1416/v1\",\n    api_key=\"not-needed\"  # Hayhooks doesn't require auth by default\n)\n\nresponse = client.chat.completions.create(\n    model=\"chat_pipeline\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"reference/api-reference/#next-steps","title":"Next Steps","text":"<ul> <li>Environment Variables - Configuration options</li> <li>Logging - Logging configuration</li> </ul>"},{"location":"reference/environment-variables/","title":"Environment Variables","text":"<p>Hayhooks can be configured via environment variables (loaded with prefix <code>HAYHOOKS_</code> or <code>LOG</code>). This page lists the canonical variables supported by the codebase.</p>"},{"location":"reference/environment-variables/#server","title":"Server","text":""},{"location":"reference/environment-variables/#hayhooks_host","title":"HAYHOOKS_HOST","text":"<ul> <li>Default: <code>localhost</code></li> <li>Description: Host for the FastAPI app</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_port","title":"HAYHOOKS_PORT","text":"<ul> <li>Default: <code>1416</code></li> <li>Description: Port for the FastAPI app</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_root_path","title":"HAYHOOKS_ROOT_PATH","text":"<ul> <li>Default: <code>\"\"</code></li> <li>Description: Root path to mount the API under (FastAPI <code>root_path</code>)</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_pipelines_dir","title":"HAYHOOKS_PIPELINES_DIR","text":"<ul> <li>Default: <code>./pipelines</code></li> <li>Description: Directory containing pipelines to auto-deploy on startup</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_additional_python_path","title":"HAYHOOKS_ADDITIONAL_PYTHON_PATH","text":"<ul> <li>Default: <code>\"\"</code></li> <li>Description: Additional path appended to <code>sys.path</code> for wrapper imports</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_use_https","title":"HAYHOOKS_USE_HTTPS","text":"<ul> <li>Default: <code>false</code></li> <li>Description: Use HTTPS when the CLI calls the server (affects CLI only)</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_disable_ssl","title":"HAYHOOKS_DISABLE_SSL","text":"<ul> <li>Default: <code>false</code></li> <li>Description: Disable SSL verification for CLI calls</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_show_tracebacks","title":"HAYHOOKS_SHOW_TRACEBACKS","text":"<ul> <li>Default: <code>false</code></li> <li>Description: Include tracebacks in error messages (server and MCP)</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_streaming_components","title":"HAYHOOKS_STREAMING_COMPONENTS","text":"<ul> <li>Default: <code>\"\"</code> (empty string)</li> <li>Description: Global configuration for which pipeline components should stream</li> <li>Options:</li> <li><code>\"\"</code> (empty): Stream only the last capable component (default)</li> <li><code>\"all\"</code>: Stream all streaming-capable components</li> <li>Comma-separated list: <code>\"llm_1,llm_2\"</code> to enable specific components</li> </ul> <p>Priority Order</p> <p>Pipeline-specific settings (via <code>streaming_components</code> parameter or YAML) override this global default.</p> <p>Component-Specific Control</p> <p>For component-specific control, use the <code>streaming_components</code> parameter in your code or YAML configuration instead of the environment variable to specify exactly which components should stream.</p> <p>Examples:</p> <pre><code># Stream all components globally\nexport HAYHOOKS_STREAMING_COMPONENTS=\"all\"\n\n# Stream specific components (comma-separated, spaces are trimmed)\nexport HAYHOOKS_STREAMING_COMPONENTS=\"llm_1,llm_2\"\nexport HAYHOOKS_STREAMING_COMPONENTS=\"llm_1, llm_2, llm_3\"\n</code></pre>"},{"location":"reference/environment-variables/#mcp","title":"MCP","text":""},{"location":"reference/environment-variables/#hayhooks_mcp_host","title":"HAYHOOKS_MCP_HOST","text":"<ul> <li>Default: <code>localhost</code></li> <li>Description: Host for the MCP server</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_mcp_port","title":"HAYHOOKS_MCP_PORT","text":"<ul> <li>Default: <code>1417</code></li> <li>Description: Port for the MCP server</li> </ul>"},{"location":"reference/environment-variables/#chainlit-ui","title":"Chainlit UI","text":""},{"location":"reference/environment-variables/#hayhooks_chainlit_enabled","title":"HAYHOOKS_CHAINLIT_ENABLED","text":"<ul> <li>Default: <code>false</code></li> <li>Description: Enable the embedded Chainlit chat UI</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_chainlit_path","title":"HAYHOOKS_CHAINLIT_PATH","text":"<ul> <li>Default: <code>/chat</code></li> <li>Description: URL path where the Chainlit UI is mounted</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_chainlit_app","title":"HAYHOOKS_CHAINLIT_APP","text":"<ul> <li>Default: <code>\"\"</code> (uses built-in default app)</li> <li>Description: Path to a custom Chainlit app file</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_chainlit_default_model","title":"HAYHOOKS_CHAINLIT_DEFAULT_MODEL","text":"<ul> <li>Default: <code>\"\"</code> (auto-selects if only one pipeline is deployed)</li> <li>Description: Default pipeline/model to auto-select in the Chainlit UI</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_chainlit_request_timeout","title":"HAYHOOKS_CHAINLIT_REQUEST_TIMEOUT","text":"<ul> <li>Default: <code>120.0</code></li> <li>Description: Timeout in seconds for chat completion requests from the Chainlit UI</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_chainlit_custom_elements_dir","title":"HAYHOOKS_CHAINLIT_CUSTOM_ELEMENTS_DIR","text":"<ul> <li>Default: <code>\"\"</code> (no custom elements)</li> <li>Description: Path to a directory containing custom <code>.jsx</code> element files. These files are copied into the Chainlit <code>public/elements/</code> directory at startup and become available as <code>cl.CustomElement</code> targets. See Custom Elements.</li> </ul> <p>Installation Required</p> <p>The Chainlit UI requires the <code>chainlit</code> extra: <code>pip install \"hayhooks[chainlit]\"</code></p>"},{"location":"reference/environment-variables/#cors","title":"CORS","text":"<p>These map 1:1 to FastAPI CORSMiddleware and the settings in <code>hayhooks.settings.AppSettings</code>.</p>"},{"location":"reference/environment-variables/#hayhooks_cors_allow_origins","title":"HAYHOOKS_CORS_ALLOW_ORIGINS","text":"<ul> <li>Default: <code>[\"*\"]</code></li> <li>Description: List of allowed origins</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_cors_allow_methods","title":"HAYHOOKS_CORS_ALLOW_METHODS","text":"<ul> <li>Default: <code>[\"*\"]</code></li> <li>Description: List of allowed HTTP methods</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_cors_allow_headers","title":"HAYHOOKS_CORS_ALLOW_HEADERS","text":"<ul> <li>Default: <code>[\"*\"]</code></li> <li>Description: List of allowed headers</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_cors_allow_credentials","title":"HAYHOOKS_CORS_ALLOW_CREDENTIALS","text":"<ul> <li>Default: <code>false</code></li> <li>Description: Allow credentials</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_cors_allow_origin_regex","title":"HAYHOOKS_CORS_ALLOW_ORIGIN_REGEX","text":"<ul> <li>Default: <code>null</code></li> <li>Description: Regex pattern for allowed origins</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_cors_expose_headers","title":"HAYHOOKS_CORS_EXPOSE_HEADERS","text":"<ul> <li>Default: <code>[]</code></li> <li>Description: Headers to expose in response</li> </ul>"},{"location":"reference/environment-variables/#hayhooks_cors_max_age","title":"HAYHOOKS_CORS_MAX_AGE","text":"<ul> <li>Default: <code>600</code></li> <li>Description: Maximum age for CORS preflight responses in seconds</li> </ul>"},{"location":"reference/environment-variables/#logging","title":"Logging","text":""},{"location":"reference/environment-variables/#log-log-level","title":"LOG (log level)","text":"<ul> <li>Default: <code>INFO</code></li> <li>Description: Global log level (consumed by Loguru). Example: <code>LOG=DEBUG hayhooks run</code></li> </ul> <p>Logging Configuration</p> <p>Format/handlers are configured internally; Hayhooks does not expose <code>HAYHOOKS_LOG_FORMAT</code> or <code>HAYHOOKS_LOG_FILE</code> env vars at this time.</p>"},{"location":"reference/environment-variables/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/environment-variables/#docker","title":"Docker","text":"<pre><code>docker run -d \\\n  -e HAYHOOKS_HOST=0.0.0.0 \\\n  -e HAYHOOKS_PORT=1416 \\\n  -e HAYHOOKS_PIPELINES_DIR=/app/pipelines \\\n  -v \"$PWD/pipelines:/app/pipelines:ro\" \\\n  -p 1416:1416 \\\n  deepset/hayhooks:main\n</code></pre> <p>Pipeline Directory Required</p> <p>Without mounting a pipelines directory (or baking pipelines into the image), the server will start but no pipelines will be deployed.</p>"},{"location":"reference/environment-variables/#development","title":"Development","text":"<pre><code>export HAYHOOKS_HOST=127.0.0.1\nexport HAYHOOKS_PORT=1416\nexport HAYHOOKS_PIPELINES_DIR=./pipelines\nexport LOG=DEBUG\n\nhayhooks run\n</code></pre>"},{"location":"reference/environment-variables/#mcp-server-startup","title":"MCP Server startup","text":"<pre><code>export HAYHOOKS_MCP_HOST=0.0.0.0\nexport HAYHOOKS_MCP_PORT=1417\n\nhayhooks mcp run\n</code></pre>"},{"location":"reference/environment-variables/#env-file-example","title":".env file example","text":"<pre><code>HAYHOOKS_HOST=0.0.0.0\nHAYHOOKS_PORT=1416\nHAYHOOKS_MCP_HOST=0.0.0.0\nHAYHOOKS_MCP_PORT=1417\nHAYHOOKS_PIPELINES_DIR=./pipelines\nHAYHOOKS_ADDITIONAL_PYTHON_PATH=./custom_code\nHAYHOOKS_USE_HTTPS=false\nHAYHOOKS_DISABLE_SSL=false\nHAYHOOKS_SHOW_TRACEBACKS=false\nHAYHOOKS_STREAMING_COMPONENTS=all\nHAYHOOKS_CORS_ALLOW_ORIGINS=[\"*\"]\nLOG=INFO\n</code></pre> <p>Configuration Note</p> <ul> <li>Worker count, timeouts, and other server process settings are CLI flags (e.g., <code>hayhooks run --workers 4</code>).</li> <li>YAML/file saving and MCP exposure are controlled per-deploy via API/CLI flags, not global env vars.</li> </ul>"},{"location":"reference/environment-variables/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration</li> <li>Logging</li> </ul>"},{"location":"reference/logging/","title":"Logging","text":"<p>Hayhooks provides comprehensive logging capabilities for monitoring, debugging, and auditing pipeline execution and server operations.</p>"},{"location":"reference/logging/#log-levels","title":"Log Levels","text":""},{"location":"reference/logging/#available-levels","title":"Available Levels","text":"<ul> <li>TRACE: Detailed information for debugging</li> <li>DEBUG: Detailed information for debugging</li> <li>INFO: General information about server operations</li> <li>SUCCESS: Success messages</li> <li>WARNING: Warning messages that don't stop execution</li> <li>ERROR: Error messages that affect functionality</li> <li>CRITICAL: Critical errors that may cause server failure</li> </ul>"},{"location":"reference/logging/#setting-log-level","title":"Setting Log Level","text":"<pre><code>export LOG=debug #\u00a0or LOG=DEBUG\nhayhooks run\n</code></pre> <p>Or in your <code>.env</code> file:</p> <pre><code>LOG=info #\u00a0or LOG=INFO\n</code></pre> <p>Or inline:</p> <pre><code>LOG=debug hayhooks run\n</code></pre>"},{"location":"reference/logging/#log-configuration","title":"Log Configuration","text":""},{"location":"reference/logging/#environment-variables","title":"Environment Variables","text":""},{"location":"reference/logging/#log","title":"LOG","text":"<ul> <li>Default: <code>info</code></li> <li>Description: Minimum log level to display (consumed by Loguru)</li> <li>Options: <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code></li> </ul> <p>Note: Hayhooks does not expose <code>HAYHOOKS_LOG_FORMAT</code> or <code>HAYHOOKS_LOG_FILE</code> env vars; formatting/handlers are configured internally in the code.</p>"},{"location":"reference/logging/#custom-log-format","title":"Custom Log Format","text":"<p>If you need custom formatting, handle it in your host app via Loguru sinks.</p>"},{"location":"reference/logging/#file-logging","title":"File Logging","text":""},{"location":"reference/logging/#basic-file-logging","title":"Basic File Logging","text":"<p>Configure file sinks in your host app using <code>log.add(...)</code>.</p>"},{"location":"reference/logging/#rotating-file-logs","title":"Rotating File Logs","text":"<p>If you embed Hayhooks programmatically and want custom logging, set up logging in your host app and direct Hayhooks logs there.</p>"},{"location":"reference/logging/#pipeline-logging","title":"Pipeline Logging","text":"<p>The <code>log</code> object in Hayhooks is a Loguru logger instance. You can use all Loguru features and capabilities in your pipeline code.</p>"},{"location":"reference/logging/#basic-usage","title":"Basic Usage","text":"<pre><code>from hayhooks import log\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def setup(self) -&gt; None:\n        log.info(\"Setting up pipeline\")\n        # ... setup code\n\n    def run_api(self, query: str) -&gt; str:\n        log.debug(\"Processing query: {query}\")\n        try:\n            result = self.pipeline.run({\"prompt\": {\"query\": query}})\n            log.info(\"Pipeline execution completed successfully\")\n            return result[\"llm\"][\"replies\"][0]\n        except Exception as e:\n            log.error(\"Pipeline execution failed: {}\", e)\n            raise\n</code></pre>"},{"location":"reference/logging/#execution-time-logging","title":"Execution Time Logging","text":"<pre><code>import time\nfrom hayhooks import log\n\nclass PipelineWrapper(BasePipelineWrapper):\n    def run_api(self, query: str) -&gt; str:\n        start_time = time.time()\n\n        result = self.pipeline.run({\"prompt\": {\"query\": query}})\n\n        execution_time = time.time() - start_time\n        log.info(\"Pipeline executed in {} seconds\", execution_time.round(2))\n\n        return result[\"llm\"][\"replies\"][0]\n</code></pre> <p>For more advanced logging patterns (structured logging, custom sinks, formatting, etc.), refer to the Loguru documentation.</p>"},{"location":"reference/logging/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Environment Variables - Configuration options</li> </ul>"}]}